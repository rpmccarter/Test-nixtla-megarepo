[
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "",
    "text": "By Fugue and Nixtla. Originally posted on TDS.\nTime-series modeling, analysis, and prediction of trends and seasonalities for data collected over time is a rapidly growing category of software applications.\nBusinesses, from electricity and economics to healthcare analytics, collect time-series data daily to predict patterns and build better data-driven product experiences. For example, temperature and humidity prediction is used in manufacturing to prevent defects, streaming metrics predictions help identify music’s popular artists, and sales forecasting for thousands of SKUs across different locations in the supply chain is used to optimize inventory costs. As data generation increases, the forecasting necessities have evolved from modeling a few time series to predicting millions.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#motivation",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Motivation",
    "text": "Motivation\nNixtla is an open-source project focused on state-of-the-art time series forecasting. They have a couple of libraries such as StatsForecast for statistical models, NeuralForecast for deep learning, and HierarchicalForecast for forecast aggregations across different levels of hierarchies. These are production-ready time series libraries focused on different modeling techniques.\nThis article looks at StatsForecast, a lightning-fast forecasting library with statistical and econometrics models. The AutoARIMA model of Nixtla is 20x faster than pmdarima, and the ETS (error, trend, seasonal) models performed 4x faster than statsmodels and are more robust. The benchmarks and code to reproduce can be found here. A huge part of the performance increase is due to using a JIT compiler called numba to achieve high speeds.\nThe faster iteration time means that data scientists can run more experiments and converge to more accurate models faster. It also means that running benchmarks at scale becomes easier.\nIn this article, we are interested in the scalability of the StatsForecast library in fitting models over Spark or Dask using the Fugue library. This combination will allow us to train a huge number of models distributedly over a temporary cluster quickly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#experiment-setup",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Experiment Setup",
    "text": "Experiment Setup\nWhen dealing with large time series data, users normally have to deal with thousands of logically independent time series (think of telemetry of different users or different product sales). In this case, we can train one big model over all of the series, or we can create one model for each series. Both are valid approaches since the bigger model will pick up trends across the population, while training thousands of models may fit individual series data better.\n\n\n\n\n\n\nNote\n\n\n\nNote: to pick up both the micro and macro trends of the time series population in one model, check the Nixtla HierarchicalForecast library, but this is also more computationally expensive and trickier to scale.\n\n\nThis article will deal with the scenario where we train a couple of models (AutoARIMA or ETS) per univariate time series. For this setup, we group the full data by time series, and then train each model for each group. The image below illustrates this. The distributed DataFrame can either be a Spark or Dask DataFrame.\n\n\n\nAutoARIMA per partition\n\n\nNixtla previously released benchmarks with Anyscale on distributing this model training on Ray. The setup and results can be found in this blog. The results are also shown below. It took 2000 cpus to run one million AutoARIMA models in 35 minutes. We’ll compare this against running on Spark.\n\n\n\nStatsForecast on Ray results"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#statsforecast-code",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "StatsForecast code",
    "text": "StatsForecast code\nFirst, we’ll look at the StatsForecast code used to run the AutoARIMA distributedly on Ray. This is a simplified version to run the scenario with a one million time series. It is also updated for the recent StatsForecast v1.0.0 release, so it may look a bit different from the code in the previous benchmarks.\nfrom time import time\n\nimport pandas as pd\nfrom statsforecast.utils import generate_series\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nseries = generate_series(n_series=1000000, seed=1)\n\nmodel = StatsForecast(df=series,\n                      models=[AutoARIMA()], \n                      freq='D', \n                      n_jobs=-1,\n              ray_address=ray_address)\n\ninit = time()\nforecasts = model.forecast(7)\nprint(f'n_series: 1000000 total time: {(time() - init) / 60}')\nThe interface of StatsForecast is very minimal. It is already designed to perform the AutoARIMA on each group of data. Just supplying the ray_address will make this code snippet run distributedly. Without it, n_jobswill indicate the number of parallel processes for forecasting. model.forecast() will do the fit and predict in one step, and the input to this method in the time horizon to forecast."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#using-fugue-to-run-on-spark-and-dask",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Using Fugue to run on Spark and Dask",
    "text": "Using Fugue to run on Spark and Dask\nFugue is an abstraction layer that ports Python, Pandas, and SQL code to Spark and Dask. The most minimal interface is the transform() function. This function takes in a function and DataFrame, and brings it to Spark or Dask. We can use the transform() function to bring StatsForecast execution to Spark.\nThere are two parts to the code below. First, we have the forecast logic defined in the forecast_series function. Some parameters are hardcoded for simplicity. The most important one is that n_jobs=1. This is because Spark or Dask will already serve as the parallelization layer, and having two stages of parallelism can cause resource deadlocks.\nfrom fugue import transform\n\ndef forecast_series(df: pd.DataFrame, models) -&gt; pd.DataFrame:\n    tdf = df.set_index(\"unique_id\")\n    model = StatsForecast(df=tdf, models=models, freq='D', n_jobs=1)\n    return model.forecast(7).reset_index()\n\ntransform(series.reset_index(),\n          forecast_series,\n          params=dict(models=[AutoARIMA()]),\n          schema=\"unique_id:int, ds:date, AutoARIMA:float\",\n          partition={\"by\": \"unique_id\"},\n          engine=\"spark\"\n          ).show()\nSecond, the transform() function is used to apply the forecast_series() function on Spark. The first two arguments are the DataFrame and function to be applied. Output schema is a requirement for Spark, so we need to pass it in, and the partition argument will take care of splitting the time series modelling by unique_id.\nThis code already works and returns a Spark DataFrame output."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#nixtlas-fuguebackend",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Nixtla’s FugueBackend",
    "text": "Nixtla’s FugueBackend\nThe transform() above is a general look at what Fugue can do. In practice, the Fugue and Nixtla teams collaborated to add a more native FugueBackend to the StatsForecast library. Along with it is a utility forecast() function to simplify the forecasting interface. Below is an end-to-end example of running StatsForecast on one million time series.\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\nforecast(spark.read.parquet(\"/tmp/1m.parquet\"), \n         [AutoARIMA()], \n         freq=\"D\", \n         h=7, \n         parallel=backend).toPandas()\nWe just need to create the FugueBackend, which takes in a SparkSession and passes it to forecast(). This function can take either a DataFrame or file path to the data. If a file path is provided, it will be loaded with the parallel backend. In this example above, we replaced the file each time we ran the experiment to generate benchmarks.\n\n\n\n\n\n\nCaution\n\n\n\nIt’s also important to note that we can test locally before running the forecast() on full data. All we have to do is not supply anything for the parallel argument; everything will run on Pandas sequentially."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#benchmark-results",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Benchmark Results",
    "text": "Benchmark Results\nThe benchmark results can be seen below. As of the time of this writing, Dask and Ray made recent releases, so only the Spark metrics are up to date. We will make a follow-up article after running these experiments with the updates.\n\n\n\nSpark and Dask benchmarks for StatsForecast at scale\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The attempt was to use 2000 cpus but we were limited by available compute instances on AWS.\n\n\nThe important part here is that AutoARIMA trained one million time series models in less than 15 minutes. The cluster configuration is attached in the appendix. With very few lines of code, we were able to orchestrate the training of these time series models distributedly."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#conclusion",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Conclusion",
    "text": "Conclusion\nTraining thousands of time series models distributedly normally takes a lot of coding with Spark and Dask, but we were able to run these experiments with very few lines of code. Nixtla’s StatsForecast offers the ability to quickly utilize all of the compute resources available to find the best model for each time series. All users need to do is supply a relevant parallel backend (Ray or Fugue) to run on a cluster.\nOn the scale of one million timeseries, our total training time took 12 minutes for AutoARIMA. This is the equivalent of close to 400 cpu-hours that we ran immediately, allowing data scientists to quickly iterate at scale without having to write the explicit code for parallelization. Because we used an ephemeral cluster, the cost is effectively the same as running this sequentially on an EC2 instance (parallelized over all cores)."
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#resources",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Resources",
    "text": "Resources\n\nNixtla StatsForecast repo\nStatsForecast docs\nFugue repo\nFugue tutorials\n\nTo chat with us:\n\nFugue Slack\nNixtla Slack"
  },
  {
    "objectID": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "href": "blog/posts/2022-10-05-distributed-fugue/index.html#appendix",
    "title": "Scalable Time Series Modeling with open-source projects",
    "section": "Appendix",
    "text": "Appendix\nFor anyone. interested in the cluster configuration, it can be seen below. This will spin up a Databricks cluster. The important thing is the node_type_id that has the machines used.\n{\n    \"num_workers\": 20,\n    \"cluster_name\": \"fugue-nixtla-2\",\n    \"spark_version\": \"10.4.x-scala2.12\",\n    \"spark_conf\": {\n        \"spark.speculation\": \"true\",\n        \"spark.sql.shuffle.partitions\": \"8000\",\n        \"spark.sql.adaptive.enabled\": \"false\",\n        \"spark.task.cpus\": \"1\"\n    },\n    \"aws_attributes\": {\n        \"first_on_demand\": 1,\n        \"availability\": \"SPOT_WITH_FALLBACK\",\n        \"zone_id\": \"us-west-2c\",\n        \"spot_bid_price_percent\": 100,\n        \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n        \"ebs_volume_count\": 1,\n        \"ebs_volume_size\": 32\n    },\n    \"node_type_id\": \"m5.24xlarge\",\n    \"driver_node_type_id\": \"m5.2xlarge\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {\n        \"MKL_NUM_THREADS\": \"1\",\n        \"OPENBLAS_NUM_THREADS\": \"1\",\n        \"VECLIB_MAXIMUM_THREADS\": \"1\",\n        \"OMP_NUM_THREADS\": \"1\",\n        \"NUMEXPR_NUM_THREADS\": \"1\"\n    },\n    \"autotermination_minutes\": 20,\n    \"enable_elastic_disk\": false,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"runtime_engine\": \"STANDARD\",\n    \"cluster_id\": \"0728-004950-oefym0ss\"\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StatsForecast ⚡️",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "StatsForecast ⚡️",
    "section": "",
    "text": "You can install StatsForecast with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\nVist our Installation Guide for further instructions."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "StatsForecast ⚡️",
    "section": "Quick Start",
    "text": "Quick Start\nMinimal Example\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\nsf.predict(h=12, level=[95])\nGet Started with this quick guide.\nFollow this end-to-end walkthrough for best practices."
  },
  {
    "objectID": "index.html#why",
    "href": "index.html#why",
    "title": "StatsForecast ⚡️",
    "section": "Why?",
    "text": "Why?\nCurrent Python alternatives for statistical models are slow, inaccurate and don’t scale well. So we created a library that can be used to forecast in production environments or as benchmarks. StatsForecast includes an extensive battery of models that can efficiently fit millions of time series."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "StatsForecast ⚡️",
    "section": "Features",
    "text": "Features\n\nFastest and most accurate implementations of AutoARIMA, AutoETS, AutoCES, MSTL and Theta in Python.\nOut-of-the-box compatibility with Spark, Dask, and Ray.\nProbabilistic Forecasting and Confidence Intervals.\nSupport for exogenous Variables and static covariates.\nAnomaly Detection.\nFamiliar sklearn syntax: .fit and .predict."
  },
  {
    "objectID": "index.html#highlights",
    "href": "index.html#highlights",
    "title": "StatsForecast ⚡️",
    "section": "Highlights",
    "text": "Highlights\n\nInclusion of exogenous variables and prediction intervals for ARIMA.\n20x faster than pmdarima.\n1.5x faster than R.\n500x faster than Prophet.\n4x faster than statsmodels.\nCompiled to high performance machine code through numba.\n1,000,000 series in 30 min with ray.\nReplace FB-Prophet in two lines of code and gain speed and accuracy. Check the experiments here.\nFit 10 benchmark models on 1,000,000 series in under 5 min.\n\nMissing something? Please open an issue or write us in"
  },
  {
    "objectID": "index.html#examples-and-guides",
    "href": "index.html#examples-and-guides",
    "title": "StatsForecast ⚡️",
    "section": "Examples and Guides",
    "text": "Examples and Guides\n📚 End to End Walkthrough: Model training, evaluation and selection for multiple time series\n🔎 Anomaly Detection: detect anomalies for time series using in-sample prediction intervals.\n👩‍🔬 Cross Validation: robust model’s performance evaluation.\n❄️ Multiple Seasonalities: how to forecast data with multiple seasonalities using an MSTL.\n🔌 Predict Demand Peaks: electricity load forecasting for detecting daily peaks and reducing electric bills.\n📈 Intermittent Demand: forecast series with very few non-zero observations.\n🌡️ Exogenous Regressors: like weather or prices"
  },
  {
    "objectID": "index.html#models",
    "href": "index.html#models",
    "title": "StatsForecast ⚡️",
    "section": "Models",
    "text": "Models\n\nAutomatic Forecasting\nAutomatic forecasting tools search for the best parameters and select the best possible model for a group of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n✅\n✅\n✅\n✅\n\n\nAutoETS\n✅\n✅\n✅\n✅\n\n\nAutoCES\n✅\n✅\n✅\n✅\n\n\nAutoTheta\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "index.html#arima-family",
    "href": "index.html#arima-family",
    "title": "StatsForecast ⚡️",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n✅\n✅\n✅\n✅\n\n\nAutoRegressive\n✅\n✅\n✅\n✅\n\n\n\n\nTheta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n✅\n✅\n✅\n✅\n\n\nOptimizedTheta\n✅\n✅\n✅\n✅\n\n\nDynamicTheta\n✅\n✅\n✅\n✅\n\n\nDynamicOptimizedTheta\n✅\n✅\n✅\n✅\n\n\n\n\n\nMultiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n✅\n✅\n✅\n✅\n\n\n\n\n\nGARCH and ARCH Models\nSuited for modeling time series that exhibit non-constant volatility over time. The ARCH model is a particular case of GARCH.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nGARCH\n✅\n✅\n✅\n✅\n\n\nARCH\n✅\n✅\n✅\n✅\n\n\n\n\n\nBaseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n✅\n✅\n✅\n✅\n\n\nNaive\n✅\n✅\n✅\n✅\n\n\nRandomWalkWithDrift\n✅\n✅\n✅\n✅\n\n\nSeasonalNaive\n✅\n✅\n✅\n✅\n\n\nWindowAverage\n✅\n\n\n\n\n\nSeasonalWindowAverage\n✅\n\n\n\n\n\n\n\n\nExponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n✅\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n✅\n\n\n\n\n\nSeasonalExponentialSmoothing\n✅\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n✅\n\n\n\n\n\nHolt\n✅\n✅\n✅\n✅\n\n\nHoltWinters\n✅\n✅\n✅\n✅\n\n\n\n\n\nSparse or Inttermitent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n✅\n\n\n\n\n\nCrostonClassic\n✅\n\n\n\n\n\nCrostonOptimized\n✅\n\n\n\n\n\nCrostonSBA\n✅\n\n\n\n\n\nIMAPA\n✅\n\n\n\n\n\nTSB\n✅"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "StatsForecast ⚡️",
    "section": "How to contribute",
    "text": "How to contribute\nSee CONTRIBUTING.md."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "StatsForecast ⚡️",
    "section": "Citing",
    "text": "Citing\n@misc{garza2022statsforecast,\n    author={Federico Garza, Max Mergenthaler Canseco, Cristian Challú, Kin G. Olivares},\n    title = {{StatsForecast}: Lightning fast forecasting with statistical and econometric models},\n    year={2022},\n    howpublished={{PyCon} Salt Lake City, Utah, US 2022},\n    url={https://github.com/Nixtla/statsforecast}\n}"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html",
    "href": "docs/getting-started/getting_started_complete.html",
    "title": "End to End Walkthrough",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start.\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#install-libraries",
    "href": "docs/getting-started/getting_started_complete.html#install-libraries",
    "title": "End to End Walkthrough",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#read-the-data",
    "href": "docs/getting-started/getting_started_complete.html#read-the-data",
    "title": "End to End Walkthrough",
    "section": "Read the data",
    "text": "Read the data\nWe will use pandas to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. The target column needs to be renamed to y if it has a different column name.\n\nThis data set already satisfies the requirements.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport pandas as pd\n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.query('unique_id in @uids') \n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#explore-data-with-the-plot-method",
    "href": "docs/getting-started/getting_started_complete.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#train-multiple-models-for-many-series",
    "href": "docs/getting-started/getting_started_complete.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models .\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See pandas available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nHoltWinters\nHoltWinters-lo-90\nHoltWinters-hi-90\nCrostonClassic\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-90\nDynamicOptimizedTheta-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n749\n592.461792\n572.325623\n612.597961\n829.0\n-246.367554\n1904.367554\n708.21405\n635.0\n537.471191\n732.528809\n660.982117\n398.03775\n923.926514\n592.701843\n577.677307\n611.652649\n\n\nH1\n750\n527.174316\n495.321777\n559.026855\n807.0\n-268.367554\n1882.367554\n708.21405\n572.0\n474.471222\n669.528809\n660.982117\n398.03775\n923.926514\n525.589111\n505.449738\n546.621826\n\n\nH1\n751\n488.418549\n445.535583\n531.301514\n785.0\n-290.367554\n1860.367554\n708.21405\n532.0\n434.471222\n629.528809\n660.982117\n398.03775\n923.926514\n489.251801\n462.072876\n512.424133\n\n\nH1\n752\n452.284454\n400.677155\n503.891785\n756.0\n-319.367554\n1831.367554\n708.21405\n493.0\n395.471222\n590.528809\n660.982117\n398.03775\n923.926514\n456.195038\n430.554291\n478.260956\n\n\nH1\n753\n433.127563\n374.070984\n492.184143\n719.0\n-356.367554\n1794.367554\n708.21405\n477.0\n379.471222\n574.528809\n660.982117\n398.03775\n923.926514\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\n\nPlot the results of 8 random series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df)\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90])\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90])"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#evaluate-the-models-performance",
    "href": "docs/getting-started/getting_started_complete.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough",
    "section": "Evaluate the model’s performance",
    "text": "Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n701\n700\n619.0\n603.925415\n847.0\n742.668762\n691.0\n661.674988\n612.767517\n\n\nH1\n702\n700\n565.0\n507.591736\n820.0\n742.668762\n618.0\n661.674988\n536.846252\n\n\nH1\n703\n700\n532.0\n481.281677\n790.0\n742.668762\n563.0\n661.674988\n497.824280\n\n\nH1\n704\n700\n495.0\n444.410248\n784.0\n742.668762\n529.0\n661.674988\n464.723236\n\n\nH1\n705\n700\n481.0\n421.168762\n752.0\n742.668762\n504.0\n661.674988\n440.972351\n\n\n\n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from datasetsforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y']).columns.tolist()\n    evals = []\n    for model in models:\n        eval_ = df.groupby(['unique_id', 'cutoff']).apply(lambda x: metric(x['y'].values, x[model].values)).to_frame() # Calculate loss for every unique_id, model and cutoff.\n        eval_.columns = [model]\n        evals.append(eval_)\n    evals = pd.concat(evals, axis=1)\n    evals = evals.groupby(['unique_id']).mean(numeric_only=True) # Averages the error metrics for all cutoffs for every combination of model and unique_id\n    evals['best_model'] = evals.idxmin(axis=1)\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df, mse)\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\nbest_model\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nH1\n1979.302490\n44888.019531\n28038.736328\n1422.666748\n20927.664062\n1296.333984\nDynamicOptimizedTheta\n\n\nH10\n458.892700\n2812.916504\n1483.484131\n96.895828\n1980.367432\n379.621124\nSeasonalNaive\n\n\nH100\n8629.948242\n121625.375000\n91945.140625\n12019.000000\n78491.187500\n21699.648438\nAutoARIMA\n\n\nH101\n6818.349121\n28453.394531\n16183.634766\n10944.458008\n18208.404297\n63698.074219\nAutoARIMA\n\n\nH102\n65489.968750\n232924.843750\n132655.296875\n12699.896484\n309110.468750\n31393.521484\nSeasonalNaive\n\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nsummary_df = evaluation_df.groupby('best_model').size().sort_values().to_frame()\n\nsummary_df.reset_index().columns = [\"Model\", \"Nr. of unique_ids\"]\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.query('best_model == \"SeasonalNaive\"').index\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"])"
  },
  {
    "objectID": "docs/getting-started/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "href": "docs/getting-started/getting_started_complete.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n    df.columns = ['model', 'best_model_forecast'] \n    df = df.join(evaluation_df[['best_model']])\n    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n    df.columns = df.columns.droplevel()\n    df = df.reset_index(level=1)\n    return df\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\n\n\n\n\n\nmodel\nds\nbest_model\nbest_model-hi-90\nbest_model-lo-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n749\n592.701843\n611.652649\n577.677307\n\n\nH1\n750\n525.589111\n546.621826\n505.449738\n\n\nH1\n751\n489.251801\n512.424133\n462.072876\n\n\nH1\n752\n456.195038\n478.260956\n430.554291\n\n\nH1\n753\n436.290527\n461.815948\n411.051239\n\n\n\n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90])"
  },
  {
    "objectID": "docs/contribute/docs.html",
    "href": "docs/contribute/docs.html",
    "title": "Nixtla Documentation",
    "section": "",
    "text": "TBD\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/step-by-step.html",
    "href": "docs/contribute/step-by-step.html",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "This document contains instructions for collaborating on the different libraries of Nixtla.\n\nSometimes, diving into a new technology can be challenging and overwhelming. We’ve been there too, and we’re more than ready to assist you with any issues you may encounter while following these steps. Don’t hesitate to reach out to us on Slack. Just give fede a ping, and she’ll be glad to assist you.\n\n\n\nPrerequisites\nGit fork-and-pull worklow\nSet Up a Conda Environment\nInstall required libraries for development\nStart editable mode\nSet Up your Notebook based development environment\nStart Coding\nExample with Screen-shots\n\n\n\n\n\nGitHub: You should already have a GitHub account and a basic understanding of its functionalities. Alternatively check this guide.\nPython: Python should be installed on your system. Alternatively check this guide.\nconda: You need to have conda installed, along with a good grasp of fundamental operations such as creating environments, and activating and deactivating them. Alternatively check this guide.\n\n\n\n\n1. Fork the Project: Start by forking the Nixtla repository to your own GitHub account. This creates a personal copy of the project where you can make changes without affecting the main repository.\n2. Clone the Forked Repository Clone the forked repository to your local machine using git clone https://github.com/&lt;your-username&gt;/nixtla.git. This allows you to work with the code directly on your system.\n3. Create a Branch:\nBranching in GitHub is a key strategy for effectively managing and isolating changes to your project. It allows you to segregate work on different features, fixes, and issues without interfering with the main, production-ready codebase.\n\nMain Branch: The default branch with production-ready code.\nFeature Branches: For new features, create branches prefixed with ‘feature/’, like git checkout -b feature/new-model.\nFix Branches: For bug fixes, use ‘fix/’ prefix, like git checkout -b fix/forecasting-bug.\nIssue Branches: For specific issues, use git checkout -b issue/issue-number or git checkout -b issue/issue-description.\n\nAfter testing, branches are merged back into the main branch via a pull request, and then typically deleted to maintain a clean repository. You can read more about github and branching here.\n\n\n\n\nIf you want to use Docker or Codespaces, let us know opening an issue and we will set you up.\n\nNext, you’ll need to set up a Conda environment. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux. It allows you to create separate environments containing files, packages, and dependencies that will not interact with each other.\nFirst, ensure you have Anaconda or Miniconda installed on your system. Alternatively checkout these guides: Anaconda, Miniconda, and Mamba.\nThen, you can create a new environment using conda create -n nixtla-env python=3.10.\nYou can also use mamba for creating the environment (mamba is faster than Conda) using mamba create -n nixtla-env python=3.10.\nYou can replace nixtla-env for something more meaningful to you. Eg. statsforecast-env or mlforecast-env. You can always check the list of environments in your system using conda env list.\nActivate your new environment with conda activate nixtla-env.\n\n\n\nThe environment.yml file contains all the dependencies required for the project. To install these dependencies, use the mamba package manager, which offers faster package installation and environment resolution than Conda. If you haven’t installed mamba yet, you can do so using conda install mamba -c conda-forge. Run the following command to install the dependencies:\nmamba env update -f environment.yml\nSometimes (e.g. StatsForecast) the enviorment.yml is sometimes inside a folder called dev. In that case, you should run mamba env update -f dev/environment.yml.\n\n\n\nInstall the library in editable mode using pip install -e \".[dev]\".\nThis means the package is linked directly to the source code, allowing any changes made to the source code to be immediately reflected in your Python environment without the need to reinstall the package. This is useful for testing changes during package development.\n\n\n\nNotebook-based development refers to using interactive notebooks, such as Jupyter Notebooks, for coding, data analysis, and visualization. Here’s a brief description of its characteristics:\n\nInteractivity: Code in notebooks is written in cells which can be run independently. This allows for iterative development and testing of small code snippets.\nVisualization: Notebooks can render charts, tables, images, and other graphical outputs within the same interface, making it great for data exploration and analysis.\nDocumentation: Notebooks support Markdown and HTML, allowing for detailed inline documentation. Code, outputs, and documentation are in one place, which is ideal for tutorials, reports, or sharing work.\n\nFor notebook based development you’ll need nbdev and a notebook editor (such as VS Code, Jupyter Notebook or Jupyter Lab). nbdev and jupyter have been installed in the previous step. If you use VS Code follow this tutorial.\nnbdev makes debugging and refactoring your code much easier than in traditional programming environments since you always have live objects at your fingertips. nbdev also promotes software engineering best practices because tests and documentation are first class.\nAll your changes must be written in the notebooks contained in the library (under the nbs directory). Once a specific notebook is open (more details to come), you can write your Python code in cells within the notebook, as you would do in a traditional Python development workflow. You can break down complex problems into smaller parts, visualizing data, and documenting your thought process. Along with your code, you can include markdown cells to add documentation directly in the notebook. This includes explanations of your logic, usage examples, and more. Also, nbdev allows you to write tests inline with your code in your notebook. After writing a function, you can immediately write tests for it in the following cells.\nOnce your code is ready, nbdev can automatically convert your notebook into Python scripts. Code cells are converted into Python code, and markdown cells into comments and docstrings.\n\n\n\nOpen a jupyter notebook using jupyter lab (or VS Code).\n\nMake Your Changes: Make changes to the codebase, ensuring your changes are self-contained and cohesive.\nCommit Your Changes: Add the changed files using git add [your_modified_file_0.ipynb] [your_modified_file_1.ipynb], then commit these changes using git commit -m \"&lt;type&gt;: &lt;Your descriptive commit message&gt;\". Please use Conventional Commits\nPush Your Changes: Push your changes to the remote repository on GitHub with git push origin feature/your-feature-name.\nOpen a Pull Request: Open a pull request from your new branch on the Nixtla repository on GitHub. Provide a thorough description of your changes when creating the pull request.\nWait for Review: The maintainers of the Nixtla project will review your changes. Be ready to iterate on your contributions based on their feedback.\n\nRemember, contributing to open-source projects is a collaborative effort. Respect the work of others, welcome feedback, and always strive to improve. Happy coding!\n\nNixtla offers the possibility of assisting with stipends for computing infrastructure for our contributors. If you are interested, please join our slack and write to fede or Max.\n\nYou can find a detailed step by step buide with screen-shots below.\n\n\n\n\n\nThe first thing you need to do is create a fork of the GitHub repository to your own account:\n\nYour fork on your account will look like this:\n\nIn that repository, you can make your changes and then request to have them added to the main repo.\n\n\n\nIn this tutorial, we are using Mac (also compatible with other Linux distributions). If you are a collaborator of Nixtla, you can request an AWS instance to collaborate from there. If this is the case, please reach out to Max or Fede on Slack to receive the appropriate access. We also use Visual Studio Code, which you can download from here.\nOnce the repository is created, you need to clone it to your own computer. Simply copy the repository URL from GitHub as shown below:\n\nThen open Visual Studio Code, click on “Clone Git Repository,” and paste the line you just copied into the top part of the window, as shown below:\n\nSelect the folder where you want to copy the repository:\n\nAnd choose to open the cloned repository:\n\nYou will end up with something like this:\n\n\n\n\nOpen a terminal within Visual Studio Code, as shown in the image:\n\nYou can use conda but we highly recommend using Mamba to speed up the creation of the Conda environment. To install it, simply use conda install mamba -c conda-forge in the terminal you just opened:\n\nCreate an empty environment named mlforecast with the following command: mamba create -n mlforecast python=3.10:\n\nActivate the newly created environment using conda activate mlforecast:\n\nInstall the libraries within the environment file environment.yml using mamba env update -f environment.yml:\n\nNow install the library to make interactive changes and other additional dependencies using pip install -e \".[dev]\":\n\n\n\n\nIn this section, we assume that we want to increase the default number of windows used to create prediction intervals from 2 to 3. The first thing we need to do is create a specific branch for that change using git checkout -b [new_branch] like this:\n\nOnce created, open the notebook you want to modify. In this case, it’s nbs/utils.ipynb, which contains the metadata for the prediction intervals. After opening it, click on the environment you want to use (top right) and select the mlforecast environment:\n\nNext, execute the notebook and make the necessary changes. In this case, we want to modify the PredictionIntervals class:\n\nWe will change the default value of n_window from 2 to 3:\n\nOnce you have made the change and performed any necessary validations, it’s time to convert the notebook to Python modules. To do this, simply use nbdev_export in the terminal.\nYou will see that the mlforecast/utils.py file has been modified (the changes from nbs/utils.ipynb are reflected in that module). Before committing the changes, we need to clean the notebooks using the command ./action_files/clean_nbs and verify that the linters pass using ./action_files/lint:\n\nOnce you have done the above, simply add the changes using git add nbs/utils.ipynb mlforecast/utils.py:\n\nCreate a descriptive commit message for the changes using git commit -m \"[description of changes]\":\n\nFinally, push your changes using git push:\n\n\n\n\nIn GitHub, open your repository that contains your fork of the original repo. Once inside, you will see the changes you just pushed. Click on “Compare and pull request”:\n\nInclude an appropriate title for your pull request and fill in the necessary information. Once you’re done, click on “Create pull request”.\n\nFinally, you will see something like this:\n\n\n\n\n\n\nThis file was generated using this file. Please change that file if you want to enhance the document.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#table-of-contents",
    "href": "docs/contribute/step-by-step.html#table-of-contents",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Prerequisites\nGit fork-and-pull worklow\nSet Up a Conda Environment\nInstall required libraries for development\nStart editable mode\nSet Up your Notebook based development environment\nStart Coding\nExample with Screen-shots"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#prerequisites",
    "href": "docs/contribute/step-by-step.html#prerequisites",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "GitHub: You should already have a GitHub account and a basic understanding of its functionalities. Alternatively check this guide.\nPython: Python should be installed on your system. Alternatively check this guide.\nconda: You need to have conda installed, along with a good grasp of fundamental operations such as creating environments, and activating and deactivating them. Alternatively check this guide."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#git-fork-and-pull-worklow",
    "href": "docs/contribute/step-by-step.html#git-fork-and-pull-worklow",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "1. Fork the Project: Start by forking the Nixtla repository to your own GitHub account. This creates a personal copy of the project where you can make changes without affecting the main repository.\n2. Clone the Forked Repository Clone the forked repository to your local machine using git clone https://github.com/&lt;your-username&gt;/nixtla.git. This allows you to work with the code directly on your system.\n3. Create a Branch:\nBranching in GitHub is a key strategy for effectively managing and isolating changes to your project. It allows you to segregate work on different features, fixes, and issues without interfering with the main, production-ready codebase.\n\nMain Branch: The default branch with production-ready code.\nFeature Branches: For new features, create branches prefixed with ‘feature/’, like git checkout -b feature/new-model.\nFix Branches: For bug fixes, use ‘fix/’ prefix, like git checkout -b fix/forecasting-bug.\nIssue Branches: For specific issues, use git checkout -b issue/issue-number or git checkout -b issue/issue-description.\n\nAfter testing, branches are merged back into the main branch via a pull request, and then typically deleted to maintain a clean repository. You can read more about github and branching here."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#set-up-a-conda-environment",
    "href": "docs/contribute/step-by-step.html#set-up-a-conda-environment",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "If you want to use Docker or Codespaces, let us know opening an issue and we will set you up.\n\nNext, you’ll need to set up a Conda environment. Conda is an open-source package management and environment management system that runs on Windows, macOS, and Linux. It allows you to create separate environments containing files, packages, and dependencies that will not interact with each other.\nFirst, ensure you have Anaconda or Miniconda installed on your system. Alternatively checkout these guides: Anaconda, Miniconda, and Mamba.\nThen, you can create a new environment using conda create -n nixtla-env python=3.10.\nYou can also use mamba for creating the environment (mamba is faster than Conda) using mamba create -n nixtla-env python=3.10.\nYou can replace nixtla-env for something more meaningful to you. Eg. statsforecast-env or mlforecast-env. You can always check the list of environments in your system using conda env list.\nActivate your new environment with conda activate nixtla-env."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#install-required-libraries-for-development",
    "href": "docs/contribute/step-by-step.html#install-required-libraries-for-development",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "The environment.yml file contains all the dependencies required for the project. To install these dependencies, use the mamba package manager, which offers faster package installation and environment resolution than Conda. If you haven’t installed mamba yet, you can do so using conda install mamba -c conda-forge. Run the following command to install the dependencies:\nmamba env update -f environment.yml\nSometimes (e.g. StatsForecast) the enviorment.yml is sometimes inside a folder called dev. In that case, you should run mamba env update -f dev/environment.yml."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#start-editable-mode",
    "href": "docs/contribute/step-by-step.html#start-editable-mode",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Install the library in editable mode using pip install -e \".[dev]\".\nThis means the package is linked directly to the source code, allowing any changes made to the source code to be immediately reflected in your Python environment without the need to reinstall the package. This is useful for testing changes during package development."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#set-up-your-notebook-based-development-environment",
    "href": "docs/contribute/step-by-step.html#set-up-your-notebook-based-development-environment",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Notebook-based development refers to using interactive notebooks, such as Jupyter Notebooks, for coding, data analysis, and visualization. Here’s a brief description of its characteristics:\n\nInteractivity: Code in notebooks is written in cells which can be run independently. This allows for iterative development and testing of small code snippets.\nVisualization: Notebooks can render charts, tables, images, and other graphical outputs within the same interface, making it great for data exploration and analysis.\nDocumentation: Notebooks support Markdown and HTML, allowing for detailed inline documentation. Code, outputs, and documentation are in one place, which is ideal for tutorials, reports, or sharing work.\n\nFor notebook based development you’ll need nbdev and a notebook editor (such as VS Code, Jupyter Notebook or Jupyter Lab). nbdev and jupyter have been installed in the previous step. If you use VS Code follow this tutorial.\nnbdev makes debugging and refactoring your code much easier than in traditional programming environments since you always have live objects at your fingertips. nbdev also promotes software engineering best practices because tests and documentation are first class.\nAll your changes must be written in the notebooks contained in the library (under the nbs directory). Once a specific notebook is open (more details to come), you can write your Python code in cells within the notebook, as you would do in a traditional Python development workflow. You can break down complex problems into smaller parts, visualizing data, and documenting your thought process. Along with your code, you can include markdown cells to add documentation directly in the notebook. This includes explanations of your logic, usage examples, and more. Also, nbdev allows you to write tests inline with your code in your notebook. After writing a function, you can immediately write tests for it in the following cells.\nOnce your code is ready, nbdev can automatically convert your notebook into Python scripts. Code cells are converted into Python code, and markdown cells into comments and docstrings."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#start-coding",
    "href": "docs/contribute/step-by-step.html#start-coding",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "Open a jupyter notebook using jupyter lab (or VS Code).\n\nMake Your Changes: Make changes to the codebase, ensuring your changes are self-contained and cohesive.\nCommit Your Changes: Add the changed files using git add [your_modified_file_0.ipynb] [your_modified_file_1.ipynb], then commit these changes using git commit -m \"&lt;type&gt;: &lt;Your descriptive commit message&gt;\". Please use Conventional Commits\nPush Your Changes: Push your changes to the remote repository on GitHub with git push origin feature/your-feature-name.\nOpen a Pull Request: Open a pull request from your new branch on the Nixtla repository on GitHub. Provide a thorough description of your changes when creating the pull request.\nWait for Review: The maintainers of the Nixtla project will review your changes. Be ready to iterate on your contributions based on their feedback.\n\nRemember, contributing to open-source projects is a collaborative effort. Respect the work of others, welcome feedback, and always strive to improve. Happy coding!\n\nNixtla offers the possibility of assisting with stipends for computing infrastructure for our contributors. If you are interested, please join our slack and write to fede or Max.\n\nYou can find a detailed step by step buide with screen-shots below."
  },
  {
    "objectID": "docs/contribute/step-by-step.html#example-with-screen-shots",
    "href": "docs/contribute/step-by-step.html#example-with-screen-shots",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "The first thing you need to do is create a fork of the GitHub repository to your own account:\n\nYour fork on your account will look like this:\n\nIn that repository, you can make your changes and then request to have them added to the main repo.\n\n\n\nIn this tutorial, we are using Mac (also compatible with other Linux distributions). If you are a collaborator of Nixtla, you can request an AWS instance to collaborate from there. If this is the case, please reach out to Max or Fede on Slack to receive the appropriate access. We also use Visual Studio Code, which you can download from here.\nOnce the repository is created, you need to clone it to your own computer. Simply copy the repository URL from GitHub as shown below:\n\nThen open Visual Studio Code, click on “Clone Git Repository,” and paste the line you just copied into the top part of the window, as shown below:\n\nSelect the folder where you want to copy the repository:\n\nAnd choose to open the cloned repository:\n\nYou will end up with something like this:\n\n\n\n\nOpen a terminal within Visual Studio Code, as shown in the image:\n\nYou can use conda but we highly recommend using Mamba to speed up the creation of the Conda environment. To install it, simply use conda install mamba -c conda-forge in the terminal you just opened:\n\nCreate an empty environment named mlforecast with the following command: mamba create -n mlforecast python=3.10:\n\nActivate the newly created environment using conda activate mlforecast:\n\nInstall the libraries within the environment file environment.yml using mamba env update -f environment.yml:\n\nNow install the library to make interactive changes and other additional dependencies using pip install -e \".[dev]\":\n\n\n\n\nIn this section, we assume that we want to increase the default number of windows used to create prediction intervals from 2 to 3. The first thing we need to do is create a specific branch for that change using git checkout -b [new_branch] like this:\n\nOnce created, open the notebook you want to modify. In this case, it’s nbs/utils.ipynb, which contains the metadata for the prediction intervals. After opening it, click on the environment you want to use (top right) and select the mlforecast environment:\n\nNext, execute the notebook and make the necessary changes. In this case, we want to modify the PredictionIntervals class:\n\nWe will change the default value of n_window from 2 to 3:\n\nOnce you have made the change and performed any necessary validations, it’s time to convert the notebook to Python modules. To do this, simply use nbdev_export in the terminal.\nYou will see that the mlforecast/utils.py file has been modified (the changes from nbs/utils.ipynb are reflected in that module). Before committing the changes, we need to clean the notebooks using the command ./action_files/clean_nbs and verify that the linters pass using ./action_files/lint:\n\nOnce you have done the above, simply add the changes using git add nbs/utils.ipynb mlforecast/utils.py:\n\nCreate a descriptive commit message for the changes using git commit -m \"[description of changes]\":\n\nFinally, push your changes using git push:\n\n\n\n\nIn GitHub, open your repository that contains your fork of the original repo. Once inside, you will see the changes you just pushed. Click on “Compare and pull request”:\n\nInclude an appropriate title for your pull request and fill in the necessary information. Once you’re done, click on “Create pull request”.\n\nFinally, you will see something like this:"
  },
  {
    "objectID": "docs/contribute/step-by-step.html#notes",
    "href": "docs/contribute/step-by-step.html#notes",
    "title": "Step-by-step Contribution Guide",
    "section": "",
    "text": "This file was generated using this file. Please change that file if you want to enhance the document."
  },
  {
    "objectID": "docs/contribute/issues.html",
    "href": "docs/contribute/issues.html",
    "title": "Submit an Issue 📢",
    "section": "",
    "text": "To report a bug, request a feature, propose a new integration, or suggest documentation improvements, please visit the Nixtla GitHub issues page. Before submitting a new issue, kindly check if it has already been reported.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/issues.html#steps-to-submit-an-issue",
    "href": "docs/contribute/issues.html#steps-to-submit-an-issue",
    "title": "Submit an Issue 📢",
    "section": "Steps to Submit an Issue",
    "text": "Steps to Submit an Issue\nHere’s a step-by-step guide on submitting an issue to the Nixtla repository.\nVisit our GitHub issues page and click on the New issue button.\nA list of available issue types will be displayed.\n\nReporting a Bug 🐞\nSelect Report a bug and click on the Get started button.\nThe form to report the bug will appear.\n\nBegin by adding a concise, informative title.\nDescribe the bug you’ve observed. This information is required. You can also attach relevant videos or screenshots.\nIf you’re aware of what the correct behavior should be, note it down here.\nDocumenting the steps leading to the bug will be of immense help to us.\nYou can also add links, references, logs, screenshots, and so on.\n\n Please ensure that your contributions abide by the contributing guidelines and code of conduct. \nThank you for your contribution! Your report aids in refining Nixtla for current and future users.\n\n\nFeature Request 🚀\nSelect Request a feature and click the Get started button.\nThe feature request form will appear.\n\nStart with a significant, clear title.\nProvide a detailed description of the feature you want to request, along with the reasoning behind the request. This field is mandatory. Feel free to attach related videos or screenshots.\nIf you have an idea of how the feature should work, include it.\nAdditional references, links, logs, and screenshots are welcome!\n\n Please ensure that your contributions abide by the contributing guidelines and code of conduct. \nThank you for your feature request! It will help us enhance Nixtla for all users.\n\n\nSuggest Documentation Improvements ✍️\nSelect Improve our docs and click the Get started button.\nA form for suggesting improvements will appear.\n\nA clear, concise title is important.\nDescribe the improvements you believe are needed. This field is mandatory. Attach any related videos or screenshots, if necessary.\nAny additional references, links, logs, screenshots are appreciated!\n\n Please ensure that your contributions abide by the contributing guidelines and code of conduct. \nThank you for your valuable suggestions! Your input helps us refine Nixtla’s documentation.\n\n\nProposing a New Integration 🧑‍🔧\nIf you have a proposal for a new database integration or a new machine learning framework, here’s how to get started:\nSelect `\nPropose a new integration` and click the Get started button.\nA form for your proposal will appear.\n\nStart with a clear, concise title.\nDescribe your proposal and why it is needed. This field is mandatory. Feel free to attach any related videos or screenshots.\nIf you have an idea of how this integration should work, include it.\nAny additional references, links, logs, screenshots, and so on, are welcome!\n\n Please ensure that your contributions abide by the contributing guidelines and code of conduct. \nThank you for your proposal! Your suggestion helps us extend the capabilities of Nixtla."
  },
  {
    "objectID": "docs/contribute/issues.html#reviewing-issues",
    "href": "docs/contribute/issues.html#reviewing-issues",
    "title": "Submit an Issue 📢",
    "section": "Reviewing Issues",
    "text": "Reviewing Issues\n\nIssues are reviewed on a regular basis, usually every day.\nIssues will be labeled as Bug or enhancement based on their type.\nPlease be ready to respond to our feedback or questions regarding your issue.\n\n\nThis document is based on the documentation from MindsDB"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html",
    "href": "docs/tutorials/conformalprediction.html",
    "title": "Conformal Prediction",
    "section": "",
    "text": "Prerequisites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#introduction",
    "href": "docs/tutorials/conformalprediction.html#introduction",
    "title": "Conformal Prediction",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn’t tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nThe problem is that some timeseries models provide forecast distributions, but some other ones only provide point forecasts. How can we then estimate the uncertainty of predictions?\n\n\n\n\n\n\nPrediction Intervals\n\n\n\nFor models that already provide the forecast distribution, check Prediction Intervals.\n\n\n\nConformal Prediction\nFor a video introduction, see the PyData Seattle presentation.\nMulti-quantile losses and statistical models can provide provide prediction intervals, but the problem is that these are uncalibrated, meaning that the actual frequency of observations falling within the interval does not align with the confidence level associated with it. For example, a calibrated 95% prediction interval should contain the true value 95% of the time in repeated sampling. An uncalibrated 95% prediction interval, on the other hand, might contain the true value only 80% of the time, or perhaps 99% of the time. In the first case, the interval is too narrow and underestimates the uncertainty, while in the second case, it is too wide and overestimates the uncertainty.\nStatistical methods also assume normality. Here, we talk about another method called conformal prediction that doesn’t require any distributional assumptions. More information on the approach can be found in this repo owned by Valery Manokhin.\nConformal prediction intervals use cross-validation on a point forecaster model to generate the intervals. This means that no prior probabilities are needed, and the output is well-calibrated. No additional training is needed, and the model is treated as a black box. The approach is compatible with any model.\nStatsforecast now supports Conformal Prediction on all available models."
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#install-libraries",
    "href": "docs/tutorials/conformalprediction.html#install-libraries",
    "title": "Conformal Prediction",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#load-and-explore-the-data",
    "href": "docs/tutorials/conformalprediction.html#load-and-explore-the-data",
    "title": "Conformal Prediction",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we’ll load the train and the test data separately. We’ll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we’ll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(train, test, plot_random = False)"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#train-models",
    "href": "docs/tutorials/conformalprediction.html#train-models",
    "title": "Conformal Prediction",
    "section": "Train models",
    "text": "Train models\nStatsForecast can train multiple models on different time series efficiently. Most of these models can generate a probabilistic forecast, which means that they can produce both point forecasts and prediction intervals.\nFor this example, we’ll use SimpleExponentialSmoothing and ADIDA which do not provide a prediction interval natively. Thus, it makes sense to use Conformal Prediction to generate the prediction interval.\nWe’ll also show using it with ARIMA to provide prediction intervals that don’t assume normality.\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them.\n\nfrom statsforecast.models import SeasonalExponentialSmoothing, ADIDA, ARIMA\nfrom statsforecast.utils import ConformalIntervals\n\n# Create a list of models and instantiation parameters \nintervals = ConformalIntervals(h=24, n_windows=2)\n\nmodels = [\n    SeasonalExponentialSmoothing(season_length=24,alpha=0.1, prediction_intervals=intervals),\n    ADIDA(prediction_intervals=intervals),\n    ARIMA(order=(24,0,12), season_length=24, prediction_intervals=intervals),\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df=train, \n    models=models, \n    freq='H', \n)\n\nNow we’re ready to generate the forecasts and the prediction intervals. To do this, we’ll use the forecast method, which takes two arguments:\n\nh: An integer that represent the forecasting horizon. In this case, we’ll forecast the next 24 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [80, 90] # confidence levels of the prediction intervals \n\nforecasts = sf.forecast(h=24, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalES\nSeasonalES-lo-90\nSeasonalES-lo-80\nSeasonalES-hi-80\nSeasonalES-hi-90\nADIDA\nADIDA-lo-90\nADIDA-lo-80\nADIDA-hi-80\nADIDA-hi-90\nARIMA\nARIMA-lo-90\nARIMA-lo-80\nARIMA-hi-80\nARIMA-hi-90\n\n\n\n\n0\nH1\n701\n624.132690\n561.315369\n565.365356\n682.900024\n686.950012\n747.292542\n668.049988\n672.099976\n822.485107\n826.535095\n634.355164\n581.760376\n585.810364\n682.900024\n686.950012\n\n\n1\nH1\n702\n555.698181\n501.886902\n510.377441\n601.018921\n609.509460\n747.292542\n560.200012\n570.400024\n924.185059\n934.385071\n578.701355\n540.992310\n542.581909\n614.820801\n616.410400\n\n\n2\nH1\n703\n514.403015\n468.656036\n471.506042\n557.299988\n560.150024\n747.292542\n546.849976\n549.700012\n944.885071\n947.735107\n544.308960\n528.375244\n531.132568\n557.485352\n560.242676\n\n\n3\nH1\n704\n482.057892\n438.715790\n442.315796\n521.799988\n525.400024\n747.292542\n508.600006\n512.200012\n982.385071\n985.985107\n516.846619\n504.739288\n504.785309\n528.907959\n528.953979\n\n\n4\nH1\n705\n460.222534\n419.595062\n422.745056\n497.700012\n500.850006\n747.292542\n486.149994\n489.299988\n1005.285095\n1008.435059\n502.623077\n485.736938\n488.473846\n516.772339\n519.509277"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#plot-prediction-intervals",
    "href": "docs/tutorials/conformalprediction.html#plot-prediction-intervals",
    "title": "Conformal Prediction",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nHere we’ll plot the different intervals using matplotlib for one timeseries.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef _plot_fcst(fcst, train, model): \n    fig, ax = plt.subplots(1, 1, figsize = (20,7))\n    plt.plot(np.arange(0, len(train['y'])), train['y'])\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[model], label=model)\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-lo-90'], color = 'r', label='lo-90')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-hi-90'], color = 'r', label='hi-90')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-lo-80'], color = 'g', label='lo-80')\n    plt.plot(np.arange(len(train['y']), len(train['y']) + 24), fcst[f'{model}-hi-80'], color = 'g', label='hi-80')\n    plt.legend()\n\n\nid = \"H105\"\ntemp_train = train.loc[train['unique_id'] == id]\ntemp_forecast = forecasts.loc[forecasts['unique_id'] == id]\n\nThe prediction interval with the SeasonalExponentialSmoothing seen below. Even if the model generates a point forecast, we are able to get a prediction interval. The 80% prediction interval does not cross the 90% prediction interval, which is a sign that the intervals are calibrated.\n\n_plot_fcst(temp_forecast, temp_train, \"SeasonalES\")\n\n\n\n\nFor weaker fitting models, the conformal prediction interval can be larger. A better model corresponds to a narrower interval.\n\n_plot_fcst(temp_forecast, temp_train,\"ADIDA\")\n\n\n\n\nARIMA is an example of a model that provides a forecast distribution, but we can still use conformal prediction to generate the prediction interval. As mentioned earlier, this method has the benefit of not assuming normality.\n\n_plot_fcst(temp_forecast, temp_train,\"ARIMA\")"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#future-work",
    "href": "docs/tutorials/conformalprediction.html#future-work",
    "title": "Conformal Prediction",
    "section": "Future work",
    "text": "Future work\nConformal prediction has become a powerful framework for uncertainty quantification, providing well-calibrated prediction intervals without making any distributional assumptions. Its use has surged in both academia and industry over the past few years. We’ll continue working on it, and future tutorials may include:\n\nExploring larger datasets\nIncorporating industry-specific examples\nInvestigating specialized methods like the jackknife+ that are closely related to conformal prediction (for details on the jackknife+ see here).\n\nIf you’re interested in any of these, or in any other related topic, please let us know by opening an issue on GitHub"
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#acknowledgements",
    "href": "docs/tutorials/conformalprediction.html#acknowledgements",
    "title": "Conformal Prediction",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Kevin Kho for writing this tutorial, and Valeriy Manokhin for his expertise on conformal prediction, as well as for promoting this work."
  },
  {
    "objectID": "docs/tutorials/conformalprediction.html#references",
    "href": "docs/tutorials/conformalprediction.html#references",
    "title": "Conformal Prediction",
    "section": "References",
    "text": "References\nManokhin, Valery. (2022). Machine Learning for Probabilistic Prediction. 10.5281/zenodo.6727505."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html",
    "href": "docs/tutorials/garch_tutorial.html",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#introduction",
    "href": "docs/tutorials/garch_tutorial.html#introduction",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Introduction",
    "text": "Introduction\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is used for time series that exhibit non-constant volatility over time. Here volatility refers to the conditional standard deviation. The GARCH(p,q) model is given by\n\\[\\begin{equation}\ny_t = v_t \\sigma_t\n\\end{equation}\\]\nwhere \\(v_t\\) is independent and identically distributed with zero mean and unit variance, and \\(\\sigma_t\\) evolves according to\n\\[\\begin{equation}\n\\sigma_t^2 = w + \\sum_{i=1}^p \\alpha_i y^2_{t-i} + \\sum_{j=1}^q \\beta_j \\sigma_{t-j}^2\n\\end{equation}\\]\nThe coefficients in the equation above must satisfy the following conditions:\n\n\\(w&gt;0\\), \\(\\alpha_i \\geq 0\\) for all \\(i\\), and \\(\\beta_j \\geq 0\\) for all \\(j\\)\n\\(\\sum_{k=1}^{max(p,q)} \\alpha_k + \\beta_k &lt; 1\\). Here it is assumed that \\(\\alpha_i=0\\) for \\(i&gt;p\\) and \\(\\beta_j=0\\) for \\(j&gt;q\\).\n\nA particular case of the GARCH model is the ARCH model, in which \\(q=0\\). Both models are commonly used in finance to model the volatility of stock prices, exchange rates, interest rates, and other financial instruments. They’re also used in risk management to estimate the probability of large variations in the price of financial assets.\nBy the end of this tutorial, you’ll have a good understanding of how to implement a GARCH or an ARCH model in StatsForecast and how they can be used to analyze and predict financial time series data.\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPerform time series cross-validation\nEvaluate results\nForecast volatility\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#install-libraries",
    "href": "docs/tutorials/garch_tutorial.html#install-libraries",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#load-and-explore-the-data",
    "href": "docs/tutorials/garch_tutorial.html#load-and-explore-the-data",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nIn this tutorial, we’ll use the last 5 years of prices from the S&P 500 and several publicly traded companies. The data can be downloaded from Yahoo! Finance using yfinance. To install it, use pip install yfinance.\n\npip install yfinance\n\nWe’ll also need pandas to deal with the dataframes.\n\nimport yfinance as yf\nimport pandas as pd \n\ntickers = ['SPY', 'MSFT', 'AAPL', 'GOOG', 'AMZN', 'TSLA', 'NVDA', 'META', 'NKE', 'NFLX'] \ndf = yf.download(tickers, start = '2018-01-01', end = '2022-12-31', interval='1mo') # use monthly prices\ndf.head()\n\n[*********************100%***********************]  10 of 10 completed\n\n\n\n\n\n\n\n\n\nAdj Close\n...\nVolume\n\n\n\nAAPL\nAMZN\nGOOG\nMETA\nMSFT\nNFLX\nNKE\nNVDA\nSPY\nTSLA\n...\nAAPL\nAMZN\nGOOG\nMETA\nMSFT\nNFLX\nNKE\nNVDA\nSPY\nTSLA\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-01\n39.741604\n72.544502\n58.497002\n186.889999\n89.248772\n270.299988\n64.929787\n60.830006\n258.821686\n23.620667\n...\n2638717600\n1927424000\n574768000\n495655700\n574258400\n238377600\n157812200\n1145621600\n1985506700\n1864072500\n\n\n2018-02-01\n42.279007\n75.622498\n55.236500\n178.320007\n88.083969\n291.380005\n63.797192\n59.889591\n249.410812\n22.870667\n...\n3711577200\n2755680000\n847640000\n516251600\n725663300\n184585800\n160317000\n1491552800\n2923722000\n1637850000\n\n\n2018-03-01\n39.987053\n72.366997\n51.589500\n159.789993\n86.138298\n295.350006\n63.235649\n57.348976\n241.606750\n17.742001\n...\n2854910800\n2608002000\n907066000\n996201700\n750754800\n263449400\n174066700\n1411844000\n2323561800\n2359027500\n\n\n2018-04-01\n39.386456\n78.306503\n50.866501\n172.000000\n88.261810\n312.459991\n65.288467\n55.692314\n243.828018\n19.593332\n...\n2664617200\n2598392000\n834318000\n750072700\n668130700\n262006000\n158981900\n1114400800\n1998466500\n2854662000\n\n\n2018-05-01\n44.536777\n81.481003\n54.249500\n191.779999\n93.282692\n351.600006\n68.543846\n62.450180\n249.755264\n18.982000\n...\n2483905200\n1432310000\n636988000\n401144100\n509417900\n142050800\n129566300\n1197824000\n1606397200\n2333671500\n\n\n\n\n5 rows × 60 columns\n\n\n\nThe data downloaded includes different prices. We’ll use the adjusted closing price, which is the closing price after accounting for any corporate actions like stock splits or dividend distributions. It is also the price that is used to examine historical returns.\nNotice that the dataframe that yfinance returns has a MultiIndex, so we need to select both the adjusted price and the tickers.\n\ndf = df.loc[:, (['Adj Close'], tickers)]\ndf.columns = df.columns.droplevel() # drop MultiIndex\ndf = df.reset_index()\ndf.head()\n\n\n\n\n\n\n\n\nDate\nSPY\nMSFT\nAAPL\nGOOG\nAMZN\nTSLA\nNVDA\nMETA\nNKE\nNFLX\n\n\n\n\n0\n2018-01-01\n258.821686\n89.248772\n39.741604\n58.497002\n72.544502\n23.620667\n60.830006\n186.889999\n64.929787\n270.299988\n\n\n1\n2018-02-01\n249.410812\n88.083969\n42.279007\n55.236500\n75.622498\n22.870667\n59.889591\n178.320007\n63.797192\n291.380005\n\n\n2\n2018-03-01\n241.606750\n86.138298\n39.987053\n51.589500\n72.366997\n17.742001\n57.348976\n159.789993\n63.235649\n295.350006\n\n\n3\n2018-04-01\n243.828018\n88.261810\n39.386456\n50.866501\n78.306503\n19.593332\n55.692314\n172.000000\n65.288467\n312.459991\n\n\n4\n2018-05-01\n249.755264\n93.282692\n44.536777\n54.249500\n81.481003\n18.982000\n62.450180\n191.779999\n68.543846\n351.600006\n\n\n\n\n\n\n\nThe input to StatsForecast is a dataframe in long format with three columns: unique_id, ds and y:\n\nunique_id: (string, int or category) A unique identifier for the series.\nds: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\ny: (numeric) The measurement we wish to forecast.\n\nHence, we need to reshape the data. We’ll do this by creating a new dataframe called price.\n\nprices = df.melt(id_vars = 'Date')\nprices = prices.rename(columns={'Date': 'ds', 'variable': 'unique_id', 'value': 'y'})\nprices = prices[['unique_id', 'ds', 'y']]\nprices\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nSPY\n2018-01-01\n258.821686\n\n\n1\nSPY\n2018-02-01\n249.410812\n\n\n2\nSPY\n2018-03-01\n241.606750\n\n\n3\nSPY\n2018-04-01\n243.828018\n\n\n4\nSPY\n2018-05-01\n249.755264\n\n\n...\n...\n...\n...\n\n\n595\nNFLX\n2022-08-01\n223.559998\n\n\n596\nNFLX\n2022-09-01\n235.440002\n\n\n597\nNFLX\n2022-10-01\n291.880005\n\n\n598\nNFLX\n2022-11-01\n305.529999\n\n\n599\nNFLX\n2022-12-01\n294.880005\n\n\n\n\n600 rows × 3 columns\n\n\n\nWe can plot this series using the plot method of the StatsForecast class.\n\nfrom statsforecast import StatsForecast\n\n/home/ubuntu/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nStatsForecast.plot(prices)\n\n\n                                                \n\n\nWith the prices, we can compute the logarithmic returns of the S&P 500 and the publicly traded companies. This is the variable we’re interested in since it’s likely to work well with the GARCH framework. The logarithmic return is given by\n\\(return_t = log \\big( \\frac{price_t}{price_{t-1}} \\big)\\)\nWe’ll compute the returns on the price dataframe and then we’ll create a return dataframe with StatsForecast’s format. To do this, we’ll need numpy.\n\nimport numpy as np \nprices['rt'] = prices['y'].div(prices.groupby('unique_id')['y'].shift(1))\nprices['rt'] = np.log(prices['rt'])\n\nreturns = prices[['unique_id', 'ds', 'rt']]\nreturns = returns.rename(columns={'rt':'y'})\nreturns\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nSPY\n2018-01-01\nNaN\n\n\n1\nSPY\n2018-02-01\n-0.037038\n\n\n2\nSPY\n2018-03-01\n-0.031790\n\n\n3\nSPY\n2018-04-01\n0.009152\n\n\n4\nSPY\n2018-05-01\n0.024018\n\n\n...\n...\n...\n...\n\n\n595\nNFLX\n2022-08-01\n-0.005976\n\n\n596\nNFLX\n2022-09-01\n0.051776\n\n\n597\nNFLX\n2022-10-01\n0.214887\n\n\n598\nNFLX\n2022-11-01\n0.045705\n\n\n599\nNFLX\n2022-12-01\n-0.035479\n\n\n\n\n600 rows × 3 columns\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the order of the data is very small (say \\(&lt;1e-5\\)), scipy.optimize.minimize might not terminate successfully. In this case, transform the data to a higher order, generate the GARCH or ARCH model, and then transform the data back.\n\n\n\nStatsForecast.plot(returns)\n\n\n                                                \n\n\nFrom this plot, we can see that the returns seem suited for the GARCH framework, since large shocks tend to be followed by other large shocks. This doesn’t mean that after every large shock we should expect another one; merely that the probability of a large variance is greater than the probability of a small one."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#train-models",
    "href": "docs/tutorials/garch_tutorial.html#train-models",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Train models",
    "text": "Train models\nWe first need to import the GARCH and the ARCH models from statsforecast.models, and then we need to fit them by instantiating a new StatsForecast object. Notice that we’ll be using different values of \\(p\\) and \\(q\\). In the next section, we’ll determine which ones produce the most accurate model using cross-validation. We’ll also import the Naive model since we’ll use it as a baseline.\n\nfrom statsforecast.models import (\n    GARCH, \n    ARCH, \n    Naive\n)\n\nmodels = [ARCH(1), \n          ARCH(2), \n          GARCH(1,1),\n          GARCH(1,2),\n          GARCH(2,2),\n          GARCH(2,1),\n          Naive()\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. Here we’ll use MS, which correspond to the start of the month. You can see the list of panda’s available frequencies here.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = returns, \n    models = models, \n    freq = 'MS',\n    n_jobs = -1\n)"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#perform-time-series-cross-validation",
    "href": "docs/tutorials/garch_tutorial.html#perform-time-series-cross-validation",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it. Here we’ll use StatsForercast’s cross-validation method to determine the most accurate model for the S&P 500 and the companies selected.\nThis method takes the following arguments:\n\ndf: The dataframe with the training data.\nh (int): represents the h steps into the future that will be forecasted.\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we’ll use 4 windows of 3 months, or all the quarters in a year.\n\ncrossvalidation_df = sf.cross_validation(\n    df = returns,\n    h = 3,\n    step_size = 3,\n    n_windows = 4\n  )\n\nThe crossvalidation_df object ia a dataframe with the following columns:\n\nunique_id: index.\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvalidation_df = crossvalidation_df.reset_index()\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True)\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nactual\nARCH(1)\nARCH(2)\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nNaive\n\n\n\n\n0\nAAPL\n2022-01-01\n2021-12-01\n-0.015837\n0.142416\n0.144013\n0.142951\n0.226098\n0.141690\n0.144018\n0.073061\n\n\n1\nAAPL\n2022-02-01\n2021-12-01\n-0.056855\n-0.056896\n-0.057158\n-0.056387\n-0.087001\n-0.058787\n-0.057161\n0.073061\n\n\n2\nAAPL\n2022-03-01\n2021-12-01\n0.057156\n-0.045899\n-0.046478\n-0.047512\n-0.073625\n-0.045714\n-0.046479\n0.073061\n\n\n3\nAAPL\n2022-04-01\n2022-03-01\n-0.102178\n0.138661\n0.140211\n0.136213\n0.136124\n0.136127\n0.136546\n0.057156\n\n\n4\nAAPL\n2022-05-01\n2022-03-01\n-0.057505\n-0.056013\n-0.056268\n-0.054599\n-0.057080\n-0.057085\n-0.053791\n0.057156\n\n\n\n\n\n\n\n\nStatsForecast.plot(returns, crossvalidation_df.drop(['cutoff', 'actual'], axis=1))\n\n\n                                                \n\n\nA tutorial on cross-validation can be found here."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#evaluate-results",
    "href": "docs/tutorials/garch_tutorial.html#evaluate-results",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Evaluate results",
    "text": "Evaluate results\nTo compute the accuracy of the forecasts, we’ll use the mean average error (mae), which is the sum of the absolute errors divided by the number of forecasts. There’s an implementation of MAE on datasetsforecast, so we’ll install it and then import the mae function.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.losses import mae\n\nThe MAE needs to be computed for every window and then it needs to be averaged across all of them. To do this, we’ll create the following function.\n\ndef compute_cv_mae(crossvalidation_df):\n    \"\"\"Compute MAE for all models generated\"\"\"\n    res = {}\n    for mod in models: \n        res[mod] = mae(crossvalidation_df['actual'], crossvalidation_df[str(mod)])\n    return pd.Series(res)\n\n\nmae_cv = crossvalidation_df.groupby(['unique_id', 'cutoff']).apply(compute_cv_mae)\n\nmae = mae_cv.groupby('unique_id').mean()\nmae.style.highlight_min(color = 'lightblue', axis = 1)\n\n\n\n\n\n\n \nARCH(1)\nARCH(2)\nGARCH(1,1)\nGARCH(1,2)\nGARCH(2,2)\nGARCH(2,1)\nNaive\n\n\nunique_id\n \n \n \n \n \n \n \n\n\n\n\nAAPL\n0.068537\n0.068927\n0.068929\n0.085630\n0.072519\n0.068556\n0.110426\n\n\nAMZN\n0.118612\n0.126182\n0.118858\n0.125470\n0.109913\n0.109912\n0.115189\n\n\nGOOG\n0.093849\n0.093752\n0.099593\n0.115136\n0.094648\n0.113645\n0.083233\n\n\nMETA\n0.198333\n0.198891\n0.199617\n0.199712\n0.199708\n0.198890\n0.185346\n\n\nMSFT\n0.080022\n0.097301\n0.082183\n0.072765\n0.073006\n0.080494\n0.086951\n\n\nNFLX\n0.159384\n0.159523\n0.219658\n0.231798\n0.230077\n0.224103\n0.167421\n\n\nNKE\n0.107842\n0.114263\n0.103097\n0.107180\n0.107179\n0.107019\n0.160405\n\n\nNVDA\n0.189462\n0.207875\n0.199004\n0.196172\n0.211928\n0.211928\n0.215289\n\n\nSPY\n0.058513\n0.065498\n0.058700\n0.057051\n0.057051\n0.058526\n0.089012\n\n\nTSLA\n0.192003\n0.192620\n0.190225\n0.192353\n0.191620\n0.191418\n0.218857\n\n\n\n\n\nHence, the most accurate model to describe the logarithmic returns of Apple’s stock is a GARCH(2,1), and so on."
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#forecast-volatility",
    "href": "docs/tutorials/garch_tutorial.html#forecast-volatility",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "Forecast volatility",
    "text": "Forecast volatility\nWe can now generate a forecast for the next quarter. To do this, we’ll use the forecast method, which requieres the following arguments:\n\nh: (int) The forecasting horizon.\nlevel: (list[float]) The confidence levels of the prediction intervals\nfitted : (bool = False) Returns insample predictions.\n\n\nlevels = [80, 95] # confidence levels for the prediction intervals \n\nforecasts = sf.forecast(h=3, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nARCH(1)\nARCH(1)-lo-95\nARCH(1)-lo-80\nARCH(1)-hi-80\nARCH(1)-hi-95\nARCH(2)\nARCH(2)-lo-95\nARCH(2)-lo-80\n...\nGARCH(2,1)\nGARCH(2,1)-lo-95\nGARCH(2,1)-lo-80\nGARCH(2,1)-hi-80\nGARCH(2,1)-hi-95\nNaive\nNaive-lo-80\nNaive-lo-95\nNaive-hi-80\nNaive-hi-95\n\n\n\n\n0\nAAPL\n2023-01-01\n0.150457\n0.133641\n0.139462\n0.161452\n0.167273\n0.150166\n0.133415\n0.139213\n...\n0.147610\n0.131424\n0.137027\n0.158193\n0.163795\n-0.128762\n-0.284463\n-0.366886\n0.026939\n0.109362\n\n\n1\nAAPL\n2023-02-01\n-0.056942\n-0.073923\n-0.068046\n-0.045839\n-0.039961\n-0.057209\n-0.074349\n-0.068417\n...\n-0.059511\n-0.078059\n-0.071639\n-0.047384\n-0.040964\n-0.128762\n-0.348956\n-0.465520\n0.091433\n0.207997\n\n\n2\nAAPL\n2023-03-01\n-0.048390\n-0.064842\n-0.059148\n-0.037633\n-0.031939\n-0.049279\n-0.066340\n-0.060435\n...\n-0.054537\n-0.075435\n-0.068201\n-0.040874\n-0.033640\n-0.128762\n-0.398444\n-0.541205\n0.140920\n0.283681\n\n\n3\nAMZN\n2023-01-01\n0.152158\n0.134960\n0.140913\n0.163404\n0.169357\n0.148659\n0.132243\n0.137925\n...\n0.148597\n0.132195\n0.137872\n0.159322\n0.165000\n-0.139141\n-0.315716\n-0.409190\n0.037435\n0.130909\n\n\n4\nAMZN\n2023-02-01\n-0.057306\n-0.074504\n-0.068551\n-0.046060\n-0.040107\n-0.061187\n-0.080794\n-0.074007\n...\n-0.069302\n-0.094455\n-0.085749\n-0.052856\n-0.044150\n-0.139141\n-0.388856\n-0.521048\n0.110575\n0.242767\n\n\n\n\n5 rows × 37 columns\n\n\n\nWith the results of the previous section, we can choose the best model for the S&P 500 and the companies selected. Some of the plots are shown below. Notice that we’re using somo additional arguments in the plot method:\n\nlevel: (list[int]) The confidence levels for the prediction intervals (this was already defined).\nunique_ids: (list[str, int or category]) The ids to plot.\nmodels: (list(str)). The model to plot. In this case, is the model selected by cross-validation.\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['AAPL'], models = ['GARCH(2,1)'])\n\n\n                                                \n\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['MSFT'], models = ['ARCH(2)'])\n\n\n                                                \n\n\n\nStatsForecast.plot(returns, forecasts, level=levels, unique_ids = ['NFLX'], models = ['ARCH(1)'])"
  },
  {
    "objectID": "docs/tutorials/garch_tutorial.html#references",
    "href": "docs/tutorials/garch_tutorial.html#references",
    "title": "Volatility forecasting (GARCH & ARCH)",
    "section": "References",
    "text": "References\n\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nHamilton, J. D. (1994). Time series analysis. Princeton university press.\nTsay, R. S. (2005). Analysis of financial time series. John wiley & sons."
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html",
    "href": "docs/tutorials/intermittentdata.html",
    "title": "Intermittent or Sparse Data",
    "section": "",
    "text": "Intermittent or sparse data has very few non-zero observations. This type of data is hard to forecast because the zero values increase the uncertainty about the underlying patterns in the data. Furthermore, once a non-zero observation occurs, there can be considerable variation in its size. Intermittent time series are common in many industries, including finance, retail, transportation, and energy. Given the ubiquity of this type of series, special methods have been developed to forecast them. The first was from Croston (1972), followed by several variants and by different aggregation frameworks.\nStatsForecast has implemented several models to forecast intermittent time series. By the end of this tutorial, you’ll have a good understanding of these models and how to use them.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#install-libraries",
    "href": "docs/tutorials/intermittentdata.html#install-libraries",
    "title": "Intermittent or Sparse Data",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#load-and-explore-the-data",
    "href": "docs/tutorials/intermittentdata.html#load-and-explore-the-data",
    "title": "Intermittent or Sparse Data",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use a subset of the M5 Competition dataset. Each time series represents the unit sales of a particular product in a given Walmart store. At this level (product-store), most of the data is intermittent. We first need to import the data, and to do that, we’ll need datasetsforecast.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load(). It requieres the following argument: - directory: (str) The directory where the data will be downloaded.\nThis function returns multiple outputs, but only the first one with the unit sales is needed.\n\ndf_total, *_ = M5.load('./data')\n\n\ndf_total.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\nFrom this dataset, we’ll select the first 8 time series. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: (bool = True) Plots the time series randomly.\nmax_insample_length: (int) The maximum number of train/insample observations to be plotted.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nHere we only plotted the last 100 observations, but we can visualize the complete history by removing max_insample_length. From these plots, we can confirm that the data is indeed intermittent since it has multiple periods with zero sales. In fact, in all cases but one, the median value is zero.\n\ndf.groupby('unique_id')[['y']].median().query('unique_id in @uids')\n\n\n\n\n\n\n\n\ny\n\n\nunique_id\n\n\n\n\n\nFOODS_1_001_CA_1\n0.0\n\n\nFOODS_1_001_CA_2\n1.0\n\n\nFOODS_1_001_CA_3\n0.0\n\n\nFOODS_1_001_CA_4\n0.0\n\n\nFOODS_1_001_TX_1\n0.0\n\n\nFOODS_1_001_TX_2\n0.0\n\n\nFOODS_1_001_TX_3\n0.0\n\n\nFOODS_1_001_WI_1\n0.0"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#train-models-for-intermittent-data",
    "href": "docs/tutorials/intermittentdata.html#train-models-for-intermittent-data",
    "title": "Intermittent or Sparse Data",
    "section": "Train models for intermittent data",
    "text": "Train models for intermittent data\nBefore training any model, we need to separate the data in a train and a test set. The M5 Competition used the last 28 days as test set, so we’ll do the same.\n\ndates = df['ds'].unique()[-28:] # last 28 days\n\ntrain = df.query('ds not in @dates')\ntest = df.query('ds in @dates')\n\nStatsForecast has efficient implementations of multiple models for intermittent data. The complete list of models available is here. In this notebook, we’ll use:\n\nAgregate-Dissagregate Intermittent Demand Approach (ADIDA)\nCroston Classic\nIntermittent Multiple Aggregation Prediction Algorithm (IMAPA)\nTeunter-Syntetos-Babai (TSB)\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them.\n\nfrom statsforecast.models import (\n    ADIDA,\n    CrostonClassic, \n    IMAPA, \n    TSB\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    ADIDA(), \n    CrostonClassic(), \n    IMAPA(), \n    TSB(alpha_d = 0.2, alpha_p = 0.2)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = train, \n    models = models, \n    freq = 'D', \n    n_jobs = -1\n)\n\nNow we’re ready to generate the forecast. To do this, we’ll use the forecast method, which requires the forecasting horizon (in this case, 28 days) as argument.\nThe models for intermittent series that are currently available in StatsForecast can only generate point-forecasts. If prediction intervals are needed, then a probabilisitic model should be used.\n\nhorizon = 28 \nforecasts = sf.forecast(h=horizon)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nADIDA\nCrostonClassic\nIMAPA\nTSB\n\n\n\n\n0\nFOODS_1_001_CA_1\n2016-05-23\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n1\nFOODS_1_001_CA_1\n2016-05-24\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n2\nFOODS_1_001_CA_1\n2016-05-25\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n3\nFOODS_1_001_CA_1\n2016-05-26\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n4\nFOODS_1_001_CA_1\n2016-05-27\n0.791852\n0.898247\n0.705835\n0.434313\n\n\n\n\n\n\n\nFinally, we’ll merge the forecast with the actual values.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "href": "docs/tutorials/intermittentdata.html#plot-forecasts-and-compute-accuracy",
    "title": "Intermittent or Sparse Data",
    "section": "Plot forecasts and compute accuracy",
    "text": "Plot forecasts and compute accuracy\nWe can generate plots using the plot described above.\n\nStatsForecast.plot(train, test, plot_random = False, max_insample_length = 100)\n\n\n                                                \n\n\nTo compute the accuracy of the forecasts, we’ll use the Mean Average Error (MAE), which is the sum of the absolute errors divided by the number of forecasts. We’ll create a function to compute the MAE, and for that, we’ll need to import numpy.\n\nimport numpy as np \n\ndef mae(y_hat, y_true):\n    return np.mean(np.abs(y_hat-y_true))\n\n\ny_true = test['y'].values\nadida_preds = test['ADIDA'].values\ncroston_preds = test['CrostonClassic'].values\nimapa_preds = test['IMAPA'].values\ntsb_preds = test['TSB'].values\n\nprint('ADIDA MAE: \\t %0.3f' % mae(adida_preds, y_true))\nprint('Croston Classic MAE: \\t %0.3f' % mae(croston_preds, y_true))\nprint('IMAPA MAE: \\t %0.3f' % mae(imapa_preds, y_true))\nprint('TSB   MAE: \\t %0.3f' % mae(tsb_preds, y_true))\n\nADIDA MAE:   0.949\nCroston Classic MAE:     0.944\nIMAPA MAE:   0.957\nTSB   MAE:   1.023\n\n\nHence, on average, the forecasts are one unit off."
  },
  {
    "objectID": "docs/tutorials/intermittentdata.html#references",
    "href": "docs/tutorials/intermittentdata.html#references",
    "title": "Intermittent or Sparse Data",
    "section": "References",
    "text": "References\n Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303."
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html",
    "href": "docs/tutorials/uncertaintyintervals.html",
    "title": "Probabilistic Forecasting",
    "section": "",
    "text": "Prerequisites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#introduction",
    "href": "docs/tutorials/uncertaintyintervals.html#introduction",
    "title": "Probabilistic Forecasting",
    "section": "Introduction",
    "text": "Introduction\nWhen we generate a forecast, we usually produce a single value known as the point forecast. This value, however, doesn’t tell us anything about the uncertainty associated with the forecast. To have a measure of this uncertainty, we need prediction intervals.\nA prediction interval is a range of values that the forecast can take with a given probability. Hence, a 95% prediction interval should contain a range of values that include the actual future value with probability 95%. Probabilistic forecasting aims to generate the full forecast distribution. Point forecasting, on the other hand, usually returns the mean or the median or said distribution. However, in real-world scenarios, it is better to forecast not only the most probable future outcome, but many alternative outcomes as well.\nStatsForecast has many models that can generate point forecasts. It also has probabilistic models than generate the same point forecasts and their prediction intervals. These models are stochastic data generating processes that can produce entire forecast distributions. By the end of this tutorial, you’ll have a good understanding of the probabilistic models available in StatsForecast and will be able to use them to generate point forecasts and prediction intervals. Furthermore, you’ll also learn how to generate plots with the historical data, the point forecasts, and the prediction intervals.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the terms are often confused, prediction intervals are not the same as confidence intervals.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn practice, most prediction intervals are too narrow since models do not account for all sources of uncertainty. A discussion about this can be found here.\n\n\nOutline:\n\nInstall libraries\nLoad and explore the data\nTrain models\nPlot prediction intervals\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#install-libraries",
    "href": "docs/tutorials/uncertaintyintervals.html#install-libraries",
    "title": "Probabilistic Forecasting",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#load-and-explore-the-data",
    "href": "docs/tutorials/uncertaintyintervals.html#load-and-explore-the-data",
    "title": "Probabilistic Forecasting",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset from the M4 Competition. We first need to download the data from a URL and then load it as a pandas dataframe. Notice that we’ll load the train and the test data separately. We’ll also rename the y column of the test data as y_test.\n\nimport pandas as pd \n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny_test\n\n\n\n\n0\nH1\n701\n619.0\n\n\n1\nH1\n702\n565.0\n\n\n2\nH1\n703\n532.0\n\n\n3\nH1\n704\n495.0\n\n\n4\nH1\n705\n481.0\n\n\n\n\n\n\n\nSince the goal of this notebook is to generate prediction intervals, we’ll only use the first 8 series of the dataset to reduce the total computational time.\n\nn_series = 8 \nuids = train['unique_id'].unique()[:n_series] # select first n_series of the dataset\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\nWe can plot these series using the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nplot_random: bool = True. Plots the time series randomly.\nmodels: List[str]. A list with the models we want to plot.\nlevel: List[float]. A list with the prediction intervals we want to plot.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(train, test, plot_random = False)\n\n/Users/fedex/projects/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#train-models",
    "href": "docs/tutorials/uncertaintyintervals.html#train-models",
    "title": "Probabilistic Forecasting",
    "section": "Train models",
    "text": "Train models\nStatsForecast can train multiple models on different time series efficiently. Most of these models can generate a probabilistic forecast, which means that they can produce both point forecasts and prediction intervals.\nFor this example, we’ll use AutoETS and the following baseline models:\n\nHistoricAverage\nNaive\nRandomWalkWithDrift\nSeasonalNaive\n\nTo use these models, we first need to import them from statsforecast.models and then we need to instantiate them. Given that we’re working with hourly data, we need to set seasonal_length=24 in the models that requiere this parameter.\n\nfrom statsforecast.models import (\n    AutoETS, \n    HistoricAverage, \n    Naive, \n    RandomWalkWithDrift, \n    SeasonalNaive\n)\n\n# Create a list of models and instantiation parameters \nmodels = [\n    AutoETS(season_length=24),\n    HistoricAverage(), \n    Naive(), \n    RandomWalkWithDrift(), \n    SeasonalNaive(season_length=24)\n]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\n\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df=train, \n    models=models, \n    freq='H', \n    n_jobs=-1\n)\n\nNow we’re ready to generate the point forecasts and the prediction intervals. To do this, we’ll use the forecast method, which takes two arguments:\n\nh: An integer that represent the forecasting horizon. In this case, we’ll forecast the next 48 hours.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nlevels = [80, 90, 95, 99] # confidence levels of the prediction intervals \n\nforecasts = sf.forecast(h=48, level=levels)\nforecasts = forecasts.reset_index()\nforecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoETS\nAutoETS-lo-99\nAutoETS-lo-95\nAutoETS-lo-90\nAutoETS-lo-80\nAutoETS-hi-80\nAutoETS-hi-90\nAutoETS-hi-95\n...\nRWD-hi-99\nSeasonalNaive\nSeasonalNaive-lo-80\nSeasonalNaive-lo-90\nSeasonalNaive-lo-95\nSeasonalNaive-lo-99\nSeasonalNaive-hi-80\nSeasonalNaive-hi-90\nSeasonalNaive-hi-95\nSeasonalNaive-hi-99\n\n\n\n\n0\nH1\n701\n631.889587\n533.371826\n556.926819\n568.978882\n582.874084\n680.905090\n694.800354\n706.852356\n...\n789.416626\n691.0\n582.823792\n552.157349\n525.558777\n473.573395\n799.176208\n829.842651\n856.441223\n908.426575\n\n\n1\nH1\n702\n559.750854\n460.738586\n484.411835\n496.524353\n510.489288\n609.012329\n622.977295\n635.089844\n...\n833.254150\n618.0\n509.823822\n479.157379\n452.558807\n400.573395\n726.176208\n756.842651\n783.441223\n835.426575\n\n\n2\nH1\n703\n519.235474\n419.731232\n443.522095\n455.694794\n469.729156\n568.741821\n582.776123\n594.948853\n...\n866.990601\n563.0\n454.823822\n424.157379\n397.558807\n345.573395\n671.176208\n701.842651\n728.441223\n780.426575\n\n\n3\nH1\n704\n486.973358\n386.979523\n410.887451\n423.120056\n437.223480\n536.723267\n550.826660\n563.059265\n...\n895.510132\n529.0\n420.823822\n390.157379\n363.558807\n311.573395\n637.176208\n667.842651\n694.441223\n746.426575\n\n\n4\nH1\n705\n464.697357\n364.216339\n388.240753\n400.532959\n414.705078\n514.689636\n528.861755\n541.153992\n...\n920.702881\n504.0\n395.823822\n365.157379\n338.558807\n286.573395\n612.176208\n642.842651\n669.441223\n721.426575\n\n\n\n\n5 rows × 47 columns\n\n\n\nWe’ll now merge the forecasts and their prediction intervals with the test set. This will allow us generate the plots of each probabilistic model.\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])"
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#plot-prediction-intervals",
    "href": "docs/tutorials/uncertaintyintervals.html#plot-prediction-intervals",
    "title": "Probabilistic Forecasting",
    "section": "Plot prediction intervals",
    "text": "Plot prediction intervals\nTo plot the point and the prediction intervals, we’ll use the statsforecast.plot method again. Notice that now we also need to specify the model and the levels that we want to plot.\n\nAutoETS\n\nsf.plot(train, test, plot_random = False, models=['AutoETS'], level=levels)\n\n\n                                                \n\n\n\n\nHistoric Average\n\nsf.plot(train, test, plot_random = False, models=['HistoricAverage'], level=levels)\n\n\n                                                \n\n\n\n\nNaive\n\nsf.plot(train, test, plot_random = False, models=['Naive'], level=levels)\n\n\n                                                \n\n\n\n\nRandom Walk with Drift\n\nsf.plot(train, test, plot_random = False, models=['RWD'], level=levels)\n\n\n                                                \n\n\n\n\nSeasonal Naive\n\nsf.plot(train, test, plot_random = False, models=['SeasonalNaive'], level=levels)\n\n\n                                                \n\n\nFrom these plots, we can conclude that the uncertainty around each forecast varies according to the model that is being used. For the same time series, one model can predict a wider range of possible future values than others."
  },
  {
    "objectID": "docs/tutorials/uncertaintyintervals.html#references",
    "href": "docs/tutorials/uncertaintyintervals.html#references",
    "title": "Probabilistic Forecasting",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, The Statistical Forecasting Perspective”."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html",
    "href": "docs/tutorials/crossvalidation.html",
    "title": "Cross validation",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#introduction",
    "href": "docs/tutorials/crossvalidation.html#introduction",
    "title": "Cross validation",
    "section": "Introduction",
    "text": "Introduction\nTime series cross-validation is a method for evaluating how a model would have performed in the past. It works by defining a sliding window across the historical data and predicting the period following it.\n\nStatsforecast has an implementation of time series cross-validation that is fast and easy to use. This implementation makes cross-validation a distributed operation, which makes it less time-consuming. In this notebook, we’ll use it on a subset of the M4 Competition hourly dataset.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nPerform time series cross-validation\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#install-libraries",
    "href": "docs/tutorials/crossvalidation.html#install-libraries",
    "title": "Cross validation",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages with pip install statsforecast\n\npip install statsforecast\n\n\nfrom statsforecast import StatsForecast # required to instantiate StastForecast object and use cross-validation method"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#load-and-explore-the-data",
    "href": "docs/tutorials/crossvalidation.html#load-and-explore-the-data",
    "title": "Cross validation",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nAs stated in the introduction, we’ll use the M4 Competition hourly dataset. We’ll first import the data from an URL using pandas.\n\nimport pandas as pd \n\nY_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet') # load the data \nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThe input to StatsForecast is a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int, or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThe data in this example already has this format, so no changes are needed.\nTo keep the time required to execute this notebook to a minimum, we’ll only use one time series from the data, namely the one with unique_id == 'H1'. However, you can use as many as you want, with no additional changes to the code needed.\n\ndf = Y_df[Y_df['unique_id'] == 'H1'] # select time series\n\nWe can plot the time series we’ll work with using StatsForecast.plot method.\n\nStatsForecast.plot(df)"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#train-model",
    "href": "docs/tutorials/crossvalidation.html#train-model",
    "title": "Cross validation",
    "section": "Train model",
    "text": "Train model\nFor this example, we’ll use StastForecast AutoETS. We first need to import it from statsforecast.models and then we need to instantiate a new StatsForecast object.\nThe StatsForecast object has the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. See panda’s available frequencies.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame df.\n\nfrom statsforecast.models import AutoETS\n\nmodels = [AutoETS(season_length = 24)]\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)"
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#perform-time-series-cross-validation",
    "href": "docs/tutorials/crossvalidation.html#perform-time-series-cross-validation",
    "title": "Cross validation",
    "section": "Perform time series cross-validation",
    "text": "Perform time series cross-validation\nOnce the StatsForecastobject has been instantiated, we can use the cross_validation method, which takes the following arguments:\n\ndf: training data frame with StatsForecast format\nh (int): represents the h steps into the future that will be forecasted\nstep_size (int): step size between each window, meaning how often do you want to run the forecasting process.\nn_windows (int): number of windows used for cross-validation, meaning the number of forecasting processes in the past you want to evaluate.\n\nFor this particular example, we’ll use 3 windows of 24 hours.\n\ncrossvalidation_df = sf.cross_validation(\n    df = df,\n    h = 24,\n    step_size = 24,\n    n_windows = 3\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id: index. If you dont like working with index just run crossvalidation_df.resetindex()\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n\n\nH1\n677\n676\n691.0\n677.761047\n\n\nH1\n678\n676\n618.0\n607.817871\n\n\nH1\n679\n676\n563.0\n569.437744\n\n\nH1\n680\n676\n529.0\n537.340027\n\n\nH1\n681\n676\n504.0\n515.571106\n\n\n\n\n\n\n\nWe’ll now plot the forecast for each cutoff period. To make the plots clearer, we’ll rename the actual values in each period.\n\ncrossvalidation_df.rename(columns = {'y' : 'actual'}, inplace = True) # rename actual values \n\ncutoff = crossvalidation_df['cutoff'].unique()\n\nfor k in range(len(cutoff)): \n    cv = crossvalidation_df[crossvalidation_df['cutoff'] == cutoff[k]]\n    StatsForecast.plot(df, cv.loc[:, cv.columns != 'cutoff'])\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nNotice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#evaluate-results",
    "href": "docs/tutorials/crossvalidation.html#evaluate-results",
    "title": "Cross validation",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe can now compute the accuracy of the forecast using an appropiate accuracy metric. Here we’ll use the Root Mean Squared Error (RMSE). To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a function to compute the RMSE.\n\npip install datasetsforecast\n\n\nfrom datasetsforecast.losses import rmse\n\nThe function to compute the RMSE takes two arguments:\n\nThe actual values.\n\nThe forecasts, in this case, AutoETS.\n\n\nrmse = rmse(crossvalidation_df['actual'], crossvalidation_df['AutoETS'])\nprint(\"RMSE using cross-validation: \", rmse)\n\nRMSE using cross-validation:  33.90342\n\n\nThis measure should better reflect the predictive abilities of our model, since it used different time periods to test its accuracy.\n\n\n\n\n\n\nTip\n\n\n\nCross validation is especially useful when comparing multiple models. Here’s an example with multiple models and time series."
  },
  {
    "objectID": "docs/tutorials/crossvalidation.html#references",
    "href": "docs/tutorials/crossvalidation.html#references",
    "title": "Cross validation",
    "section": "References",
    "text": "References\nRob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Time series cross-validation”."
  },
  {
    "objectID": "docs/how-to-guides/migrating_R.html",
    "href": "docs/how-to-guides/migrating_R.html",
    "title": "Migrating from R",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/migrating_R.html#we-are-working-on-this-site.",
    "href": "docs/how-to-guides/migrating_R.html#we-are-working-on-this-site.",
    "title": "Migrating from R",
    "section": "",
    "text": "This site is currently in development. If you are particularly interested in this section, please open a GitHub Issue, and we will prioritize it."
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html",
    "href": "docs/how-to-guides/prophet_spark_m5.html",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "",
    "text": "The purpose of this notebook is to create a scalability benchmark (time and performance). To that end, Nixtla’s StatsForecast (using the ETS model) is trained on the M5 dataset using spark to distribute the training. As a comparison, Facebook’s Prophet model is used.\nAn AWS cluster (mounted on databricks) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM) with runtime 10.4 LTS was used. This notebook was used as base case.\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#main-results",
    "href": "docs/how-to-guides/prophet_spark_m5.html#main-results",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Main results",
    "text": "Main results\n\n\n\nMethod\nTime (mins)\nPerformance (wRMSSE)\n\n\n\n\nStatsForecast\n7.5\n0.68\n\n\nProphet\n18.23\n0.77"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#installing-libraries",
    "href": "docs/how-to-guides/prophet_spark_m5.html#installing-libraries",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Installing libraries",
    "text": "Installing libraries\n\npip install prophet \"neuralforecast&lt;1.0.0\" \"statsforecast[fugue]\""
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#statsforecast-pipeline",
    "href": "docs/how-to-guides/prophet_spark_m5.html#statsforecast-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "StatsForecast pipeline",
    "text": "StatsForecast pipeline\n\nfrom time import time\n\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast.distributed.utils import forecast\nfrom statsforecast.distributed.fugue import FugueBackend\nfrom statsforecast.models import ETS, SeasonalNaive\nfrom statsforecast.core import StatsForecast\n\nfrom pyspark.sql import SparkSession\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\nbackend = FugueBackend(spark, {\"fugue.spark.use_pandas_udf\":True})\n\n\n\n\n\n\nForecast\nWith statsforecast you don’t have to download your data. The distributed backend can handle a file with your data.\n\ninit = time()\nets_forecasts = backend.forecast(\n    \"s3://m5-benchmarks/data/train/m5-target.parquet\", \n    [ETS(season_length=7, model='ZAA')], \n    freq=\"D\", \n    h=28, \n).toPandas()\nend = time()\nprint(f'Minutes taken by StatsForecast on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by StatsForecast on a Spark cluster: 7.471468730767568\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = ets_forecasts.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\nwrmsse_ets = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\nwrmsse_ets\n\n\nOut[14]: \n\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.682358\n\n\nLevel1\n0.449115\n\n\nLevel2\n0.533754\n\n\nLevel3\n0.592317\n\n\nLevel4\n0.497086\n\n\nLevel5\n0.572189\n\n\nLevel6\n0.593880\n\n\nLevel7\n0.665358\n\n\nLevel8\n0.652183\n\n\nLevel9\n0.734492\n\n\nLevel10\n1.012633\n\n\nLevel11\n0.969902\n\n\nLevel12\n0.915380"
  },
  {
    "objectID": "docs/how-to-guides/prophet_spark_m5.html#prophet-pipeline",
    "href": "docs/how-to-guides/prophet_spark_m5.html#prophet-pipeline",
    "title": "StatsForecast ETS and Facebook Prophet on Spark (M5)",
    "section": "Prophet pipeline",
    "text": "Prophet pipeline\n\nimport logging\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom prophet import Prophet\nfrom pyspark.sql.types import *\n\n# disable informational messages from prophet\nlogging.getLogger('py4j').setLevel(logging.ERROR)\n\n\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\nINFO:py4j.java_gateway:Received command c on object id p0\n\n\n\n\nDownload data\n\n# structure of the training data set\ntrain_schema = StructType([\n  StructField('unique_id', StringType()),  \n  StructField('ds', DateType()),\n  StructField('y', DoubleType())\n  ])\n \n# read the training file into a dataframe\ntrain = spark.read.parquet(\n  's3://m5-benchmarks/data/train/m5-target.parquet', \n  header=True, \n  schema=train_schema\n )\n \n# make the dataframe queriable as a temporary view\ntrain.createOrReplaceTempView('train')\n\n\n\n\n\n\nsql_statement = '''\n  SELECT\n    unique_id AS unique_id,\n    CAST(ds as date) as ds,\n    y as y\n  FROM train\n  '''\n \nm5_history = (\n  spark\n    .sql( sql_statement )\n    .repartition(sc.defaultParallelism, ['unique_id'])\n  ).cache()\n\n\n\n\n\n\n\nForecast function using Prophet\n\ndef forecast( history_pd: pd.DataFrame ) -&gt; pd.DataFrame:\n  \n  # TRAIN MODEL AS BEFORE\n  # --------------------------------------\n  # remove missing values (more likely at day-store-item level)\n    history_pd = history_pd.dropna()\n\n    # configure the model\n    model = Prophet(\n        growth='linear',\n        daily_seasonality=False,\n        weekly_seasonality=True,\n        yearly_seasonality=True,\n        seasonality_mode='multiplicative'\n    )\n\n    # train the model\n    model.fit( history_pd )\n    # --------------------------------------\n\n    # BUILD FORECAST AS BEFORE\n    # --------------------------------------\n    # make predictions\n    future_pd = model.make_future_dataframe(\n        periods=28, \n        freq='d', \n        include_history=False\n    )\n    forecast_pd = model.predict( future_pd )  \n    # --------------------------------------\n\n    # ASSEMBLE EXPECTED RESULT SET\n    # --------------------------------------\n    # get relevant fields from forecast\n    forecast_pd['unique_id'] = history_pd['unique_id'].unique()[0]\n    f_pd = forecast_pd[['unique_id', 'ds','yhat']]\n    # --------------------------------------\n\n    # return expected dataset\n    return f_pd\n\n\n\n\n\n\nresult_schema = StructType([\n  StructField('unique_id', StringType()), \n  StructField('ds',DateType()),\n  StructField('yhat',FloatType()),\n])\n\n\n\n\n\n\nTraining Prophet on the M5 dataset\n\ninit = time()\nresults = (\n  m5_history\n    .groupBy('unique_id')\n      .applyInPandas(forecast, schema=result_schema)\n    ).toPandas()\nend = time()\nprint(f'Minutes taken by Prophet on a Spark cluster: {(end - init) / 60}')\n\n\nMinutes taken by Prophet on a Spark cluster: 18.23116923570633\n\n\n\n\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = results.set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n\n\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])#.drop(columns=['unique_id'])\n\n\n\n\n\n\nwrmsse = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\nwrmsse\n\n\nOut[10]: \n\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.771800\n\n\nLevel1\n0.507905\n\n\nLevel2\n0.586328\n\n\nLevel3\n0.666686\n\n\nLevel4\n0.549358\n\n\nLevel5\n0.655003\n\n\nLevel6\n0.647176\n\n\nLevel7\n0.747047\n\n\nLevel8\n0.743422\n\n\nLevel9\n0.824667\n\n\nLevel10\n1.207069\n\n\nLevel11\n1.108780\n\n\nLevel12\n1.018163"
  },
  {
    "objectID": "docs/how-to-guides/ray.html",
    "href": "docs/how-to-guides/ray.html",
    "title": "Ray",
    "section": "",
    "text": "As long as Ray is installed and configured, StatsForecast will be able to use it. If executing on a distributed Ray cluster, make use the statsforecast library is installed across all the workers.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/ray.html#installation",
    "href": "docs/how-to-guides/ray.html#installation",
    "title": "Ray",
    "section": "",
    "text": "As long as Ray is installed and configured, StatsForecast will be able to use it. If executing on a distributed Ray cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/ray.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/ray.html#statsforecast-on-pandas",
    "title": "Ray",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Ray, it’s recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Ray.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/ray.html#executing-on-ray",
    "href": "docs/how-to-guides/ray.html#executing-on-ray",
    "title": "Ray",
    "section": "Executing on Ray",
    "text": "Executing on Ray\nTo run the forecasts distributed on Ray, just pass in a Ray Dataset instead. Instead of having the unique_id as an index, it needs to be a column because Ray has no index.\n\nimport ray\nimport logging\nray.init(logging_level=logging.ERROR)\n\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\nctx = ray.data.context.DatasetContext.get_current()\nctx.use_streaming_executor = False\nray_series = ray.data.from_pandas(series).repartition(4)\n\n\nsf.forecast(df=ray_series, h=horizon).take(5)\n\n2023-06-17 01:39:08,329 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Repartition]\n2023-06-17 01:39:09,554 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Repartition]\n2023-06-17 01:39:09,727 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[MapBatches(add_simple_key)]\n2023-06-17 01:39:11,134 INFO bulk_executor.py:42 -- Executing DAG InputDataBuffer[Input] -&gt; AllToAllOperator[Sort] -&gt; TaskPoolMapOperator[MapBatches(group_fn)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[{'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 10, 0, 0),\n  'AutoETS': 5.261609077453613},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 11, 0, 0),\n  'AutoETS': 6.196357250213623},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 12, 0, 0),\n  'AutoETS': 0.28230854868888855},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 13, 0, 0),\n  'AutoETS': 1.2641948461532593},\n {'unique_id': '0',\n  'ds': datetime.datetime(2000, 8, 14, 0, 0),\n  'AutoETS': 2.2624528408050537}]"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html",
    "href": "docs/how-to-guides/getting_started_complete_polars.html",
    "title": "End to End Walkthrough with Polars",
    "section": "",
    "text": "This document aims to highlight the recent integration of Polars, a robust and high-speed DataFrame library developed in Rust, into the functionality of StatsForecast. Polars, with its nimble and potent capabilities, has rapidly established a strong reputation within the Data Science community, further solidifying its position as a reliable tool for managing and manipulating substantial data sets.\nAvailable in languages including Rust, Python, Node.js, and R, Polars demonstrates a remarkable ability to handle sizable data sets with efficiency and speed that surpasses many other DataFrame libraries, such as Pandas. Polars’ open-source nature invites ongoing enhancements and contributions, augmenting its appeal within the data science arena.\nThe most significant features of Polars that contribute to its rapid adoption are:\n\nPerformance Efficiency: Constructed using Rust, Polars exhibits an exemplary ability to manage substantial datasets with remarkable speed and minimal memory usage.\nLazy Evaluation: Polars operates on the principle of ‘lazy evaluation’, creating an optimized logical plan of operations for efficient execution, a feature that mirrors the functionality of Apache Spark.\nParallel Execution: Demonstrating the capability to exploit multi-core CPUs, Polars facilitates parallel execution of operations, substantially accelerating data processing tasks.\n\n\n\n\n\n\n\nPrerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\n\n\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\n\nInstall packages.\nRead the data.\nExplore the data.\nTrain many models for every unique combination of time series.\nEvaluate the model’s performance using cross-validation.\nSelect the best model for every unique time series.\n\n\n\n\n\n\n\nNot Covered in this guide\n\n\n\n\n\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\n\nTraining models on Multiple Seasonalities.\n\nLearn to use multiple seasonality in this Electricity Load forecasting tutorial.\n\nUsing external regressors or exogenous variables\n\nFollow this tutorial to include exogenous variables like weather or holidays or static variables like category or family.\n\nComparing StatsForecast with other popular libraries.\n\nYou can reproduce our benchmarks here.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#introducing-polars-a-high-performance-dataframe-library",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#introducing-polars-a-high-performance-dataframe-library",
    "title": "End to End Walkthrough with Polars",
    "section": "",
    "text": "This document aims to highlight the recent integration of Polars, a robust and high-speed DataFrame library developed in Rust, into the functionality of StatsForecast. Polars, with its nimble and potent capabilities, has rapidly established a strong reputation within the Data Science community, further solidifying its position as a reliable tool for managing and manipulating substantial data sets.\nAvailable in languages including Rust, Python, Node.js, and R, Polars demonstrates a remarkable ability to handle sizable data sets with efficiency and speed that surpasses many other DataFrame libraries, such as Pandas. Polars’ open-source nature invites ongoing enhancements and contributions, augmenting its appeal within the data science arena.\nThe most significant features of Polars that contribute to its rapid adoption are:\n\nPerformance Efficiency: Constructed using Rust, Polars exhibits an exemplary ability to manage substantial datasets with remarkable speed and minimal memory usage.\nLazy Evaluation: Polars operates on the principle of ‘lazy evaluation’, creating an optimized logical plan of operations for efficient execution, a feature that mirrors the functionality of Apache Spark.\nParallel Execution: Demonstrating the capability to exploit multi-core CPUs, Polars facilitates parallel execution of operations, substantially accelerating data processing tasks.\n\n\n\n\n\n\n\nPrerequesites\n\n\n\n\n\nThis Guide assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\n\n\n\nFollow this article for a step to step guide on building a production-ready forecasting pipeline for multiple time series.\nDuring this guide you will gain familiary with the core StatsForecastclass and some relevant methods like StatsForecast.plot, StatsForecast.forecast and StatsForecast.cross_validation.\nWe will use a classical benchmarking dataset from the M4 competition. The dataset includes time series from different domains like finance, economy and sales. In this example, we will use a subset of the Hourly dataset.\nWe will model each time series individually. Forecasting at this level is also known as local forecasting. Therefore, you will train a series of models for every unique series and then select the best one. StatsForecast focuses on speed, simplicity, and scalability, which makes it ideal for this task.\nOutline:\n\nInstall packages.\nRead the data.\nExplore the data.\nTrain many models for every unique combination of time series.\nEvaluate the model’s performance using cross-validation.\nSelect the best model for every unique time series.\n\n\n\n\n\n\n\nNot Covered in this guide\n\n\n\n\n\n\nForecasting at scale using clusters on the cloud.\n\nForecast the M5 Dataset in 5min using Ray clusters.\nForecast the M5 Dataset in 5min using Spark clusters.\nLearn how to predict 1M series in less than 30min.\n\nTraining models on Multiple Seasonalities.\n\nLearn to use multiple seasonality in this Electricity Load forecasting tutorial.\n\nUsing external regressors or exogenous variables\n\nFollow this tutorial to include exogenous variables like weather or holidays or static variables like category or family.\n\nComparing StatsForecast with other popular libraries.\n\nYou can reproduce our benchmarks here."
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#install-libraries",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#install-libraries",
    "title": "End to End Walkthrough with Polars",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS and datasetsforecast for common error metrics like MAE or MASE.\nInstall the necessary packages using pip install statsforecast s3fs datasetsforecast ``"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#read-the-data",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#read-the-data",
    "title": "End to End Walkthrough with Polars",
    "section": "Read the data",
    "text": "Read the data\nWe will use polars to read the M4 Hourly data set stored in a parquet file for efficiency. You can use ordinary polars operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestampe ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nThis data set already satisfies the requirement.\nDepending on your internet connection, this step should take around 10 seconds.\n\nimport polars as pl\n\nY_df = pl.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n\nY_df.head()\n\n\nshape: (5, 3)\n\n\n\nunique_id\nds\ny\n\n\nstr\ni64\nf64\n\n\n\n\n\"H1\"\n1\n605.0\n\n\n\"H1\"\n2\n586.0\n\n\n\"H1\"\n3\n586.0\n\n\n\"H1\"\n4\n559.0\n\n\n\"H1\"\n5\n511.0\n\n\n\n\n\n\nThis dataset contains 414 unique series with 900 observations on average. For this example and reproducibility’s sake, we will select only 10 unique IDs and keep only the last week. Depending on your processing infrastructure feel free to select more or less series.\n\n\n\n\n\n\nNote\n\n\n\nProcessing time is dependent on the available computing resources. Running this example with the complete dataset takes around 10 minutes in a c5d.24xlarge (96 cores) instance from AWS.\n\n\n\nuids = Y_df['unique_id'].unique(maintain_order=True)[:10] # Select 10 ids to make the example faster\n\nY_df = Y_df.filter(pl.col('unique_id').is_in(uids))\n\nY_df = Y_df.groupby('unique_id').tail(7 * 24) #Select last 7 days of data to make example faster"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#explore-data-with-the-plot-method",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#explore-data-with-the-plot-method",
    "title": "End to End Walkthrough with Polars",
    "section": "Explore Data with the plot method",
    "text": "Explore Data with the plot method\nPlot some series using the plot method from the StatsForecast class. This method prints 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a defaul engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#train-multiple-models-for-many-series",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#train-multiple-models-for-many-series",
    "title": "End to End Walkthrough with Polars",
    "section": "Train multiple models for many series",
    "text": "Train multiple models for many series\nStatsForecast can train many models on many time series efficiently.\nStart by importing and instantiating the desired models. StatsForecast offers a wide variety of models grouped in the following categories:\n\nAuto Forecast: Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series. Includes automatic versions of: Arima, ETS, Theta, CES.\nExponential Smoothing: Uses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Examples: SES, Holt’s Winters, SSO.\nBenchmark models: classical models for establishing baselines. Examples: Mean, Naive, Random Walk\nIntermittent or Sparse models: suited for series with very few non-zero observations. Examples: CROSTON, ADIDA, IMAPA\nMultiple Seasonalities: suited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs. Examples: MSTL.\nTheta Models: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Examples: Theta, DynamicTheta\n\nHere you can check the complete list of models.\nFor this example we will use:\n\nAutoARIMA: Automatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Ref: AutoARIMA.\nHoltWinters: triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. Ref: HoltWinters\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive\nHistoricAverage: arthimetic mean. Ref: HistoricAverage.\nDynamicOptimizedTheta: The theta family of models has been shown to perform well in various datasets such as M3. Models the deseasonalized time series. Ref: DynamicOptimizedTheta.\n\nImport and instantiate the models. Setting the season_length argument is sometimes tricky. This article on Seasonal periods) by the master, Rob Hyndmann, can be useful.\n\nfrom statsforecast.models import (\n    AutoARIMA,\n    HoltWinters,\n    CrostonClassic as Croston, \n    HistoricAverage,\n    DynamicOptimizedTheta as DOT,\n    SeasonalNaive\n)\n\n\n# Create a list of models and instantiation parameters\nmodels = [\n    AutoARIMA(season_length=24),\n    HoltWinters(),\n    Croston(),\n    SeasonalNaive(season_length=24),\n    HistoricAverage(),\n    DOT(season_length=24)\n]\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.) This is also available with Polars.\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n    n_jobs=-1,\n    fallback_model = SeasonalNaive(season_length=7),\n    verbose=True\n)\n\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nThe forecast method is compatible with distributed clusters, so it does not store any model parameters. If you want to store parameters for every model you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nforecasts_df = sf.forecast(h=48, level=[90])\n\nforecasts_df.head()\n\n\nshape: (5, 18)\n\n\n\nunique_id\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\nHoltWinters\nHoltWinters-lo-90\nHoltWinters-hi-90\nCrostonClassic\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nDynamicOptimizedTheta\nDynamicOptimizedTheta-lo-90\nDynamicOptimizedTheta-hi-90\n\n\nstr\ni64\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n749\n592.461792\n572.325623\n612.597961\n829.0\n-246.367554\n1904.367554\n708.21405\n635.0\n537.471191\n732.528809\n660.982117\n398.03775\n923.926514\n592.701843\n577.677307\n611.652649\n\n\n\"H1\"\n750\n527.174316\n495.321777\n559.026855\n807.0\n-268.367554\n1882.367554\n708.21405\n572.0\n474.471222\n669.528809\n660.982117\n398.03775\n923.926514\n525.589111\n505.449738\n546.621826\n\n\n\"H1\"\n751\n488.418549\n445.535583\n531.301514\n785.0\n-290.367554\n1860.367554\n708.21405\n532.0\n434.471222\n629.528809\n660.982117\n398.03775\n923.926514\n489.251801\n462.072876\n512.424133\n\n\n\"H1\"\n752\n452.284454\n400.677155\n503.891785\n756.0\n-319.367554\n1831.367554\n708.21405\n493.0\n395.471222\n590.528809\n660.982117\n398.03775\n923.926514\n456.195038\n430.554291\n478.260956\n\n\n\"H1\"\n753\n433.127563\n374.070984\n492.184143\n719.0\n-356.367554\n1794.367554\n708.21405\n477.0\n379.471222\n574.528809\n660.982117\n398.03775\n923.926514\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\nPlot the results of 8 randon series using the StatsForecast.plot method.\n\nsf.plot(Y_df,forecasts_df)\n\n\n                                                \n\n\nThe StatsForecast.plot allows for further customization. For example, plot the results of the different models and unique ids.\n\n# Plot to unique_ids and some selected models\nsf.plot(Y_df, forecasts_df, models=[\"HoltWinters\",\"DynamicOptimizedTheta\"], unique_ids=[\"H10\", \"H105\"], level=[90])\n\n\n                                                \n\n\n\n# Explore other models \nsf.plot(Y_df, forecasts_df, models=[\"AutoARIMA\"], unique_ids=[\"H10\", \"H105\"], level=[90])"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#evaluate-the-models-performance",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#evaluate-the-models-performance",
    "title": "End to End Walkthrough with Polars",
    "section": "Evaluate the model’s performance",
    "text": "Evaluate the model’s performance\nIn previous steps, we’ve taken our historical data to predict the future. However, to asses its accuracy we would also like to know how the model would have performed in the past. To assess the accuracy and robustness of your models on your data perform Cross-Validation.\nWith time series data, Cross Validation is done by defining a sliding window across the historical data and predicting the period following it. This form of cross-validation allows us to arrive at a better estimation of our model’s predictive abilities across a wider range of temporal instances while also keeping the data in the training set contiguous as is required by our models.\nThe following graph depicts such a Cross Validation Strategy:\n\nCross-validation of time series models is considered a best practice but most implementations are very slow. The statsforecast library implements cross-validation as a distributed operation, making the process less time-consuming to perform. If you have big datasets you can also perform Cross Validation in a distributed cluster using Ray, Dask or Spark.\nIn this case, we want to evaluate the performance of each model for the last 2 days (n_windows=2), forecasting every second day (step_size=48). Depending on your computer, this step should take around 1 min.\n\n\n\n\n\n\nTip\n\n\n\nSetting n_windows=1 mirrors a traditional train-test split with our historical data serving as the training set and the last 48 hours serving as the testing set.\n\n\nThe cross_validation method from the StatsForecast class takes the following arguments.\n\ndf: training data frame\nh (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows(int): number of windows used for cross validation. In other words: what number of forecasting processes in the past do you want to evaluate.\n\n\ncrossvaldation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=2\n  )\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncrossvaldation_df.head()\n\n\nshape: (5, 10)\n\n\n\nunique_id\nds\ncutoff\ny\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\n\n\nstr\ni64\ni64\nf32\nf32\nf32\nf32\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n701\n700\n619.0\n603.925415\n847.0\n742.668762\n691.0\n661.674988\n612.767517\n\n\n\"H1\"\n702\n700\n565.0\n507.591736\n820.0\n742.668762\n618.0\n661.674988\n536.846252\n\n\n\"H1\"\n703\n700\n532.0\n481.281677\n790.0\n742.668762\n563.0\n661.674988\n497.82428\n\n\n\"H1\"\n704\n700\n495.0\n444.410248\n784.0\n742.668762\n529.0\n661.674988\n464.723236\n\n\n\"H1\"\n705\n700\n481.0\n421.168762\n752.0\n742.668762\n504.0\n661.674988\n440.972351\n\n\n\n\n\n\nNext, we will evaluate the performance of every model for every series using common error metrics like Mean Absolute Error (MAE) or Mean Square Error (MSE) Define a utility function to evaluate different error metrics for the cross validation data frame.\nFirst import the desired error metrics from datasetsforecast.losses. Then define a utility function that takes a cross-validation data frame as a metric and returns an evaluation data frame with the average of the error metric for every unique id and fitted model and all cutoffs.\n\nfrom datasetsforecast.losses import mse, mae, rmse\n\n\ndef evaluate_cross_validation(df, metric):\n    models = df.drop(columns=['ds', 'cutoff', 'y', 'unique_id']).columns\n    evals = []\n    for model in models:\n        eval_ = (\n            df\n            .groupby(['unique_id', 'cutoff'])\n            # Calculate the metric for the model using a custom function \n            # and actual 'y' values\n            .agg(\n                pl.apply(\n                    exprs=['y', model],\n                    function=lambda args: metric(args[0], args[1]),\n                )\n            )\n            # Rename the result column to the model's name\n            .rename({'y': model})\n            .sort(by=['unique_id', 'cutoff'])\n        )\n        evals.append(eval_)\n\n    # Concatenate uid_cutoff and eval_dfs horizontally, alight concats based\n    # on common columns found in multiple DataFrames\n    evals = (\n        pl.concat(evals, how='align')\n        .drop('cutoff')\n    )\n\n    # Calculate the mean of each 'unique_id' group\n    evals = evals.groupby(['unique_id'], maintain_order=True).mean() \n\n    # For each row in evals (excluding 'unique_id'), find the model with the lowest value\n    best_model = [min(row, key=row.get) for row in evals.drop('unique_id').rows(named=True)]\n\n    # Add a 'best_model' column to evals dataframe with the best model for each 'unique_id'\n    evals = evals.with_columns(pl.lit(best_model).alias('best_model')).sort(by=['unique_id'])\n    return evals\n\n\n\n\n\n\n\nWarning\n\n\n\nYou can also use Mean Average Percentage Error (MAPE), however for granular forecasts, MAPE values are extremely hard to judge and not useful to assess forecasting quality.\n\n\nCreate the data frame with the results of the evaluation of your cross-validation data frame using a Mean Squared Error metric.\n\nevaluation_df = evaluate_cross_validation(crossvaldation_df, mse)\n\nevaluation_df.head()\n\n\nshape: (5, 8)\n\n\n\nunique_id\nAutoARIMA\nHoltWinters\nCrostonClassic\nSeasonalNaive\nHistoricAverage\nDynamicOptimizedTheta\nbest_model\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"H1\"\n1979.302185\n44888.019531\n28038.736328\n1422.666687\n20927.664062\n1296.333984\n\"DynamicOptimiz…\n\n\n\"H10\"\n458.892715\n2812.916626\n1483.484131\n96.895832\n1980.367493\n379.621124\n\"SeasonalNaive\"\n\n\n\"H100\"\n8629.948242\n121625.375\n91945.140625\n12019.0\n78491.191406\n21699.647949\n\"AutoARIMA\"\n\n\n\"H101\"\n6818.348633\n28453.395508\n16183.634766\n10944.458008\n18208.404297\n63698.073242\n\"AutoARIMA\"\n\n\n\"H102\"\n65489.96582\n232924.851562\n132655.300781\n12699.895996\n309110.46875\n31393.521484\n\"SeasonalNaive\"\n\n\n\n\n\n\nCreate a summary table with a model column and the number of series where that model performs best. In this case, the Arima and Seasonal Naive are the best models for 10 series and the Theta model should be used for two.\n\nselect_cols = ['best_model', 'unique_id']\n\nsummary_df = (\n    evaluation_df\n    .groupby('best_model')\n    .n_unique()\n    [select_cols]\n    .sort(by='unique_id')\n    .rename(dict(zip(select_cols, [\"Model\", \"Nr. of unique_ids\"])))\n)\n\nsummary_df\n\n\nshape: (3, 2)\n\n\n\nModel\nNr. of unique_ids\n\n\nstr\nu32\n\n\n\n\n\"DynamicOptimiz…\n2\n\n\n\"SeasonalNaive\"\n4\n\n\n\"AutoARIMA\"\n4\n\n\n\n\n\n\nYou can further explore your results by plotting the unique_ids where a specific model wins.\n\nseasonal_ids = evaluation_df.filter(pl.col('best_model') == 'SeasonalNaive')['unique_id']\n\nsf.plot(Y_df,forecasts_df, unique_ids=seasonal_ids, models=[\"SeasonalNaive\",\"DynamicOptimizedTheta\"])"
  },
  {
    "objectID": "docs/how-to-guides/getting_started_complete_polars.html#select-the-best-model-for-every-unique-series",
    "href": "docs/how-to-guides/getting_started_complete_polars.html#select-the-best-model-for-every-unique-series",
    "title": "End to End Walkthrough with Polars",
    "section": "Select the best model for every unique series",
    "text": "Select the best model for every unique series\nDefine a utility function that takes your forecast’s data frame with the predictions and the evaluation data frame and returns a data frame with the best possible forecast for every unique_id.\n\ndef get_best_model_forecast(forecasts_df, evaluation_df):\n    # Melt the 'forecasts_df' dataframe to long format, where each row represents\n    # a unique ID, a timestamp, a model, and that model's forecast.\n    df = (\n        forecasts_df\n        .melt(\n            id_vars=[\"unique_id\", \"ds\"], \n            value_vars=forecasts_df.columns[2:], \n            variable_name=\"model\", \n            value_name=\"best_model_forecast\"\n        )\n        # Join this dataframe with 'evaluation_df' on 'unique_id', attaching \n        # the 'best_model' for each unique ID.\n        .join(\n            evaluation_df[['unique_id', 'best_model']],\n            on='unique_id',\n            how=\"left\",\n        )\n    )\n\n    # Clean up the 'model' names by removing \"-lo-90\" and \"-hi-90\" from them,\n    # and store the cleaned names in a new column called 'clean_model'.\n    # Filter the dataframe to keep only the rows where 'clean_model' matches 'best_model'.\n    # After that, drop the 'clean_model' and 'best_model' columns, as they are no longer needed.\n    df = (\n        df\n        .with_columns(\n            pl.col('model').str.replace(\"-lo-90|-hi-90\", \"\").alias(\"clean_model\")\n        )\n        .filter(pl.col('clean_model') == pl.col('best_model'))\n        .drop('clean_model', 'best_model')\n    )\n\n    # Rename all the 'model' names to \"best_model\" for clarity, \n    # because at this point the dataframe only contains forecasts from the best model for each unique ID.\n    # Then, reshape the dataframe back to wide format using the 'pivot()' method.\n    # The pivoted dataframe has one row per unique ID and timestamp, with a column for each 'model' \n    # (in this case, all models are renamed to 'best_model'), and the value in each cell is the 'best_model_forecast'.\n    # The 'pivot()' method requires an aggregate function to apply if there are multiple values for the same index and column.\n    # Here, it uses 'first', meaning it keeps the first value if there are multiple.\n    # Finally, sort the dataframe by 'unique_id' and 'ds'.\n    return (\n        df\n        .with_columns(\n            pl.col('model').str.replace(\"[A-Za-z0-9]+\", \"best_model\")\n        )\n        .pivot(\n            values='best_model_forecast',\n            index=['unique_id', 'ds'],\n            columns='model',\n            aggregate_function='first',\n        )\n        .sort(by=['unique_id', 'ds'])\n    )\n\nCreate your production-ready data frame with the best forecast for every unique_id.\n\nprod_forecasts_df = get_best_model_forecast(forecasts_df, evaluation_df)\n\nprod_forecasts_df.head()\n\n\nshape: (5, 5)\n\n\n\nunique_id\nds\nbest_model\nbest_model-lo-90\nbest_model-hi-90\n\n\nstr\ni64\nf32\nf32\nf32\n\n\n\n\n\"H1\"\n749\n592.701843\n577.677307\n611.652649\n\n\n\"H1\"\n750\n525.589111\n505.449738\n546.621826\n\n\n\"H1\"\n751\n489.251801\n462.072876\n512.424133\n\n\n\"H1\"\n752\n456.195038\n430.554291\n478.260956\n\n\n\"H1\"\n753\n436.290527\n411.051239\n461.815948\n\n\n\n\n\n\nPlot the results.\n\nsf.plot(Y_df, prod_forecasts_df, level=[90])"
  },
  {
    "objectID": "docs/how-to-guides/dask.html",
    "href": "docs/how-to-guides/dask.html",
    "title": "Dask",
    "section": "",
    "text": "StatsForecast works on top of Spark, Dask, and Ray through Fugue. StatsForecast will read the input DataFrame and use the corresponding engine. For example, if the input is a Spark DataFrame, StatsForecast will use the existing Spark session to run the forecast.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/dask.html#installation",
    "href": "docs/how-to-guides/dask.html#installation",
    "title": "Dask",
    "section": "Installation",
    "text": "Installation\nAs long as Dask is installed and configured, StatsForecast will be able to use it. If executing on a distributed Dask cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/dask.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/dask.html#statsforecast-on-pandas",
    "title": "Dask",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Dask, it’s recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Dask.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/dask.html#executing-on-dask",
    "href": "docs/how-to-guides/dask.html#executing-on-dask",
    "title": "Dask",
    "section": "Executing on Dask",
    "text": "Executing on Dask\nTo run the forecasts distributed on Dask, just pass in a Dask DataFrame instead. Instead of having the unique_id as an index, it needs to be a column because Dask handles the index differently.\n\nimport dask.dataframe as dd\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\nddf = dd.from_pandas(series, npartitions=4)\n\n\nsf.forecast(df=ddf, h=horizon).compute().head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoETS\n\n\n\n\n0\n0\n2000-08-10\n5.261609\n\n\n1\n0\n2000-08-11\n6.196357\n\n\n2\n0\n2000-08-12\n0.282309\n\n\n3\n0\n2000-08-13\n1.264195\n\n\n4\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html",
    "href": "docs/how-to-guides/amazonstatsforecast.html",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "",
    "text": "We will make use of the M5 competition dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances. Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address.\nThe data are ready for download at the following URLs:\n\nTrain set: https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet\nTemporal exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet\nStatic exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet\n\nA more detailed description of the data can be found here.\n\n\n\n\n\n\nWarning\n\n\n\nThe M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#data",
    "href": "docs/how-to-guides/amazonstatsforecast.html#data",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "",
    "text": "We will make use of the M5 competition dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances. Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address.\nThe data are ready for download at the following URLs:\n\nTrain set: https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet\nTemporal exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet\nStatic exogenous variables (used by AmazonForecast): https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet\n\nA more detailed description of the data can be found here.\n\n\n\n\n\n\nWarning\n\n\n\nThe M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies."
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#amazon-forecast",
    "href": "docs/how-to-guides/amazonstatsforecast.html#amazon-forecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Amazon Forecast",
    "text": "Amazon Forecast\nAmazon Forecast is a fully automated solution for time series forecasting. The solution can take the time series to forecast and exogenous variables (temporal and static). For this experiment, we used the AutoPredict functionality of Amazon Forecast following the steps of this tutorial. A detailed description of the particular steps for this dataset can be found here.\nAmazon Forecast creates predictors with AutoPredictor, which involves applying the optimal combination of algorithms to each time series in your datasets. The predictor is an Amazon Forecast model that is trained using your target time series, related time series, item metadata, and any additional datasets you include.\nIncluded algorithms range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.: CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS.\nTo leverage the probabilistic features of Amazon Forecast and enable confidence intervals for further analysis we forecasted the following quantiles: 0.1 | 0.5 | 0.9.\nThe full pipeline of Amazon Forecast took 4.1 hours and the results can be found here: s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#nixtlas-statsforecast",
    "href": "docs/how-to-guides/amazonstatsforecast.html#nixtlas-statsforecast",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Nixtla’s StatsForecast",
    "text": "Nixtla’s StatsForecast\n\nInstall necessary libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nAdditionally, we will install s3fs to read from the S3 Filesystem of AWS. (If you don’t want to use a cloud storage provider, you can read your files locally using pandas)\n\n!pip install statsforecast s3fs\n\n\n\nInput format\nWe will use pandas to read the data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes .csv.\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nSo we will rename the original columns to make it compatible with StatsForecast.\nDepending on your internet connection, this step should take around 20 seconds.\n\n\n\n\n\n\nWarning\n\n\n\nWe are reading a file from S3, so you need to install the s3fs library. To install it, run ! pip install s3fs\n\n\n\n\nRead data\n\nimport pandas as pd\n\nY_df_m5 = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet') \n\nY_df_m5 = Y_df_m5.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n}) \n\nY_df_m5.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\n\n\nTrain statistical models\nWe fit the model by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them. For this example, we will use AutoETS and DynamicOptimizedTheta. We set season_length to 7 because we expect seasonal effects every week. (See: Seasonal periods)\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails.\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nAutoETS: Exponential Smoothing model. Automatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Ref: AutoETS.\nSeasonalNaive: Memory Efficient Seasonal Naive predictions. Ref: SeasonalNaive.\nDynamicOptimizedTheta: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Ref: DynamicOptimizedTheta.\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import (\n    AutoETS,\n    DynamicOptimizedTheta,\n    SeasonalNaive\n)\n\n# Create list of models\nmodels = [\n    AutoETS(season_length=7),\n    DynamicOptimizedTheta(season_length=7),\n]\n\n# Instantiate StatsForecast class\nsf = StatsForecast( \n    models=models,\n    freq='D', \n    n_jobs=-1,\n    fallback_model=SeasonalNaive(season_length=7)\n)\n\n/home/ubuntu/fede/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\nThe forecast method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\n\n\n\n\n\nNote\n\n\n\nThe forecast is inteded to be compatible with distributed clusters, so it does not store any model parameters. If you want to store parameter for everymodel you can use the fit and predict methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n\n\n\nfrom time import time\n\n\ninit = time()\nforecasts_df = sf.forecast(df=Y_df_m5, h=28)\nend = time()\nprint(f'Statsforecast time M5 {(end - init) / 60}')\n\nStatsforecast time M5 14.274124479293823\n\n\nStore the results for further evaluation.\n\nforecasts_df['ThETS'] = forecasts_df[['DynamicOptimizedTheta', 'AutoETS']].clip(0).median(axis=1, numeric_only=True)\nforecasts_df.to_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')"
  },
  {
    "objectID": "docs/how-to-guides/amazonstatsforecast.html#evaluation",
    "href": "docs/how-to-guides/amazonstatsforecast.html#evaluation",
    "title": "Amazon Forecast vs StatsForecast",
    "section": "Evaluation",
    "text": "Evaluation\nThis section evaluates the performance of StatsForecast and AmazonForecast. To do this, we first need to install datasetsforecast, a Python library developed by Nixtla that includes a large battery of benchmark datasets and evaluation utilities. The library will allow us to calculate the performance of the models using the original evaluation used in the competition.\n\n!pip install datasetsforecast\n\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\n\nThe following function will allow us to evaluate a specific model included in the input dataframe. The function is useful for evaluating different models.\n\nfrom datasetsforecast.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\n\n### Evaluator\ndef evaluate_forecasts(df, model, model_name):\n    Y_hat = df.set_index('ds', append=True)[model].unstack()\n    *_, S_df = M5.load('data')\n    Y_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n    eval_ = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n    eval_ = eval_.rename(columns={'wrmsse': f'{model_name}_{model}_wrmsse'})\n    return eval_\n\nNow let’s read the forecasts generated for each solution.\n\n### Read Forecasts\nstatsforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')\namazonforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet')\n\n### Amazon Forecast wrangling\namazonforecasts_df = amazonforecasts_df.rename(columns={'item_id': 'unique_id', 'date': 'ds'})\n# amazon forecast returns the unique_id column in lower case\n# we need to transform it to upper case to ensure proper merging\namazonforecasts_df['unique_id'] = amazonforecasts_df['unique_id'].str.upper()\namazonforecasts_df = amazonforecasts_df.set_index('unique_id')\n# parse datestamp\namazonforecasts_df['ds'] = pd.to_datetime(amazonforecasts_df['ds']).dt.tz_localize(None)\n\nFinally, let’s use our predefined function to compute the performance of each model.\n\n### Evaluate performances\nm5_eval_df = pd.concat([\n    evaluate_forecasts(statsforecasts_df, 'ThETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'AutoETS', 'StatsForecast'),\n    evaluate_forecasts(statsforecasts_df, 'DynamicOptimizedTheta', 'StatsForecast'),\n    evaluate_forecasts(amazonforecasts_df, 'p50', 'AmazonForecast'),\n], axis=1)\nm5_eval_df.T\n\n\n\n\n\n\n\n\nTotal\nLevel1\nLevel2\nLevel3\nLevel4\nLevel5\nLevel6\nLevel7\nLevel8\nLevel9\nLevel10\nLevel11\nLevel12\n\n\n\n\nStatsForecast_ThETS_wrmsse\n0.669606\n0.424331\n0.515777\n0.580670\n0.474098\n0.552459\n0.578092\n0.651079\n0.642446\n0.725324\n1.009390\n0.967537\n0.914068\n\n\nStatsForecast_AutoETS_wrmsse\n0.672404\n0.430474\n0.516340\n0.580736\n0.482090\n0.559721\n0.579939\n0.655362\n0.643638\n0.727967\n1.010596\n0.968168\n0.913820\n\n\nStatsForecast_DynamicOptimizedTheta_wrmsse\n0.675333\n0.429670\n0.521640\n0.589278\n0.478730\n0.557520\n0.584278\n0.656283\n0.650613\n0.731735\n1.013910\n0.971758\n0.918576\n\n\nAmazonForecast_p50_wrmsse\n1.617815\n1.912144\n1.786991\n1.736382\n1.972658\n2.010498\n1.805926\n1.819329\n1.667225\n1.619216\n1.156432\n1.012942\n0.914040\n\n\n\n\n\n\n\nThe results (including processing time and costs) can be summarized in the following table."
  },
  {
    "objectID": "src/distributed.core.html",
    "href": "src/distributed.core.html",
    "title": "Core",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "src/ces.html",
    "href": "src/ces.html",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/ces.html#cescalc",
    "href": "src/ces.html#cescalc",
    "title": "CES Model",
    "section": "",
    "text": "source\n\n\n\n ces_target_fn (optimal_param, init_alpha_0, init_alpha_1, init_beta_0,\n                init_beta_1, opt_alpha_0, opt_alpha_1, opt_beta_0,\n                opt_beta_1, y, m, init_states, n_components, seasontype,\n                nmse)"
  },
  {
    "objectID": "src/adapters.prophet.html",
    "href": "src/adapters.prophet.html",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‘linear’, ‘logistic’ or ‘flat’ to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast’s level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA’s exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). “Prophet Forecasting at Scale”\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). “NeuralProphet: Explainable Forecasting at Scale”.\nRob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet’s seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/adapters.prophet.html#autoarimaprophet",
    "href": "src/adapters.prophet.html#autoarimaprophet",
    "title": "Replace FB-Prophet",
    "section": "",
    "text": "source\n\n\n\n AutoARIMAProphet (growth='linear', changepoints=None, n_changepoints=25,\n                   changepoint_range=0.8, yearly_seasonality='auto',\n                   weekly_seasonality='auto', daily_seasonality='auto',\n                   holidays=None, seasonality_mode='additive',\n                   seasonality_prior_scale=10.0,\n                   holidays_prior_scale=10.0,\n                   changepoint_prior_scale=0.05, mcmc_samples=0,\n                   interval_width=0.8, uncertainty_samples=1000,\n                   stan_backend=None, d=None, D=None, max_p=5, max_q=5,\n                   max_P=2, max_Q=2, max_order=5, max_d=2, max_D=1,\n                   start_p=2, start_q=2, start_P=1, start_Q=1,\n                   stationary=False, seasonal=True, ic='aicc',\n                   stepwise=True, nmodels=94, trace=False,\n                   approximation=False, method=None, truncate=None,\n                   test='kpss', test_kwargs=None, seasonal_test='seas',\n                   seasonal_test_kwargs=None, allowdrift=False,\n                   allowmean=False, blambda=None, biasadj=False, period=1)\n\nAutoARIMAProphet adapter.\nReturns best ARIMA model using external variables created by the Prophet interface. This class receives as parameters the same as prophet.Prophet and uses a models.AutoARIMA backend.\nIf your forecasting pipeline uses Prophet the AutoARIMAProphet adapter helps to easily substitute Prophet with an AutoARIMA.\nParameters: growth: String ‘linear’, ‘logistic’ or ‘flat’ to specify a linear, logistic or flat trend. changepoints: List of dates of potential changepoints. Otherwise selected automatically. n_changepoints: Number of potential changepoints to include. changepoint_range: Proportion of history in which trend changepoints will be estimated. yearly_seasonality: Fit yearly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. weekly_seasonality: Fit weekly seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. daily_seasonality: Fit daily seasonality. Can be ‘auto’, True, False, or a number of Fourier terms to generate. holidays: pandas.DataFrame with columns holiday (string) and ds (date type). interval_width: float, uncertainty forecast intervals width. StatsForecast’s level \nNotes: You can create automated exogenous variables from the Prophet data processing pipeline these exogenous will be included into AutoARIMA’s exogenous features. Parameters like seasonality_mode, seasonality_prior_scale, holidays_prior_scale, changepoint_prior_scale, mcmc_samples, uncertainty_samples, stan_backend are Prophet exclusive.\nReferences: Sean J. Taylor, Benjamin Letham (2017). “Prophet Forecasting at Scale”\nOskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, Ram Rajagopal (2021). “NeuralProphet: Explainable Forecasting at Scale”.\nRob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\nsource\n\n\n\n\n AutoARIMAProphet.fit (df, disable_seasonal_features=True, **kwargs)\n\nFit the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series. disable_seasonal_features: bool, Wheter disable Prophet’s seasonal features. kwargs: Additional arguments.\nReturns: self: AutoARIMAProphet adapter object with AutoARIMA fitted model.\n\nsource\n\n\n\n\n AutoARIMAProphet.predict (df=None)\n\nPredict using the AutoARIMAProphet adapter.\nParameters: df: pandas.DataFrame, with columns ds (date type) and y, the time series.\nReturns: fcsts_df: A pandas.DataFrame with the forecast components."
  },
  {
    "objectID": "src/adapters.prophet.html#univariate-prophet",
    "href": "src/adapters.prophet.html#univariate-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.1 Univariate Prophet ",
    "text": "2.1 Univariate Prophet \nHere we forecast with Prophet without external regressors. We first instantiate a new Prophet object, and define its forecasting procedure into its constructor. After that a classic sklearn fit and predict is used to obtain the predictions.\n\nm = Prophet(daily_seasonality=False)\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nHere we forecast with AutoARIMAProphet adapter without external regressors. It inherits the Prophet constructor as well as its fit and predict methods.\nWith the class AutoARIMAProphet you can simply substitute Prophet and you’ll be training an AutoARIMA model without changing anything in your forecasting pipeline.\n\nm = AutoARIMAProphet(daily_seasonality=False)\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "src/adapters.prophet.html#holiday-prophet",
    "href": "src/adapters.prophet.html#holiday-prophet",
    "title": "Replace FB-Prophet",
    "section": "2.2 Holiday Prophet ",
    "text": "2.2 Holiday Prophet \nUsually Prophet pipelines include the usage of external regressors such as holidays.\nSuppose you want to include holidays or other recurring calendar events, you can create a pandas.DataFrame for them. The DataFrame needs two columns [holiday, ds] and a row for each holiday. It requires all the occurrences of the holiday (as far as the historical data allows) and the future events of the holiday. If the future does not have the holidays registered, they will be modeled but not included in the forecast.\nYou can also include into the events DataFrame, lower_window and upper_window that extends the effect of the holidays through dates to [lower_window, upper_window] days around the date. For example if you wanted to account for Christmas Eve in addition to Christmas you’d include lower_window=-1,upper_window=0, or Black Friday in addition to Thanksgiving, you’d include lower_window=0,upper_window=1.\nHere we Peyton Manning’s playoff appearances dates:\n\nplayoffs = pd.DataFrame({\n  'holiday': 'playoff',\n  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n                        '2010-01-24', '2010-02-07', '2011-01-08',\n                        '2013-01-12', '2014-01-12', '2014-01-19',\n                        '2014-02-02', '2015-01-11', '2016-01-17',\n                        '2016-01-24', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((playoffs, superbowls))\n\n\nm = Prophet(daily_seasonality=False, holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)\n\nThe class AutoARIMAProphet adapter allows to handle these scenarios to fit an AutoARIMA model with exogenous variables.\nYou can enjoy your Prophet pipelines with the improved performance of a classic ARIMA.\n\nm = AutoARIMAProphet(daily_seasonality=False,\n                     holidays=holidays)\nm.add_country_holidays(country_name='US')\nm.fit(df)\n# m.fit(df, disable_seasonal_features=False) # Uncomment for better AutoARIMA predictions\nfuture = m.make_future_dataframe(365)\nforecast = m.predict(future)\n\n\nfig = m.plot(forecast)"
  },
  {
    "objectID": "src/mstl.html",
    "href": "src/mstl.html",
    "title": "MSTL model",
    "section": "",
    "text": "source\n\nmstl\n\n mstl (x:numpy.ndarray, period:Union[int,List[int]],\n       blambda:Optional[float]=None, iterate:int=1,\n       s_window:Optional[numpy.ndarray]=None,\n       stl_kwargs:Optional[Dict]={})\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nndarray\n\ntime series\n\n\nperiod\ntyping.Union[int, typing.List[int]]\n\nseason length\n\n\nblambda\ntyping.Optional[float]\nNone\nbox-cox transform\n\n\niterate\nint\n1\nnumber of iterations\n\n\ns_window\ntyping.Optional[numpy.ndarray]\nNone\nseasonal window\n\n\nstl_kwargs\ntyping.Optional[typing.Dict]\n{}\n\n\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/garch.html",
    "href": "src/garch.html",
    "title": "GARCH",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "src/garch.html#generate-garchpq-model",
    "href": "src/garch.html#generate-garchpq-model",
    "title": "GARCH",
    "section": "Generate GARCH(p,q) model",
    "text": "Generate GARCH(p,q) model\n\nComparison with arch library\nThis section compares the coefficients generated by the previous functions with the coefficients generated by the arch library for \\(p=q\\), \\(p&gt;q\\), \\(p&lt;q\\), and \\(q=0\\)."
  },
  {
    "objectID": "src/core/core.html",
    "href": "src/core/core.html",
    "title": "Core Methods",
    "section": "",
    "text": "The core methods of StatsForecast are:\ndef test_gp_df(df, sort_df):\n    df = df.set_index(\"ds\", append=True)\n    if not df.index.is_monotonic_increasing and sort_df:\n        df = df.sort_index()\n    data = df.values.astype(np.float32)\n    indices_sizes = df.index.get_level_values(\"unique_id\").value_counts(sort=False)\n    indices = indices_sizes.index\n    sizes = indices_sizes.values\n    cum_sizes = sizes.cumsum()\n    dates = df.index.get_level_values(\"ds\")[cum_sizes - 1]\n    indptr = np.append(0, cum_sizes).astype(np.int32)\n    return GroupedArray(data, indptr), indices, dates, df.index\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/core/core.html#statsforecast",
    "href": "src/core/core.html#statsforecast",
    "title": "Core Methods",
    "section": "StatsForecast",
    "text": "StatsForecast\n\n StatsForecast (models:List[Any], freq:str, n_jobs:int=1, df:Union[pandas.\n                core.frame.DataFrame,polars.dataframe.frame.DataFrame,None\n                Type]=None, sort_df:bool=True,\n                fallback_model:Optional[Any]=None, verbose:bool=False)\n\nTrain statistical models.\nThe StatsForecast class allows you to efficiently fit multiple StatsForecast models for large sets of time series. It operates with pandas DataFrame df that identifies series and datestamps with the unique_id and ds columns. The y column denotes the target time series variable.\nThe class has memory-efficient StatsForecast.forecast method that avoids storing partial model outputs. While the StatsForecast.fit and StatsForecast.predict methods with Scikit-learn interface store the fitted models.\nThe StatsForecast class offers parallelization utilities with Dask, Spark and Ray back-ends. See distributed computing example here.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodels\ntyping.List[typing.Any]\n\nList of instantiated objects models.StatsForecast.\n\n\nfreq\nstr\n\nFrequency of the data.See panda’s available frequencies.\n\n\nn_jobs\nint\n1\nNumber of jobs used in the parallel processing, use -1 for all cores.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nfallback_model\ntyping.Optional[typing.Any]\nNone\nModel to be used if a model fails. Only works with the forecast and cross_validation methods.\n\n\nverbose\nbool\nFalse\nPrints TQDM progress bar when n_jobs=1.\n\n\n\n\n# StatsForecast's class usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    ADIDA,\n    AutoARIMA,\n    CrostonClassic,\n    CrostonOptimized,\n    CrostonSBA,\n    HistoricAverage,\n    IMAPA,\n    Naive,\n    RandomWalkWithDrift,\n    SeasonalExponentialSmoothing,\n    SeasonalNaive,\n    SeasonalWindowAverage,\n    SimpleExponentialSmoothing,\n    TSB,\n    WindowAverage,\n    DynamicOptimizedTheta,\n    AutoETS,\n    AutoCES\n)\n\n# Generate synthetic panel DataFrame for example\npanel_df = generate_series(n_series=9, equal_ends=False, engine='pandas')\npanel_df.groupby('unique_id').tail(4)\n\n\n\n\n\n\n\n\ny\nds\n\n\nunique_id\n\n\n\n\n\n\n0\n1.212726\n2000-08-06\n\n\n0\n2.442669\n2000-08-07\n\n\n0\n3.339940\n2000-08-08\n\n\n0\n4.228065\n2000-08-09\n\n\n1\n0.048275\n2000-04-03\n\n\n1\n1.128070\n2000-04-04\n\n\n1\n2.295968\n2000-04-05\n\n\n1\n3.238239\n2000-04-06\n\n\n2\n6.480128\n2000-06-12\n\n\n2\n0.036217\n2000-06-13\n\n\n2\n1.009650\n2000-06-14\n\n\n2\n2.489787\n2000-06-15\n\n\n3\n3.289840\n2000-08-26\n\n\n3\n4.227949\n2000-08-27\n\n\n3\n5.321176\n2000-08-28\n\n\n3\n6.127013\n2000-08-29\n\n\n4\n5.403709\n2001-01-04\n\n\n4\n6.081779\n2001-01-05\n\n\n4\n0.438420\n2001-01-06\n\n\n4\n1.386855\n2001-01-07\n\n\n5\n5.011166\n2000-10-24\n\n\n5\n6.397153\n2000-10-25\n\n\n5\n0.462146\n2000-10-26\n\n\n5\n1.253125\n2000-10-27\n\n\n6\n5.407805\n2000-08-29\n\n\n6\n6.340789\n2000-08-30\n\n\n6\n0.202894\n2000-08-31\n\n\n6\n1.491204\n2000-09-01\n\n\n7\n1.068102\n2001-02-09\n\n\n7\n2.233974\n2001-02-10\n\n\n7\n3.484143\n2001-02-11\n\n\n7\n4.176505\n2001-02-12\n\n\n8\n4.110373\n2000-02-25\n\n\n8\n5.483879\n2000-02-26\n\n\n8\n6.068916\n2000-02-27\n\n\n8\n0.040499\n2000-02-28\n\n\n\n\n\n\n\n\n# Declare list of instantiated StatsForecast estimators to be fitted\n# You can try other estimator's hyperparameters\n# You can try other methods from the `models.StatsForecast` collection\n# Check them here: https://nixtla.github.io/statsforecast/models.html\nmodels=[AutoARIMA(), Naive(), \n        AutoETS(), AutoARIMA(allowmean=True, alias='MeanAutoARIMA')] \n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=models,\n                     freq='D', \n                     n_jobs=1, \n                     verbose=True)\n\n# Efficiently predict\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.fit",
    "href": "src/core/core.html#statsforecast.fit",
    "title": "Core Methods",
    "section": "StatsForecast.fit",
    "text": "StatsForecast.fit\n\n StatsForecast.fit (df:Union[pandas.core.frame.DataFrame,polars.dataframe.\n                    frame.DataFrame,NoneType]=None, sort_df:bool=True, pre\n                    diction_intervals:Optional[statsforecast.utils.Conform\n                    alIntervals]=None)\n\nFit statistical models.\nFit models to a large set of time series from DataFrame df and store fitted models for later inspection.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\nStatsForecast\n\nReturns with stored StatsForecast fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#satstforecast.predict",
    "href": "src/core/core.html#satstforecast.predict",
    "title": "Core Methods",
    "section": "SatstForecast.predict",
    "text": "SatstForecast.predict\n\n SatstForecast.predict (h:int, X_df:Union[pandas.core.frame.DataFrame,pola\n                        rs.dataframe.frame.DataFrame,NoneType]=None,\n                        level:Optional[List[int]]=None)\n\nPredict statistical models.\nUse stored fitted models to predict large set of time series from DataFrame df.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.fit_predict",
    "href": "src/core/core.html#statsforecast.fit_predict",
    "title": "Core Methods",
    "section": "StatsForecast.fit_predict",
    "text": "StatsForecast.fit_predict\n\n StatsForecast.fit_predict (h:int, df:Union[pandas.core.frame.DataFrame,po\n                            lars.dataframe.frame.DataFrame,NoneType]=None,\n                            X_df:Union[pandas.core.frame.DataFrame,polars.\n                            dataframe.frame.DataFrame,NoneType]=None,\n                            level:Optional[List[int]]=None,\n                            sort_df:bool=True, prediction_intervals:Option\n                            al[statsforecast.utils.ConformalIntervals]=Non\n                            e)\n\nFit and Predict with statistical models.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\nIn contrast to StatsForecast.forecast this method stores partial models outputs.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous variables.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.forecast",
    "href": "src/core/core.html#statsforecast.forecast",
    "title": "Core Methods",
    "section": "StatsForecast.forecast",
    "text": "StatsForecast.forecast\n\n StatsForecast.forecast (h:int, df:Union[pandas.core.frame.DataFrame,polar\n                         s.dataframe.frame.DataFrame,NoneType]=None, X_df:\n                         Union[pandas.core.frame.DataFrame,polars.datafram\n                         e.frame.DataFrame,NoneType]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False, sort_df:bool=True, prediction_\n                         intervals:Optional[statsforecast.utils.ConformalI\n                         ntervals]=None)\n\nMemory Efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to Scikit-Learn fit_predict without storing information. It requires the forecast horizon h in advance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nX_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with [unique_id, ds] columns and df’s future exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not return insample predictions.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by [unique_id,ds].\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame | polars.DataFrame\n\nDataFrame with models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.forecast method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import AutoARIMA, Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA(), Naive()],\n                     freq='D', n_jobs=1)\n\n# Efficiently predict without storing memory\nfcsts_df = fcst.forecast(h=4, fitted=True)\nfcsts_df.groupby('unique_id').tail(4)\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n1.0\n1961-01-01\n476.006500\n432.0\n\n\n1.0\n1961-01-02\n482.846222\n432.0\n\n\n1.0\n1961-01-03\n512.423523\n432.0\n\n\n1.0\n1961-01-04\n502.038269\n432.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.forecast_fitted_values",
    "href": "src/core/core.html#statsforecast.forecast_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.forecast_fitted_values",
    "text": "StatsForecast.forecast_fitted_values\n\n StatsForecast.forecast_fitted_values ()\n\nAccess insample predictions.\nAfter executing StatsForecast.forecast, you can access the insample prediction values for each model. To get them, you need to pass fitted=True to the StatsForecast.forecast method and then use the StatsForecast.forecast_fitted_values method.\n\n# StatsForecast.forecast_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[AutoARIMA()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nfcsts_df = fcst.forecast(h=12, fitted=True, level=(90, 10))\ninsample_fcsts_df = fcst.forecast_fitted_values()\ninsample_fcsts_df.tail(4)\n\n\n\n\n\n\n\n\nds\ny\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-lo-10\nAutoARIMA-hi-10\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n1.0\n1960-09-30\n508.0\n572.654175\n525.092163\n569.020630\n576.287781\n620.216187\n\n\n1.0\n1960-10-31\n461.0\n451.528259\n403.966248\n447.894684\n455.161835\n499.090271\n\n\n1.0\n1960-11-30\n390.0\n437.915375\n390.353363\n434.281799\n441.548981\n485.477386\n\n\n1.0\n1960-12-31\n432.0\n369.718781\n322.156769\n366.085205\n373.352356\n417.280792\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.cross_validation",
    "href": "src/core/core.html#statsforecast.cross_validation",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation",
    "text": "StatsForecast.cross_validation\n\n StatsForecast.cross_validation (h:int, df:Union[pandas.core.frame.DataFra\n                                 me,polars.dataframe.frame.DataFrame,NoneT\n                                 ype]=None, n_windows:int=1,\n                                 step_size:int=1,\n                                 test_size:Optional[int]=None,\n                                 input_size:Optional[int]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False, refit:bool=True,\n                                 sort_df:bool=True, prediction_intervals:O\n                                 ptional[statsforecast.utils.ConformalInte\n                                 rvals]=None)\n\nTemporal Cross-Validation.\nEfficiently fits a list of StatsForecast models through multiple training windows, in either chained or rolled manner.\nStatsForecast.models’ speed allows to overcome this evaluation technique high computational costs. Temporal cross-validation provides better model’s generalization measurements by increasing the test’s length and diversity.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds, y] and exogenous.If None, the StatsForecast class should have been instantiatedusing df.\n\n\nn_windows\nint\n1\nNumber of windows used for cross validation.\n\n\nstep_size\nint\n1\nStep size between each window.\n\n\ntest_size\ntyping.Optional[int]\nNone\nLength of test size. If passed, set n_windows=None.\n\n\ninput_size\ntyping.Optional[int]\nNone\nInput size for each window, if not none rolled windows.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels between 0 and 100 for prediction intervals.\n\n\nfitted\nbool\nFalse\nWether or not returns insample predictions.\n\n\nrefit\nbool\nTrue\nWether or not refit the model for each window.\n\n\nsort_df\nbool\nTrue\nIf True, sort df by unique_id and ds.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nConfiguration to calibrate prediction intervals (Conformal Prediction).\n\n\nReturns\npandas.DataFrame\n\nDataFrame with insample models columns for point predictions and probabilisticpredictions for all fitted models.\n\n\n\n\n# StatsForecast.crossvalidation method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1, verbose=True)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(14, n_windows=2)\nrolled_fcsts_df.head(4)\n\nCross Validation Time Series 1:   0%|          | 0/2 [00:00&lt;?, ?it/s]Cross Validation Time Series 1: 100%|##########| 2/2 [00:00&lt;00:00, 9167.88it/s]\n\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n\n1.0\n1960-12-17\n1960-12-16\n407.0\n463.0\n\n\n1.0\n1960-12-18\n1960-12-16\n362.0\n463.0\n\n\n1.0\n1960-12-19\n1960-12-16\n405.0\n463.0\n\n\n1.0\n1960-12-20\n1960-12-16\n417.0\n463.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.cross_validation_fitted_values",
    "href": "src/core/core.html#statsforecast.cross_validation_fitted_values",
    "title": "Core Methods",
    "section": "StatsForecast.cross_validation_fitted_values",
    "text": "StatsForecast.cross_validation_fitted_values\n\n StatsForecast.cross_validation_fitted_values ()\n\nAccess insample cross validated predictions.\nAfter executing StatsForecast.cross_validation, you can access the insample prediction values for each model and window. To get them, you need to pass fitted=True to the StatsForecast.cross_validation method and then use the StatsForecast.cross_validation_fitted_values method.\n\n# StatsForecast.cross_validation_fitted_values method usage example\n\n#from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengersDF as panel_df\nfrom statsforecast.models import Naive\n\n# Instantiate StatsForecast class\nfcst = StatsForecast(df=panel_df,\n                     models=[Naive()],\n                     freq='D', n_jobs=1)\n\n# Access insample predictions\nrolled_fcsts_df = fcst.cross_validation(h=12, n_windows=2, fitted=True)\ninsample_rolled_fcsts_df = fcst.cross_validation_fitted_values()\ninsample_rolled_fcsts_df.tail(4)\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nNaive\n\n\nunique_id\n\n\n\n\n\n\n\n\n1.0\n1959-09-30\n1959-12-31\n463.0\n559.0\n\n\n1.0\n1959-10-31\n1959-12-31\n407.0\n463.0\n\n\n1.0\n1959-11-30\n1959-12-31\n362.0\n407.0\n\n\n1.0\n1959-12-31\n1959-12-31\n405.0\n362.0\n\n\n\n\n\n\n\n\nsource"
  },
  {
    "objectID": "src/core/core.html#statsforecast.plot",
    "href": "src/core/core.html#statsforecast.plot",
    "title": "Core Methods",
    "section": "StatsForecast.plot",
    "text": "StatsForecast.plot\n\n StatsForecast.plot\n                     (df:Union[pandas.core.frame.DataFrame,polars.datafram\n                     e.frame.DataFrame], forecasts_df:Union[pandas.core.fr\n                     ame.DataFrame,polars.dataframe.frame.DataFrame,NoneTy\n                     pe]=None, unique_ids:Union[List[str],NoneType,numpy.n\n                     darray]=None, plot_random:bool=True,\n                     models:Optional[List[str]]=None,\n                     level:Optional[List[float]]=None,\n                     max_insample_length:Optional[int]=None,\n                     plot_anomalies:bool=False, engine:str='matplotlib',\n                     resampler_kwargs:Optional[Dict]=None)\n\nPlot forecasts and insample values.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame]\n\nDataFrame with columns [unique_id, ds, y].\n\n\nforecasts_df\ntyping.Union[pandas.core.frame.DataFrame, polars.dataframe.frame.DataFrame, NoneType]\nNone\nDataFrame with columns [unique_id, ds] and models.\n\n\nunique_ids\ntyping.Union[typing.List[str], NoneType, numpy.ndarray]\nNone\nTime Series to plot.If None, time series are selected randomly.\n\n\nplot_random\nbool\nTrue\nSelect time series to plot randomly.\n\n\nmodels\ntyping.Optional[typing.List[str]]\nNone\nList of models to plot.\n\n\nlevel\ntyping.Optional[typing.List[float]]\nNone\nList of prediction intervals to plot if paseed.\n\n\nmax_insample_length\ntyping.Optional[int]\nNone\nMax number of train/insample observations to be plotted.\n\n\nplot_anomalies\nbool\nFalse\nPlot anomalies for each prediction interval.\n\n\nengine\nstr\nmatplotlib\nLibrary used to plot. ‘plotly’, ‘plotly-resampler’ or ‘matplotlib’.\n\n\nresampler_kwargs\ntyping.Optional[typing.Dict]\nNone\nKwargs to be passed to plotly-resampler constructor. For further custumization (“show_dash”) call the method,store the plotting object and add the extra arguments toits show_dash method."
  },
  {
    "objectID": "src/core/core.html#integer-datestamp",
    "href": "src/core/core.html#integer-datestamp",
    "title": "Core Methods",
    "section": "Integer datestamp",
    "text": "Integer datestamp\nThe StatsForecast class can also receive integers as datestamp, the following example shows how to do it.\n\n# from statsforecast.core import StatsForecast\nfrom statsforecast.utils import AirPassengers as ap\nfrom statsforecast.models import HistoricAverage\n\n\nint_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\nint_ds_df.insert(0, 'unique_id', 'AirPassengers')\nint_ds_df.set_index('unique_id', inplace=True)\nint_ds_df.head()\n\n\nint_ds_df.tail()\n\n\nint_ds_df\n\n\nfcst = StatsForecast(df=int_ds_df, models=[HistoricAverage()], freq='D')\nhorizon = 7\nforecast = fcst.forecast(horizon)\nforecast.head()\n\n\nlast_date = int_ds_df['ds'].max()\ntest_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))\n\n\nint_ds_cv = fcst.cross_validation(h=7, test_size=8, n_windows=None)\nint_ds_cv"
  },
  {
    "objectID": "src/core/core.html#external-regressors",
    "href": "src/core/core.html#external-regressors",
    "title": "Core Methods",
    "section": "External regressors",
    "text": "External regressors\nEvery column after y is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the StatsForecast.forecast method.\n\nclass LinearRegression:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, y, X):\n        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return self\n    \n    def predict(self, h, X):\n        mean = X @ coefs\n        return mean\n    \n    def __repr__(self):\n        return 'LinearRegression()'\n    \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n        return {'mean': X_future @ coefs}\n    \n    def new(self):\n        b = type(self).__new__(type(self))\n        b.__dict__.update(self.__dict__)\n        return b\n\n\nseries_xreg = series = generate_series(10_000, equal_ends=True)\nseries_xreg['intercept'] = 1\nseries_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\nseries_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\nseries_xreg\n\n\ndates = sorted(series_xreg['ds'].unique())\nvalid_start = dates[-14]\ntrain_mask = series_xreg['ds'] &lt; valid_start\nseries_train = series_xreg[train_mask]\nseries_valid = series_xreg[~train_mask]\nX_valid = series_valid.drop(columns=['y'])\nfcst = StatsForecast(\n    df=series_train,\n    models=[LinearRegression()],\n    freq='D',\n)\nxreg_res = fcst.forecast(14, X_df=X_valid)\nxreg_res['y'] = series_valid['y'].values\n\n\nxreg_res.groupby('ds').mean().plot()\n\n\nxreg_res_cv = fcst.cross_validation(h=3, test_size=5, n_windows=None)"
  },
  {
    "objectID": "src/core/core.html#prediction-intervals",
    "href": "src/core/core.html#prediction-intervals",
    "title": "Core Methods",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nYou can pass the argument level to the StatsForecast.forecast method to calculate prediction intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented.\n\nap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap}, index=pd.Index([0] * ap.size, name='unique_id'))\nsf = StatsForecast(\n    models=[\n        SeasonalNaive(season_length=12), \n        AutoARIMA(season_length=12)\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95))\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
  },
  {
    "objectID": "src/core/core.html#conformal-prediction-intervals",
    "href": "src/core/core.html#conformal-prediction-intervals",
    "title": "Core Methods",
    "section": "Conformal Prediction intervals",
    "text": "Conformal Prediction intervals\nYou can also add conformal intervals using the following code.\n\nfrom statsforecast.utils import ConformalIntervals\n\n\nsf = StatsForecast(\n    models=[\n        AutoARIMA(season_length=12),\n        AutoARIMA(\n            season_length=12, \n            prediction_intervals=ConformalIntervals(n_windows=2, h=12),\n            alias='ConformalAutoARIMA'\n        ),\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95))\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"plotly\")\n\nYou can also compute conformal intervals for all the models that support them, using the following,\n\nsf = StatsForecast(\n    models=[\n        AutoARIMA(season_length=12),\n    ],\n    freq='M',\n    n_jobs=1\n)\nap_ci = sf.forecast(\n    df=ap_df, \n    h=12, \n    level=(50, 80, 95), \n    prediction_intervals=ConformalIntervals(h=12),\n)\nfcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
  },
  {
    "objectID": "src/core/models.html",
    "href": "src/core/models.html",
    "title": "Models",
    "section": "",
    "text": "StatsForecast offers a wide variety of models grouped in the following categories:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/core/models.html#autoarima",
    "href": "src/core/models.html#autoarima",
    "title": "Models",
    "section": "AutoARIMA",
    "text": "AutoARIMA\n\nsource\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=False, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=False, allowmean:bool=False,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            season_length:int=1, alias:str='AutoARIMA', prediction_interva\n            ls:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutoARIMA model.\nAutomatically selects the best ARIMA (AutoRegressive Integrated Moving Average) model using an information criterion. Default is Akaike Information Criterion (AICc).\nNote: This implementation is a mirror of Hyndman’s forecast::auto.arima.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\ntyping.Optional[int]\nNone\nOrder of first-differencing.\n\n\nD\ntyping.Optional[int]\nNone\nOrder of seasonal-differencing.\n\n\nmax_p\nint\n5\nMax autorregresives p.\n\n\nmax_q\nint\n5\nMax moving averages q.\n\n\nmax_P\nint\n2\nMax seasonal autorregresives P.\n\n\nmax_Q\nint\n2\nMax seasonal moving averages Q.\n\n\nmax_order\nint\n5\nMax p+q+P+Q value if not stepwise selection.\n\n\nmax_d\nint\n2\nMax non-seasonal differences.\n\n\nmax_D\nint\n1\nMax seasonal differences.\n\n\nstart_p\nint\n2\nStarting value of p in stepwise procedure.\n\n\nstart_q\nint\n2\nStarting value of q in stepwise procedure.\n\n\nstart_P\nint\n1\nStarting value of P in stepwise procedure.\n\n\nstart_Q\nint\n1\nStarting value of Q in stepwise procedure.\n\n\nstationary\nbool\nFalse\nIf True, restricts search to stationary models.\n\n\nseasonal\nbool\nTrue\nIf False, restricts search to non-seasonal models.\n\n\nic\nstr\naicc\nInformation criterion to be used in model selection.\n\n\nstepwise\nbool\nTrue\nIf True, will do stepwise selection (faster).\n\n\nnmodels\nint\n94\nNumber of models considered in stepwise search.\n\n\ntrace\nbool\nFalse\nIf True, the searched ARIMA models is reported.\n\n\napproximation\ntyping.Optional[bool]\nFalse\nIf True, conditional sums-of-squares estimation, final MLE.\n\n\nmethod\ntyping.Optional[str]\nNone\nFitting method between maximum likelihood or sums-of-squares.\n\n\ntruncate\ntyping.Optional[bool]\nNone\nObservations truncated series used in model selection.\n\n\ntest\nstr\nkpss\nUnit root test to use. See ndiffs for details.\n\n\ntest_kwargs\ntyping.Optional[str]\nNone\nUnit root test additional arguments.\n\n\nseasonal_test\nstr\nseas\nSelection method for seasonal differences.\n\n\nseasonal_test_kwargs\ntyping.Optional[typing.Dict]\nNone\nSeasonal unit root test arguments.\n\n\nallowdrift\nbool\nFalse\nIf True, drift models terms considered.\n\n\nallowmean\nbool\nFalse\nIf True, non-zero mean models considered.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nAutoARIMA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoARIMA.fit\n\n AutoARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoARIMA model.\nFit an AutoARIMA to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoARIMA fitted model.\n\n\n\n\nsource\n\n\nAutoARIMA.predict\n\n AutoARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoArima.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.predict_in_sample\n\n AutoARIMA.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoArima insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forecast\n\n AutoARIMA.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoARIMA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoARIMA.forward\n\n AutoARIMA.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted ARIMA model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoARIMA's usage example\n\nfrom statsforecast.models import AutoARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\narima = AutoARIMA(season_length=4)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([497.95290378, 486.7806859 , 500.08214752, 494.10983682]),\n 'lo-80': 0    467.167462\n 1    442.112227\n 2    450.786736\n 3    440.963293\n Name: 80%, dtype: float64,\n 'hi-80': 0    528.738346\n 1    531.449145\n 2    549.377559\n 3    547.256381\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#autoets",
    "href": "src/core/models.html#autoets",
    "title": "Models",
    "section": "AutoETS",
    "text": "AutoETS\n\nsource\n\nAutoETS\n\n AutoETS (season_length:int=1, model:str='ZZZ',\n          damped:Optional[bool]=None, alias:str='AutoETS', prediction_inte\n          rvals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‘ANN’ (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote: This implementation is a mirror of Hyndman’s forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\nHyndman, Rob, et al (2008). “Forecasting with exponential smoothing: the state space approach”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‘dampens’ the trend.\n\n\nalias\nstr\nAutoETS\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoETS.fit\n\n AutoETS.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoETS.predict\n\n AutoETS.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.predict_in_sample\n\n AutoETS.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forecast\n\n AutoETS.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoETS.forward\n\n AutoETS.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoETS' usage example\n\nfrom statsforecast.models import AutoETS\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nautoets = AutoETS(model='ZMZ',  \n              season_length=4)\nautoets = autoets.fit(y=ap)\ny_hat_dict = autoets.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.63294737, 419.65915384, 442.66309931, 457.33314074])}\n\n\n\nsource\n\n\nETS\n\n ETS (season_length:int=1, model:str='ZZZ', damped:Optional[bool]=None,\n      alias:str='ETS', prediction_intervals:Optional[statsforecast.utils.C\n      onformalIntervals]=None)\n\nAutomatic Exponential Smoothing model.\nAutomatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(M\\) multiplicative, \\(A\\) additive, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the ETS equations: E in [\\(M, A, Z\\)], T in [\\(N, A, M, Z\\)], and S in [\\(N, A, M, Z\\)].\nFor example when model=‘ANN’ (additive error, no trend, and no seasonality), ETS will explore only a simple exponential smoothing.\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoETS model to figure out the best parameter.\nNote: This implementation is a mirror of Hyndman’s forecast::ets.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\nHyndman, Rob, et al (2008). “Forecasting with exponential smoothing: the state space approach”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZZZ\nControlling state-space-equations.\n\n\ndamped\ntyping.Optional[bool]\nNone\nA parameter that ‘dampens’ the trend.\n\n\nalias\nstr\nETS\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nets = ETS(model='ZMZ', season_length=4)"
  },
  {
    "objectID": "src/core/models.html#autoces",
    "href": "src/core/models.html#autoces",
    "title": "Models",
    "section": "AutoCES",
    "text": "AutoCES\n\nsource\n\nAutoCES\n\n AutoCES (season_length:int=1, model:str='Z', alias:str='CES', prediction_\n          intervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nComplex Exponential Smoothing model.\nAutomatically selects the best Complex Exponential Smoothing model using an information criterion. Default is Akaike Information Criterion (AICc), while particular models are estimated using maximum likelihood. The state-space equations can be determined based on their \\(S\\) simple, \\(P\\) parial, \\(Z\\) optimized or \\(N\\) ommited components. The model string parameter defines the kind of CES model: \\(N\\) for simple CES (withous seasonality), \\(S\\) for simple seasonality (lagged CES), \\(P\\) for partial seasonality (without complex part), \\(F\\) for full seasonality (lagged CES with real and complex seasonal parts).\nIf the component is selected as ‘Z’, it operates as a placeholder to ask the AutoCES model to figure out the best parameter.\nReferences: Svetunkov, Ivan & Kourentzes, Nikolaos. (2015). “Complex Exponential Smoothing”. 10.13140/RG.2.1.3757.2562..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nmodel\nstr\nZ\nControlling state-space-equations.\n\n\nalias\nstr\nCES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoCES.fit\n\n AutoCES.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Complex Exponential Smoothing model.\nFit the Complex Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nComplex Exponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nAutoCES.predict\n\n AutoCES.predict (h:int, X:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.predict_in_sample\n\n AutoCES.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forecast\n\n AutoCES.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                   X_future:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Complex Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoCES.forward\n\n AutoCES.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                  X_future:Optional[numpy.ndarray]=None,\n                  level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Complex Exponential Smoothing to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CES' usage example\n\nfrom statsforecast.models import AutoCES\nfrom statsforecast.utils import AirPassengers as ap\n\n# Multiplicative trend, optimal error and seasonality\nces = AutoCES(model='Z',  \n              season_length=4)\nces = ces.fit(y=ap)\ny_hat_dict = ces.predict(h=4)\ny_hat_dict\n\n{'mean': array([424.30716324, 405.69589186, 442.02640533, 443.63488996])}"
  },
  {
    "objectID": "src/core/models.html#autotheta",
    "href": "src/core/models.html#autotheta",
    "title": "Models",
    "section": "AutoTheta",
    "text": "AutoTheta\n\nsource\n\nAutoTheta\n\n AutoTheta (season_length:int=1, decomposition_type:str='multiplicative',\n            model:Optional[str]=None, alias:str='AutoTheta', prediction_in\n            tervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nAutoTheta model.\nAutomatically selects the best Theta (Standard Theta Model (‘STM’), Optimized Theta Model (‘OTM’), Dynamic Standard Theta Model (‘DSTM’), Dynamic Optimized Theta Model (‘DOTM’)) model using mse.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nmodel\ntyping.Optional[str]\nNone\nControlling Theta Model. By default searchs the best model.\n\n\nalias\nstr\nAutoTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoTheta.fit\n\n AutoTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nAutoTheta.predict\n\n AutoTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.predict_in_sample\n\n AutoTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forecast\n\n AutoTheta.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoTheta.forward\n\n AutoTheta.forward (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoTheta's usage example\n\nfrom statsforecast.models import AutoTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\ntheta = AutoTheta(season_length=4)\ntheta = theta.fit(y=ap)\ny_hat_dict = theta.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.86262032, 410.60532872, 429.59124482, 440.16565301])}"
  },
  {
    "objectID": "src/core/models.html#arima",
    "href": "src/core/models.html#arima",
    "title": "Models",
    "section": "ARIMA",
    "text": "ARIMA\n\nsource\n\nARIMA\n\n ARIMA (order:Tuple[int,int,int]=(0, 0, 0), season_length:int=1,\n        seasonal_order:Tuple[int,int,int]=(0, 0, 0),\n        include_mean:bool=True, include_drift:bool=False,\n        include_constant:Optional[bool]=None,\n        blambda:Optional[float]=None, biasadj:bool=False, method:str='CSS-\n        ML', fixed:Optional[dict]=None, alias:str='ARIMA', prediction_inte\n        rvals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nARIMA model.\nAutoRegressive Integrated Moving Average model.\nReferences: Rob J. Hyndman, Yeasmin Khandakar (2008). “Automatic Time Series Forecasting: The forecast package for R”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\norder\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the non-seasonal part of the ARIMA model: the three components (p, d, q) are the AR order, the degree of differencing, and the MA order.\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nseasonal_order\ntyping.Tuple[int, int, int]\n(0, 0, 0)\nA specification of the seasonal part of the ARIMA model.(P, D, Q) for the AR order, the degree of differencing, the MA order.\n\n\ninclude_mean\nbool\nTrue\nShould the ARIMA model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the ARIMA model include a linear drift term? (i.e., a linear regression with ARIMA errors is fitted.)\n\n\ninclude_constant\ntyping.Optional[bool]\nNone\nIf True, then includ_mean is set to be True for undifferenced series and include_drift is set to be True for differenced series. Note that if there is more than one difference taken, no constant is included regardless of the value of this argument. This is deliberate as otherwise quadratic and higher order polynomial trends would be induced.\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the arima model. Example: {'ar1': 0.5, 'ma2': 0.75}.For autoregressive terms use the ar{i} keys. For its seasonal version use sar{i}.For moving average terms use the ma{i} keys. For its seasonal version use sma{i}.For intercept and drift use the intercept and drift keys.For exogenous variables use the ex_{i} keys.\n\n\nalias\nstr\nARIMA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nARIMA.fit\n\n ARIMA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nARIMA.predict\n\n ARIMA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.predict_in_sample\n\n ARIMA.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forecast\n\n ARIMA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARIMA.forward\n\n ARIMA.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# ARIMA's usage example\n\nfrom statsforecast.models import ARIMA\nfrom statsforecast.utils import AirPassengers as ap\n\n\narima = ARIMA(order=(1, 0, 0), season_length=12)\narima = arima.fit(y=ap)\ny_hat_dict = arima.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([426.54529705, 421.28383474, 416.20876726, 411.31349129]),\n 'lo-80': 0    383.228999\n 1    361.100640\n 2    343.777875\n 3    329.110028\n Name: 80%, dtype: float64,\n 'hi-80': 0    469.861595\n 1    481.467029\n 2    488.639659\n 3    493.516954\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#autoregressive",
    "href": "src/core/models.html#autoregressive",
    "title": "Models",
    "section": "AutoRegressive",
    "text": "AutoRegressive\n\nsource\n\nAutoRegressive\n\n AutoRegressive (lags:Tuple[int,List], include_mean:bool=True,\n                 include_drift:bool=False, blambda:Optional[float]=None,\n                 biasadj:bool=False, method:str='CSS-ML',\n                 fixed:Optional[dict]=None, alias:str='AutoRegressive', pr\n                 ediction_intervals:Optional[statsforecast.utils.Conformal\n                 Intervals]=None)\n\nSimple Autoregressive model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlags\ntyping.Tuple[int, typing.List]\n\nNumber of lags to include in the model. If an int is passed then all lags up to lags are considered.If a list, only the elements of the list are considered as lags.\n\n\ninclude_mean\nbool\nTrue\nShould the AutoRegressive model include a mean term? The default is True for undifferenced series, False for differenced ones (where a mean would not affect the fit nor predictions).\n\n\ninclude_drift\nbool\nFalse\nShould the AutoRegressive model include a linear drift term? (i.e., a linear regression with AutoRegressive errors is fitted.)\n\n\nblambda\ntyping.Optional[float]\nNone\nBox-Cox transformation parameter.\n\n\nbiasadj\nbool\nFalse\nUse adjusted back-transformed mean Box-Cox.\n\n\nmethod\nstr\nCSS-ML\nFitting method: maximum likelihood or minimize conditional sum-of-squares. The default (unless there are missing values) is to use conditional-sum-of-squares to find starting values, then maximum likelihood.\n\n\nfixed\ntyping.Optional[dict]\nNone\nDictionary containing fixed coefficients for the AutoRegressive model. Example: {'ar1': 0.5, 'ar5': 0.75}.For autoregressive terms use the ar{i} keys.\n\n\nalias\nstr\nAutoRegressive\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nAutoRegressive.fit\n\n AutoRegressive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nFitted model.\n\n\n\n\nsource\n\n\nAutoRegressive.predict\n\n AutoRegressive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.predict_in_sample\n\n AutoRegressive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forecast\n\n AutoRegressive.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory efficient predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x) optional exogenous.\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nAutoRegressive.forward\n\n AutoRegressive.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# AutoRegressive's usage example\n\nfrom statsforecast.models import AutoRegressive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nar = AutoRegressive(lags=[12])\nar = ar.fit(y=ap)\ny_hat_dict = ar.predict(h=4, level=[80])\ny_hat_dict\n\n/home/runner/work/statsforecast/statsforecast/statsforecast/arima.py:828: UserWarning: some AR parameters were fixed: setting transform_pars = False\n  warnings.warn(\n\n\n{'mean': array([460.01874698, 432.12629561, 462.16432016, 507.22135699]),\n 'lo-80': 0    439.539875\n 1    411.647423\n 2    441.685448\n 3    486.742485\n Name: 80%, dtype: float64,\n 'hi-80': 0    480.497619\n 1    452.605168\n 2    482.643192\n 3    527.700229\n Name: 80%, dtype: float64}"
  },
  {
    "objectID": "src/core/models.html#simplesmooth",
    "href": "src/core/models.html#simplesmooth",
    "title": "Models",
    "section": "SimpleSmooth",
    "text": "SimpleSmooth\n\nsource\n\nSimpleExponentialSmoothing\n\n SimpleExponentialSmoothing (alpha:float, alias:str='SES', prediction_inte\n                             rvals:Optional[statsforecast.utils.ConformalI\n                             ntervals]=None)\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe rate \\(0 \\leq \\alpha \\leq 1\\) at which the weights decrease is called the smoothing parameter. When \\(\\alpha = 1\\), SES is equal to the naive method.\nReferences: Charles C Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.forecast\n\n SimpleExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                      X:Optional[numpy.ndarray]=None, X_fu\n                                      ture:Optional[numpy.ndarray]=None,\n                                      level:Optional[List[int]]=None,\n                                      fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.fit\n\n SimpleExponentialSmoothing.fit (y:numpy.ndarray,\n                                 X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothing model.\nFit an SimpleExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict\n\n SimpleExponentialSmoothing.predict (h:int,\n                                     X:Optional[numpy.ndarray]=None,\n                                     level:Optional[List[int]]=None)\n\nPredict with fitted SimpleExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothing.predict_in_sample\n\n SimpleExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothing insample predictions.\n\n# SimpleExponentialSmoothing's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nses = SimpleExponentialSmoothing(alpha=0.5)\nses = ses.fit(y=ap)\ny_hat_dict = ses.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#simplesmoothoptimized",
    "href": "src/core/models.html#simplesmoothoptimized",
    "title": "Models",
    "section": "SimpleSmoothOptimized",
    "text": "SimpleSmoothOptimized\n\nsource\n\nSimpleExponentialSmoothingOptimized\n\n SimpleExponentialSmoothingOptimized (alias:str='SESOpt', prediction_inter\n                                      vals:Optional[statsforecast.utils.Co\n                                      nformalIntervals]=None)\n\nSimpleExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\(\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nReferences: Charles C Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nSESOpt\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.fit\n\n SimpleExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                          X:Optional[numpy.ndarray]=None)\n\nFit the SimpleExponentialSmoothingOptimized model.\nFit an SimpleExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSimpleExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict\n\n SimpleExponentialSmoothingOptimized.predict (h:int,\n                                              X:Optional[numpy.ndarray]=No\n                                              ne, level:Optional[List[int]\n                                              ]=None)\n\nPredict with fitted SimpleExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.predict_in_sample\n\n SimpleExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SimpleExponentialSmoothingOptimized insample predictions.\n\nsource\n\n\nSimpleExponentialSmoothingOptimized.forecast\n\n SimpleExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                               X:Optional[numpy.ndarray]=N\n                                               one, X_future:Optional[nump\n                                               y.ndarray]=None, level:Opti\n                                               onal[List[int]]=None,\n                                               fitted:bool=False)\n\nMemory Efficient SimpleExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SimpleExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SimpleExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nseso = SimpleExponentialSmoothingOptimized()\nseso = seso.fit(y=ap)\ny_hat_dict = seso.predict(h=4)\ny_hat_dict\n\n{'mean': array([431.58716, 431.58716, 431.58716, 431.58716], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalsmooth",
    "href": "src/core/models.html#seasonalsmooth",
    "title": "Models",
    "section": "SeasonalSmooth",
    "text": "SeasonalSmooth\n\nplt.plot(np.concatenate([ap[6:], seas_es.forecast(ap[6:], h=12)['mean']]))\n\n\nsource\n\nSeasonalExponentialSmoothing\n\n SeasonalExponentialSmoothing (season_length:int, alpha:float,\n                               alias:str='SeasonalES', prediction_interval\n                               s:Optional[statsforecast.utils.ConformalInt\n                               ervals]=None)\n\nSeasonalExponentialSmoothing model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nNote: This method is an extremely simplified of Holt-Winter’s method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences: Charles. C. Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”, ONR Research Memorandum, Carnegie Institute of Technology 52.. Peter R. Winters (1960). “Forecasting sales by exponentially weighted moving averages”. Management Science.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalpha\nfloat\n\nSmoothing parameter.\n\n\nalias\nstr\nSeasonalES\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.fit\n\n SeasonalExponentialSmoothing.fit (y:numpy.ndarray,\n                                   X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalExponentialSmoothing model.\nFit an SeasonalExponentialSmoothing to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothing fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict\n\n SeasonalExponentialSmoothing.predict (h:int,\n                                       X:Optional[numpy.ndarray]=None,\n                                       level:Optional[List[int]]=None)\n\nPredict with fitted SeasonalExponentialSmoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothing.predict_in_sample\n\n SeasonalExponentialSmoothing.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothing insample predictions.\n\nsource\n\n\nSeasonalExponentialSmoothing.forecast\n\n SeasonalExponentialSmoothing.forecast (y:numpy.ndarray, h:int,\n                                        X:Optional[numpy.ndarray]=None, X_\n                                        future:Optional[numpy.ndarray]=Non\n                                        e, level:Optional[List[int]]=None,\n                                        fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalExponentialSmoothing's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothing\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothing(alpha=0.5, season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([376.28955, 354.71094, 396.02002, 412.06738], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalsmoothoptimized",
    "href": "src/core/models.html#seasonalsmoothoptimized",
    "title": "Models",
    "section": "SeasonalSmoothOptimized",
    "text": "SeasonalSmoothOptimized\n\nsource\n\nSeasonalExponentialSmoothingOptimized\n\n SeasonalExponentialSmoothingOptimized (season_length:int,\n                                        alias:str='SeasESOpt', prediction_\n                                        intervals:Optional[statsforecast.u\n                                        tils.ConformalIntervals]=None)\n\nSeasonalExponentialSmoothingOptimized model.\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with no clear trend or seasonality. Assuming there are \\(t\\) observations and season \\(s\\), the one-step forecast is given by: \\(\\hat{y}_{t+1,s} = \\alpha y_t + (1-\\alpha) \\hat{y}_{t-1,s}\\)\nThe smoothing parameter \\(\\alpha^*\\) is optimized by square error minimization.\nNote: This method is an extremely simplified of Holt-Winter’s method where the trend and level are set to zero. And a single seasonal smoothing parameter \\(\\alpha\\) is shared across seasons.\nReferences: Charles. C. Holt (1957). “Forecasting seasonals and trends by exponentially weighted moving averages”, ONR Research Memorandum, Carnegie Institute of Technology 52.. Peter R. Winters (1960). “Forecasting sales by exponentially weighted moving averages”. Management Science.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nSeasESOpt\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.forecast\n\n SeasonalExponentialSmoothingOptimized.forecast (y:numpy.ndarray, h:int,\n                                                 X:Optional[numpy.ndarray]\n                                                 =None, X_future:Optional[\n                                                 numpy.ndarray]=None, leve\n                                                 l:Optional[List[int]]=Non\n                                                 e, fitted:bool=False)\n\nMemory Efficient SeasonalExponentialSmoothingOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.fit\n\n SeasonalExponentialSmoothingOptimized.fit (y:numpy.ndarray,\n                                            X:Optional[numpy.ndarray]=None\n                                            )\n\nFit the SeasonalExponentialSmoothingOptimized model.\nFit an SeasonalExponentialSmoothingOptimized to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalExponentialSmoothingOptimized fitted model.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict\n\n SeasonalExponentialSmoothingOptimized.predict (h:int,\n                                                X:Optional[numpy.ndarray]=\n                                                None, level:Optional[List[\n                                                int]]=None)\n\nPredict with fitted SeasonalExponentialSmoothingOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalExponentialSmoothingOptimized.predict_in_sample\n\n SeasonalExponentialSmoothingOptimized.predict_in_sample ()\n\nAccess fitted SeasonalExponentialSmoothingOptimized insample predictions.\n\n# SeasonalExponentialSmoothingOptimized's usage example\n\nfrom statsforecast.models import SeasonalExponentialSmoothingOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalExponentialSmoothingOptimized(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([416.42798, 390.50757, 418.8656 , 460.3452 ], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#holts-method",
    "href": "src/core/models.html#holts-method",
    "title": "Models",
    "section": "Holt’s method",
    "text": "Holt’s method\n\nsource\n\nHolt\n\n Holt (season_length:int=1, error_type:str='A', alias:str='Holt', predicti\n       on_intervals:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nHolt’s method.\nAlso known as double exponential smoothing, Holt’s method is an extension of exponential smoothing for series with a trend. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‘AAN’ or ‘MAN’).\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Methods with trend”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 12 Monthly data.\n\n\nerror_type\nstr\nA\nThe type of error of the ETS model. Can be additive (A) or multiplicative (M).\n\n\nalias\nstr\nHolt\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHolt.forecast\n\n Holt.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.fit\n\n Holt.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHolt.predict\n\n Holt.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.predict_in_sample\n\n Holt.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHolt.forward\n\n Holt.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt's usage example\n\nfrom statsforecast.models import Holt\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Holt(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23881352, 436.48160242, 438.72439132, 440.96718023])}"
  },
  {
    "objectID": "src/core/models.html#holt-winters-method",
    "href": "src/core/models.html#holt-winters-method",
    "title": "Models",
    "section": "Holt-Winters’ method",
    "text": "Holt-Winters’ method\n\nsource\n\nHoltWinters\n\n HoltWinters (season_length:int=1, error_type:str='A',\n              alias:str='HoltWinters', prediction_intervals:Optional[stats\n              forecast.utils.ConformalIntervals]=None)\n\nHolt-Winters’ method.\nAlso known as triple exponential smoothing, Holt-Winters’ method is an extension of exponential smoothing for series that contain both trend and seasonality. This implementation returns the corresponding ETS model with additive (A) or multiplicative (M) errors (so either ‘AAA’ or ‘MAM’).\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Methods with seasonality”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nseason length\n\n\nerror_type\nstr\nA\nerror type\n\n\nalias\nstr\nHoltWinters\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHoltWinters.forecast\n\n HoltWinters.forecast (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Exponential Smoothing predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.fit\n\n HoltWinters.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Exponential Smoothing model.\nFit an Exponential Smoothing model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nExponential Smoothing fitted model.\n\n\n\n\nsource\n\n\nHoltWinters.predict\n\n HoltWinters.predict (h:int, X:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None)\n\nPredict with fitted Exponential Smoothing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.predict_in_sample\n\n HoltWinters.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Exponential Smoothing insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHoltWinters.forward\n\n HoltWinters.forward (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted Exponential Smoothing model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenpus of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Holt-Winters' usage example\n\nfrom statsforecast.models import HoltWinters\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HoltWinters(season_length=12, error_type='A')\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([453.08875135, 427.4765464 , 460.63176307, 497.03681052])}"
  },
  {
    "objectID": "src/core/models.html#historicaverage",
    "href": "src/core/models.html#historicaverage",
    "title": "Models",
    "section": "HistoricAverage",
    "text": "HistoricAverage\n\nsource\n\nHistoricAverage\n\n HistoricAverage (alias:str='HistoricAverage', prediction_intervals:Option\n                  al[statsforecast.utils.ConformalIntervals]=None)\n\nHistoricAverage model.\nAlso known as mean method. Uses a simple average of all past observations. Assuming there are \\(t\\) observations, the one-step forecast is given by: \\[ \\hat{y}_{t+1} = \\frac{1}{t} \\sum_{j=1}^t y_j \\]\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “Forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nHistoricAverage\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nHistoricAverage.forecast\n\n HistoricAverage.forecast (y:numpy.ndarray, h:int,\n                           X:Optional[numpy.ndarray]=None,\n                           X_future:Optional[numpy.ndarray]=None,\n                           level:Optional[List[int]]=None,\n                           fitted:bool=False)\n\nMemory Efficient HistoricAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.fit\n\n HistoricAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the HistoricAverage model.\nFit an HistoricAverage to a time series (numpy array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself\n\nHistoricAverage fitted model.\n\n\n\n\nsource\n\n\nHistoricAverage.predict\n\n HistoricAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None)\n\nPredict with fitted HistoricAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nHistoricAverage.predict_in_sample\n\n HistoricAverage.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted HistoricAverage insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# HistoricAverage's usage example\n\nfrom statsforecast.models import HistoricAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = HistoricAverage()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([280.2986, 280.2986, 280.2986, 280.2986], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#naive",
    "href": "src/core/models.html#naive",
    "title": "Models",
    "section": "Naive",
    "text": "Naive\n\nsource\n\nNaive\n\n Naive (alias:str='Naive', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nNaive model.\nAll forecasts have the value of the last observation:\n\\(\\hat{y}_{t+1} = y_t\\) for all \\(t\\)\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nNaive\n\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nNaive.forecast\n\n Naive.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Naive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.fit\n\n Naive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Naive model.\nFit an Naive to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nNaive fitted model.\n\n\n\n\nsource\n\n\nNaive.predict\n\n Naive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaive.predict_in_sample\n\n Naive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Naive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Naive's usage example\n\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Naive()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([432., 432., 432., 432.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#randomwalkwithdrift",
    "href": "src/core/models.html#randomwalkwithdrift",
    "title": "Models",
    "section": "RandomWalkWithDrift",
    "text": "RandomWalkWithDrift\n\nsource\n\nRandomWalkWithDrift\n\n RandomWalkWithDrift (alias:str='RWD', prediction_intervals:Optional[stats\n                      forecast.utils.ConformalIntervals]=None)\n\nRandomWalkWithDrift model.\nA variation of the naive method allows the forecasts to change over time. The amout of change, called drift, is the average change seen in the historical data.\n\\[ \\hat{y}_{t+1} = y_t+\\frac{1}{t-1}\\sum_{j=1}^t (y_j-y_{j-1}) = y_t+ \\frac{y_t-y_1}{t-1} \\]\nFrom the previous equation, we can see that this is equivalent to extrapolating a line between the first and the last observation.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nRWD\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.forecast\n\n RandomWalkWithDrift.forecast (y:numpy.ndarray, h:int,\n                               X:Optional[numpy.ndarray]=None,\n                               X_future:Optional[numpy.ndarray]=None,\n                               level:Optional[List[int]]=None,\n                               fitted:bool=False)\n\nMemory Efficient RandomWalkWithDrift predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\nforecasts: dict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.fit\n\n RandomWalkWithDrift.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the RandomWalkWithDrift model.\nFit an RandomWalkWithDrift to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nRandomWalkWithDrift fitted model.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict\n\n RandomWalkWithDrift.predict (h:int, X:Optional[numpy.ndarray]=None,\n                              level:Optional[List[int]]=None)\n\nPredict with fitted RandomWalkWithDrift.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nRandomWalkWithDrift.predict_in_sample\n\n RandomWalkWithDrift.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted RandomWalkWithDrift insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# RandomWalkWithDrift's usage example\n\nfrom statsforecast.models import RandomWalkWithDrift\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = RandomWalkWithDrift()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([434.23776, 436.47552, 438.7133 , 440.95105], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalnaive",
    "href": "src/core/models.html#seasonalnaive",
    "title": "Models",
    "section": "SeasonalNaive",
    "text": "SeasonalNaive\n\nsource\n\nSeasonalNaive\n\n SeasonalNaive (season_length:int, alias:str='SeasonalNaive', prediction_i\n                ntervals:Optional[statsforecast.utils.ConformalIntervals]=\n                None)\n\nSeasonal naive model.\nA method similar to the naive, but uses the last known observation of the same period (e.g. the same month of the previous year) in order to capture seasonal variations.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\nalias\nstr\nSeasonalNaive\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalNaive.forecast\n\n SeasonalNaive.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient SeasonalNaive predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.fit\n\n SeasonalNaive.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalNaive model.\nFit an SeasonalNaive to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nSeasonalNaive fitted model.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict\n\n SeasonalNaive.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted Naive.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalNaive.predict_in_sample\n\n SeasonalNaive.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted SeasonalNaive insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# SeasonalNaive's usage example\n\nfrom statsforecast.models import SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalNaive(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([417., 391., 419., 461.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#windowaverage",
    "href": "src/core/models.html#windowaverage",
    "title": "Models",
    "section": "WindowAverage",
    "text": "WindowAverage\n\nsource\n\nWindowAverage\n\n WindowAverage (window_size:int, alias:str='WindowAverage', prediction_int\n                ervals:Optional[statsforecast.utils.ConformalIntervals]=No\n                ne)\n\nWindowAverage model.\nUses the average of the last \\(k\\) observations, with \\(k\\) the length of the window. Wider windows will capture global trends, while narrow windows will reveal local trends. The length of the window selected should take into account the importance of past observations and how fast the series changes.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nWindowAverage\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nWindowAverage.forecast\n\n WindowAverage.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient WindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.fit\n\n WindowAverage.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the WindowAverage model.\nFit an WindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nWindowAverage fitted model.\n\n\n\n\nsource\n\n\nWindowAverage.predict\n\n WindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted WindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nWindowAverage.predict_in_sample\n\n WindowAverage.predict_in_sample ()\n\nAccess fitted WindowAverage insample predictions.\n\n# WindowAverage's usage example\n\nfrom statsforecast.models import WindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = WindowAverage(window_size=12*4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([413.47916, 413.47916, 413.47916, 413.47916], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#seasonalwindowaverage",
    "href": "src/core/models.html#seasonalwindowaverage",
    "title": "Models",
    "section": "SeasonalWindowAverage",
    "text": "SeasonalWindowAverage\n\nsource\n\nSeasonalWindowAverage\n\n SeasonalWindowAverage (season_length:int, window_size:int,\n                        alias:str='SeasWA', prediction_intervals:Optional[\n                        statsforecast.utils.ConformalIntervals]=None)\n\nSeasonalWindowAverage model.\nAn average of the last \\(k\\) observations of the same period, with \\(k\\) the length of the window.\nReferences: Rob J. Hyndman and George Athanasopoulos (2018). “forecasting principles and practice, Simple Methods”.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n\n\n\n\nwindow_size\nint\n\nSize of truncated series on which average is estimated.\n\n\nalias\nstr\nSeasWA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.forecast\n\n SeasonalWindowAverage.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient SeasonalWindowAverage predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.fit\n\n SeasonalWindowAverage.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the SeasonalWindowAverage model.\nFit an SeasonalWindowAverage to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenpus of shape (t, n_x).\n\n\nReturns\n\n\nSeasonalWindowAverage fitted model.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict\n\n SeasonalWindowAverage.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None)\n\nPredict with fitted SeasonalWindowAverage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nSeasonalWindowAverage.predict_in_sample\n\n SeasonalWindowAverage.predict_in_sample ()\n\nAccess fitted SeasonalWindowAverage insample predictions.\n\n# SeasonalWindowAverage's usage example\n\nfrom statsforecast.models import SeasonalWindowAverage\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = SeasonalWindowAverage(season_length=12, window_size=4)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([358.  , 338.  , 385.75, 388.25], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#adida",
    "href": "src/core/models.html#adida",
    "title": "Models",
    "section": "ADIDA",
    "text": "ADIDA\n\nsource\n\nADIDA\n\n ADIDA (alias:str='ADIDA', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nADIDA model.\nAggregate-Dissagregate Intermittent Demand Approach: Uses temporal aggregation to reduce the number of zero observations. Once the data has been agregated, it uses the optimized SES to generate the forecasts at the new level. It then breaks down the forecast to the original level using equal weights.\nADIDA specializes on sparse or intermittent series are series with very few non-zero observations. They are notoriously hard to forecast, and so, different methods have been developed especifically for them.\nReferences: Nikolopoulos, K., Syntetos, A. A., Boylan, J. E., Petropoulos, F., & Assimakopoulos, V. (2011). An aggregate–disaggregate intermittent demand approach (ADIDA) to forecasting: an empirical proposition and analysis. Journal of the Operational Research Society, 62(3), 544-554..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nADIDA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nADIDA.forecast\n\n ADIDA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient ADIDA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.fit\n\n ADIDA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the ADIDA model.\nFit an ADIDA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nADIDA fitted model.\n\n\n\n\nsource\n\n\nADIDA.predict\n\n ADIDA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted ADIDA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nADIDA.predict_in_sample\n\n ADIDA.predict_in_sample ()\n\nAccess fitted ADIDA insample predictions.\n\n# ADIDA's usage example\n\nfrom statsforecast.models import ADIDA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ADIDA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonclassic",
    "href": "src/core/models.html#crostonclassic",
    "title": "Models",
    "section": "CrostonClassic",
    "text": "CrostonClassic\n\nsource\n\nCrostonClassic\n\n CrostonClassic (alias:str='CrostonClassic', prediction_intervals:Optional\n                 [statsforecast.utils.ConformalIntervals]=None)\n\nCrostonClassic model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nwhere \\(\\hat{z}_t\\) and \\(\\hat{p}_t\\) are forecasted using SES. The smoothing parameter of both components is set equal to 0.1\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonClassic\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonClassic.forecast\n\n CrostonClassic.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient CrostonClassic predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.fit\n\n CrostonClassic.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonClassic model.\nFit an CrostonClassic to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonClassic fitted model.\n\n\n\n\nsource\n\n\nCrostonClassic.predict\n\n CrostonClassic.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted CrostonClassic.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonClassic.predict_in_sample\n\n CrostonClassic.predict_in_sample (level)\n\nAccess fitted CrostonClassic insample predictions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nlevel\n\n\n\n\nReturns\ndict\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# CrostonClassic's usage example\n\nfrom statsforecast.models import CrostonClassic\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonClassic()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([460.30276, 460.30276, 460.30276, 460.30276], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonoptimized",
    "href": "src/core/models.html#crostonoptimized",
    "title": "Models",
    "section": "CrostonOptimized",
    "text": "CrostonOptimized\n\nsource\n\nCrostonOptimized\n\n CrostonOptimized (alias:str='CrostonOptimized', prediction_intervals:Opti\n                   onal[statsforecast.utils.ConformalIntervals]=None)\n\nCrostonOptimized model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston’s method where the smooting paramater is optimally selected from the range \\([0.1,0.3]\\). Both the non-zero demand \\(z_t\\) and the inter-demand intervals \\(p_t\\) are smoothed separately, so their smoothing parameters can be different.\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonOptimized\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonOptimized.forecast\n\n CrostonOptimized.forecast (y:numpy.ndarray, h:int,\n                            X:Optional[numpy.ndarray]=None,\n                            X_future:Optional[numpy.ndarray]=None,\n                            level:Optional[List[int]]=None,\n                            fitted:bool=False)\n\nMemory Efficient CrostonOptimized predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.fit\n\n CrostonOptimized.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonOptimized model.\nFit an CrostonOptimized to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonOptimized fitted model.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict\n\n CrostonOptimized.predict (h:int, X:Optional[numpy.ndarray]=None,\n                           level:Optional[List[int]]=None)\n\nPredict with fitted CrostonOptimized.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonOptimized.predict_in_sample\n\n CrostonOptimized.predict_in_sample ()\n\nAccess fitted CrostonOptimized insample predictions.\n\n# CrostonOptimized's usage example\n\nfrom statsforecast.models import CrostonOptimized\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonOptimized()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#crostonsba",
    "href": "src/core/models.html#crostonsba",
    "title": "Models",
    "section": "CrostonSBA",
    "text": "CrostonSBA\n\nsource\n\nCrostonSBA\n\n CrostonSBA (alias:str='CrostonSBA', prediction_intervals:Optional[statsfo\n             recast.utils.ConformalIntervals]=None)\n\nCrostonSBA model.\nA method to forecast time series that exhibit intermittent demand. It decomposes the original time series into a non-zero demand size \\(z_t\\) and inter-demand intervals \\(p_t\\). Then the forecast is given by: \\[ \\hat{y}_t = \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nA variation of the classic Croston’s method that uses a debiasing factor, so that the forecast is given by: \\[ \\hat{y}_t = 0.95  \\frac{\\hat{z}_t}{\\hat{p}_t} \\]\nReferences: Croston, J. D. (1972). Forecasting and stock control for intermittent demands. Journal of the Operational Research Society, 23(3), 289-303..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nCrostonSBA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nCrostonSBA.forecast\n\n CrostonSBA.forecast (y:numpy.ndarray, h:int,\n                      X:Optional[numpy.ndarray]=None,\n                      X_future:Optional[numpy.ndarray]=None,\n                      level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient CrostonSBA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.fit\n\n CrostonSBA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the CrostonSBA model.\nFit an CrostonSBA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nCrostonSBA fitted model.\n\n\n\n\nsource\n\n\nCrostonSBA.predict\n\n CrostonSBA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None)\n\nPredict with fitted CrostonSBA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nCrostonSBA.predict_in_sample\n\n CrostonSBA.predict_in_sample ()\n\nAccess fitted CrostonSBA insample predictions.\n\n# CrostonSBA's usage example\n\nfrom statsforecast.models import CrostonSBA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = CrostonSBA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([437.28763, 437.28763, 437.28763, 437.28763], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#imapa",
    "href": "src/core/models.html#imapa",
    "title": "Models",
    "section": "IMAPA",
    "text": "IMAPA\n\nsource\n\nIMAPA\n\n IMAPA (alias:str='IMAPA', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nIMAPA model.\nIntermittent Multiple Aggregation Prediction Algorithm: Similar to ADIDA, but instead of using a single aggregation level, it considers multiple in order to capture different dynamics of the data. Uses the optimized SES to generate the forecasts at the new levels and then combines them using a simple average.\nReferences: Syntetos, A. A., & Boylan, J. E. (2021). Intermittent demand forecasting: Context, methods and applications. John Wiley & Sons..\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalias\nstr\nIMAPA\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nIMAPA.forecast\n\n IMAPA.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient IMAPA predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.fit\n\n IMAPA.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the IMAPA model.\nFit an IMAPA to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nIMAPA fitted model.\n\n\n\n\nsource\n\n\nIMAPA.predict\n\n IMAPA.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted IMAPA.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nIMAPA.predict_in_sample\n\n IMAPA.predict_in_sample ()\n\nAccess fitted IMAPA insample predictions.\n\n# IMAPA's usage example\n\nfrom statsforecast.models import IMAPA\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = IMAPA()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([461.7666, 461.7666, 461.7666, 461.7666], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#tsb",
    "href": "src/core/models.html#tsb",
    "title": "Models",
    "section": "TSB",
    "text": "TSB\n\nsource\n\nTSB\n\n TSB (alpha_d:float, alpha_p:float, alias:str='TSB', prediction_intervals:\n      Optional[statsforecast.utils.ConformalIntervals]=None)\n\nTSB model.\nTeunter-Syntetos-Babai: A modification of Croston’s method that replaces the inter-demand intervals with the demand probability \\(d_t\\), which is defined as follows.\n\\[\nd_t = \\begin{cases}\n    1  & \\text{if demand occurs at time t} \\\\\n    0  & \\text{otherwise.}\n\\end{cases}\n\\]\nHence, the forecast is given by\n\\[\\hat{y}_t= \\hat{d}_t\\hat{z_t}\\]\nBoth \\(d_t\\) and \\(z_t\\) are forecasted using SES. The smooting paramaters of each may differ, like in the optimized Croston’s method.\nReferences: Teunter, R. H., Syntetos, A. A., & Babai, M. Z. (2011). Intermittent demand: Linking forecasting to inventory obsolescence. European Journal of Operational Research, 214(3), 606-615.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nalpha_d\nfloat\n\nSmoothing parameter for demand.\n\n\nalpha_p\nfloat\n\nSmoothing parameter for probability.\n\n\nalias\nstr\nTSB\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nTSB.forecast\n\n TSB.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient TSB predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\n\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.fit\n\n TSB.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the TSB model.\nFit an TSB to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nTSB fitted model.\n\n\n\n\nsource\n\n\nTSB.predict\n\n TSB.predict (h:int, X:Optional[numpy.ndarray]=None,\n              level:Optional[List[int]]=None)\n\nPredict with fitted TSB.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTSB.predict_in_sample\n\n TSB.predict_in_sample ()\n\nAccess fitted TSB insample predictions.\n\n# TSB's usage example\n\nfrom statsforecast.models import TSB\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = TSB(alpha_d=0.5, alpha_p=0.5)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([439.256, 439.256, 439.256, 439.256], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#mstl",
    "href": "src/core/models.html#mstl",
    "title": "Models",
    "section": "MSTL",
    "text": "MSTL\n\nsource\n\nMSTL\n\n MSTL (season_length:Union[int,List[int]], trend_forecaster=AutoETS,\n       stl_kwargs:Optional[Dict]=None, alias:str='MSTL', prediction_interv\n       als:Optional[statsforecast.utils.ConformalIntervals]=None)\n\nMSTL model.\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) decomposes the time series in multiple seasonalities using LOESS. Then forecasts the trend using a custom non-seaonal model and each seasonality using a SeasonalNaive model.\nReferences: Bandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”..\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\ntyping.Union[int, typing.List[int]]\n\nNumber of observations per unit of time. For multiple seasonalities use a list.\n\n\ntrend_forecaster\nAutoETS\nAutoETS\nStatsForecast model used to forecast the trend component.\n\n\nstl_kwargs\ntyping.Optional[typing.Dict]\nNone\nExtra arguments to pass to statsmodels.tsa.seasonal.STL.The period and seasonal arguments are reserved.\n\n\nalias\nstr\nMSTL\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nMSTL.fit\n\n MSTL.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the MSTL model.\nFit MSTL to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nMSTL fitted model.\n\n\n\n\nsource\n\n\nMSTL.predict\n\n MSTL.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted MSTL.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.predict_in_sample\n\n MSTL.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted MSTL insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.forecast\n\n MSTL.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient MSTL predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nMSTL.forward\n\n MSTL.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n               X_future:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted MSTL model to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# MSTL's usage example\n\nfrom statsforecast.models import MSTL\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmstl_model = MSTL(season_length=[3, 12], trend_forecaster=AutoARIMA(prediction_intervals=ConformalIntervals(h=4, n_windows=2)))\nmstl_model = mstl_model.fit(y=ap)\ny_hat_dict = mstl_model.predict(h=4, level=[80])\ny_hat_dict\n\n{'mean': array([449.64812085, 411.68927237, 467.03003869, 472.74771836]),\n 'lo-80': array([439.9824947 , 400.44724213, 462.79116282, 463.15713004]),\n 'hi-80': array([459.31374699, 422.93130261, 471.26891456, 482.33830668])}"
  },
  {
    "objectID": "src/core/models.html#standard-theta-method",
    "href": "src/core/models.html#standard-theta-method",
    "title": "Models",
    "section": "Standard Theta Method",
    "text": "Standard Theta Method\n\nsource\n\nTheta\n\n Theta (season_length:int=1, decomposition_type:str='multiplicative',\n        alias:str='Theta', prediction_intervals:Optional[statsforecast.uti\n        ls.ConformalIntervals]=None)\n\nStandard Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nTheta.forecast\n\n Theta.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.fit\n\n Theta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nTheta.predict\n\n Theta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.predict_in_sample\n\n Theta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nTheta.forward\n\n Theta.forward (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# Theta's usage example\n\nfrom statsforecast.models import Theta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = Theta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.96918204, 429.24926382, 490.69323393, 476.65998055])}"
  },
  {
    "objectID": "src/core/models.html#optimized-theta-method",
    "href": "src/core/models.html#optimized-theta-method",
    "title": "Models",
    "section": "Optimized Theta Method",
    "text": "Optimized Theta Method\n\nsource\n\nOptimizedTheta\n\n OptimizedTheta (season_length:int=1,\n                 decomposition_type:str='multiplicative',\n                 alias:str='OptimizedTheta', prediction_intervals:Optional\n                 [statsforecast.utils.ConformalIntervals]=None)\n\nOptimized Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nOptimizedTheta\nCustom name of the model. Default OptimizedTheta.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nOptimizedTheta.forecast\n\n OptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                          X:Optional[numpy.ndarray]=None,\n                          X_future:Optional[numpy.ndarray]=None,\n                          level:Optional[List[int]]=None,\n                          fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.fit\n\n OptimizedTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict\n\n OptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.predict_in_sample\n\n OptimizedTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nOptimizedTheta.forward\n\n OptimizedTheta.forward (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetA's usage example\n\nfrom statsforecast.models import OptimizedTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = OptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94078295, 432.22936898, 495.30609727, 482.30625563])}"
  },
  {
    "objectID": "src/core/models.html#dynamic-standard-theta-method",
    "href": "src/core/models.html#dynamic-standard-theta-method",
    "title": "Models",
    "section": "Dynamic Standard Theta Method",
    "text": "Dynamic Standard Theta Method\n\nsource\n\nDynamicTheta\n\n DynamicTheta (season_length:int=1,\n               decomposition_type:str='multiplicative',\n               alias:str='DynamicTheta', prediction_intervals:Optional[sta\n               tsforecast.utils.ConformalIntervals]=None)\n\nDynamic Standard Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nDynamicTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nDynamicTheta.forecast\n\n DynamicTheta.forecast (y:numpy.ndarray, h:int,\n                        X:Optional[numpy.ndarray]=None,\n                        X_future:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.fit\n\n DynamicTheta.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicTheta.predict\n\n DynamicTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.predict_in_sample\n\n DynamicTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicTheta.forward\n\n DynamicTheta.forward (y:numpy.ndarray, h:int,\n                       X:Optional[numpy.ndarray]=None,\n                       X_future:Optional[numpy.ndarray]=None,\n                       level:Optional[List[int]]=None, fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# DynStandardThetaMethod's usage example\n\nfrom statsforecast.models import DynamicTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([440.77412474, 429.06190332, 490.48332496, 476.46133269])}"
  },
  {
    "objectID": "src/core/models.html#dynamic-optimized-theta-method",
    "href": "src/core/models.html#dynamic-optimized-theta-method",
    "title": "Models",
    "section": "Dynamic Optimized Theta Method",
    "text": "Dynamic Optimized Theta Method\n\nsource\n\nDynamicOptimizedTheta\n\n DynamicOptimizedTheta (season_length:int=1,\n                        decomposition_type:str='multiplicative',\n                        alias:str='DynamicOptimizedTheta', prediction_inte\n                        rvals:Optional[statsforecast.utils.ConformalInterv\n                        als]=None)\n\nDynamic Optimized Theta Method.\nReferences: Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler (2016). “Models for optimising the theta method and their relationship to state space models”. International Journal of Forecasting\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nseason_length\nint\n1\nNumber of observations per unit of time. Ex: 24 Hourly data.\n\n\ndecomposition_type\nstr\nmultiplicative\nSesonal decomposition type, ‘multiplicative’ (default) or ‘additive’.\n\n\nalias\nstr\nDynamicOptimizedTheta\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forecast\n\n DynamicOptimizedTheta.forecast (y:numpy.ndarray, h:int,\n                                 X:Optional[numpy.ndarray]=None,\n                                 X_future:Optional[numpy.ndarray]=None,\n                                 level:Optional[List[int]]=None,\n                                 fitted:bool=False)\n\nMemory Efficient AutoTheta predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.fit\n\n DynamicOptimizedTheta.fit (y:numpy.ndarray,\n                            X:Optional[numpy.ndarray]=None)\n\nFit the AutoTheta model.\nFit an AutoTheta model to a time series (numpy array) y and optionally exogenous variables (numpy array) X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\n\n\nAutoTheta fitted model.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict\n\n DynamicOptimizedTheta.predict (h:int, X:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None)\n\nPredict with fitted AutoTheta.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.predict_in_sample\n\n DynamicOptimizedTheta.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted AutoTheta insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nDynamicOptimizedTheta.forward\n\n DynamicOptimizedTheta.forward (y:numpy.ndarray, h:int,\n                                X:Optional[numpy.ndarray]=None,\n                                X_future:Optional[numpy.ndarray]=None,\n                                level:Optional[List[int]]=None,\n                                fitted:bool=False)\n\nApply fitted AutoTheta to a new time series.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# OptimzedThetaMethod's usage example\n\nfrom statsforecast.models import DynamicOptimizedTheta\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = DynamicOptimizedTheta(season_length=12)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([442.94256075, 432.31255941, 495.49774527, 482.58930649])}"
  },
  {
    "objectID": "src/core/models.html#garch-model",
    "href": "src/core/models.html#garch-model",
    "title": "Models",
    "section": "Garch model",
    "text": "Garch model\n\nsource\n\nGARCH\n\n GARCH (p:int=1, q:int=1, alias:str='GARCH', prediction_intervals:Optional\n        [statsforecast.utils.ConformalIntervals]=None)\n\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) model.\nA method for modeling time series that exhibit non-constant volatility over time. The GARCH model assumes that at time \\(t\\), \\(y_t\\) is given by:\n\\[ y_t = v_t \\sigma_t\\]\nwith\n\\[ \\sigma_t^2 = w + \\sum_{i=1}^p a_i y_{t-i}^2 + \\sum_{j=1}^q b_j \\sigma_{t-j}^2\\].\nHere {\\(v_t\\)} is a sequence of iid random variables with zero mean and unit variance. The coefficients \\(w\\), \\(a_i\\), \\(i=1,...,p\\), and \\(b_j\\), \\(j=1,...,q\\) must satisfy the following conditions:\n\n\\(w &gt; 0\\) and \\(a_i, b_j \\geq 0\\) for all \\(i\\) and \\(j\\).\n\\(\\sum_{k=1}^{max(p,q)} a_k + b_k &lt; 1\\). Here it is assumed that \\(a_i=0\\) for \\(i&gt;p\\) and \\(b_j=0\\) for \\(j&gt;q\\).\n\nThe ARCH model is a particular case of the GARCH model when \\(q=0\\).\nReferences: Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nBollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of econometrics, 31(3), 307-327.\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nint\n1\nNumber of lagged versions of the series.\n\n\nq\nint\n1\n\n\n\nalias\nstr\nGARCH\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nGARCH.fit\n\n GARCH.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit GARCH model.\nFit GARCH model to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nGARCH model.\n\n\n\n\nsource\n\n\nGARCH.predict\n\n GARCH.predict (h:int, X:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None)\n\nPredict with fitted GARCH model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nGARCH.predict_in_sample\n\n GARCH.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted GARCH model predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nGARCH.forecast\n\n GARCH.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                 X_future:Optional[numpy.ndarray]=None,\n                 level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient GARCH model.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions."
  },
  {
    "objectID": "src/core/models.html#arch-model",
    "href": "src/core/models.html#arch-model",
    "title": "Models",
    "section": "ARCH model",
    "text": "ARCH model\n\nsource\n\nARCH\n\n ARCH (p:int=1, alias:str='ARCH', prediction_intervals:Optional[statsforec\n       ast.utils.ConformalIntervals]=None)\n\nAutoregressive Conditional Heteroskedasticity (ARCH) model.\nA particular case of the GARCH(p,q) model where \\(q=0\\). It assumes that at time \\(t\\), \\(y_t\\) is given by:\n\\[ y_t = \\epsilon_t \\sigma_t\\]\nwith\n\\[ \\sigma_t^2 = w0 + \\sum_{i=1}^p a_i y_{t-i}^2\\].\nHere {\\(\\epsilon_t\\)} is a sequence of iid random variables with zero mean and unit variance. The coefficients \\(w\\) and \\(a_i\\), \\(i=1,...,p\\) must be nonnegative and \\(\\sum_{k=1}^p a_k &lt; 1\\).\nReferences: Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica: Journal of the econometric society, 987-1007.\nJames D. Hamilton. Time Series Analysis Princeton University Press, Princeton, New Jersey, 1st Edition, 1994.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nint\n1\nNumber of lagged versions of the series.\n\n\nalias\nstr\nARCH\nCustom name of the model.\n\n\nprediction_intervals\ntyping.Optional[statsforecast.utils.ConformalIntervals]\nNone\nInformation to compute conformal prediction intervals.By default, the model will compute the native predictionintervals.\n\n\n\n\nsource\n\n\nARCH.fit\n\n ARCH.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit GARCH model.\nFit GARCH model to a time series (numpy array) y.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nReturns\n\n\nGARCH model.\n\n\n\n\nsource\n\n\nARCH.predict\n\n ARCH.predict (h:int, X:Optional[numpy.ndarray]=None,\n               level:Optional[List[int]]=None)\n\nPredict with fitted GARCH model.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARCH.predict_in_sample\n\n ARCH.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted GARCH model predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nARCH.forecast\n\n ARCH.forecast (y:numpy.ndarray, h:int, X:Optional[numpy.ndarray]=None,\n                X_future:Optional[numpy.ndarray]=None,\n                level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient GARCH model.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n, ).\n\n\nh\nint\n\nForecast horizon.\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not returns insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions."
  },
  {
    "objectID": "src/core/models.html#constantmodel",
    "href": "src/core/models.html#constantmodel",
    "title": "Models",
    "section": "ConstantModel",
    "text": "ConstantModel\n\nsource\n\nConstantModel\n\n ConstantModel (constant:float, alias:str='ConstantModel')\n\nConstant Model.\nReturns Constant values.\n\nsource\n\n\nConstantModel.forecast\n\n ConstantModel.forecast (y:numpy.ndarray, h:int,\n                         X:Optional[numpy.ndarray]=None,\n                         X_future:Optional[numpy.ndarray]=None,\n                         level:Optional[List[int]]=None,\n                         fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nConstantModel.fit\n\n ConstantModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nConstantModel.predict\n\n ConstantModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                        level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nConstantModel.predict_in_sample\n\n ConstantModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# ConstantModel's usage example\n\nfrom statsforecast.models import ConstantModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ConstantModel(1)\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([1., 1., 1., 1.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#zeromodel",
    "href": "src/core/models.html#zeromodel",
    "title": "Models",
    "section": "ZeroModel",
    "text": "ZeroModel\n\nsource\n\nZeroModel\n\n ZeroModel (alias:str='ZeroModel')\n\nReturns Zero forecasts.\nReturns Zero values.\n\nsource\n\n\nZeroModel.forecast\n\n ZeroModel.forecast (y:numpy.ndarray, h:int,\n                     X:Optional[numpy.ndarray]=None,\n                     X_future:Optional[numpy.ndarray]=None,\n                     level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nZeroModel.fit\n\n ZeroModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nZeroModel.predict\n\n ZeroModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nZeroModel.predict_in_sample\n\n ZeroModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# NanModel's usage example\n\nfrom statsforecast.models import ZeroModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = ZeroModel()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([0., 0., 0., 0.], dtype=float32)}"
  },
  {
    "objectID": "src/core/models.html#nanmodel",
    "href": "src/core/models.html#nanmodel",
    "title": "Models",
    "section": "NaNModel",
    "text": "NaNModel\n\nsource\n\nNaNModel\n\n NaNModel (alias:str='NaNModel')\n\nNaN Model.\nReturns NaN values.\n\nsource\n\n\nNaNModel.forecast\n\n NaNModel.forecast (y:numpy.ndarray, h:int,\n                    X:Optional[numpy.ndarray]=None,\n                    X_future:Optional[numpy.ndarray]=None,\n                    level:Optional[List[int]]=None, fitted:bool=False)\n\nMemory Efficient Constant Model predictions.\nThis method avoids memory burden due from object storage. It is analogous to fit_predict without storing information. It assumes you know the forecast horizon in advance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (n,).\n\n\nh\nint\n\n\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional insample exogenous of shape (t, n_x).\n\n\nX_future\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (h, n_x).\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nfitted\nbool\nFalse\nWhether or not to return insample predictions.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaNModel.fit\n\n NaNModel.fit (y:numpy.ndarray, X:Optional[numpy.ndarray]=None)\n\nFit the Constant model.\nFit an Constant Model to a time series (numpy.array) y.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\nClean time series of shape (t, ).\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nOptional exogenous of shape (t, n_x).\n\n\nReturns\nself:\n\nConstant fitted model.\n\n\n\n\nsource\n\n\nNaNModel.predict\n\n NaNModel.predict (h:int, X:Optional[numpy.ndarray]=None,\n                   level:Optional[List[int]]=None)\n\nPredict with fitted ConstantModel.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nh\nint\n\nforecasting horizon\n\n\nX\ntyping.Optional[numpy.ndarray]\nNone\nexogenous regressors\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nconfidence level\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\nsource\n\n\nNaNModel.predict_in_sample\n\n NaNModel.predict_in_sample (level:Optional[List[int]]=None)\n\nAccess fitted Constant Model insample predictions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlevel\ntyping.Optional[typing.List[int]]\nNone\nConfidence levels (0-100) for prediction intervals.\n\n\nReturns\ndict\n\nDictionary with entries mean for point predictions and level_* for probabilistic predictions.\n\n\n\n\n# NanModel's usage example\n\nfrom statsforecast.models import NaNModel\nfrom statsforecast.utils import AirPassengers as ap\n\n\nmodel = NaNModel()\nmodel = model.fit(y=ap)\ny_hat_dict = model.predict(h=4)\ny_hat_dict\n\n{'mean': array([nan, nan, nan, nan], dtype=float32)}"
  },
  {
    "objectID": "src/theta.html",
    "href": "src/theta.html",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/theta.html#thetacalc",
    "href": "src/theta.html#thetacalc",
    "title": "Theta Model",
    "section": "",
    "text": "source\n\n\n\n theta_target_fn (optimal_param, init_level, init_alpha, init_theta,\n                  opt_level, opt_alpha, opt_theta, y, modeltype, nmse)\n\n\nsource\n\n\n\n\n is_constant (x)\n\n\nis_constant(ap)\n\n\nforecast_theta(res, 12, level=[90, 80])"
  },
  {
    "objectID": "src/core/models_intro.html",
    "href": "src/core/models_intro.html",
    "title": "StatsForecast’s Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n✅\n✅\n✅\n✅\n\n\nAutoETS\n✅\n✅\n✅\n✅\n\n\nAutoCES\n✅\n✅\n✅\n✅\n\n\nAutoTheta\n✅\n✅\n✅\n✅\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/core/models_intro.html#automatic-forecasting",
    "href": "src/core/models_intro.html#automatic-forecasting",
    "title": "StatsForecast’s Models",
    "section": "",
    "text": "Automatic forecasting tools search for the best parameters and select the best possible model for a series of time series. These tools are useful for large collections of univariate time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nAutoARIMA\n✅\n✅\n✅\n✅\n\n\nAutoETS\n✅\n✅\n✅\n✅\n\n\nAutoCES\n✅\n✅\n✅\n✅\n\n\nAutoTheta\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#arima-family",
    "href": "src/core/models_intro.html#arima-family",
    "title": "StatsForecast’s Models",
    "section": "ARIMA Family",
    "text": "ARIMA Family\nThese models exploit the existing autocorrelations in the time series.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nARIMA\n✅\n✅\n✅\n✅\n\n\nAutoRegressive\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#theta-family",
    "href": "src/core/models_intro.html#theta-family",
    "title": "StatsForecast’s Models",
    "section": "Theta Family",
    "text": "Theta Family\nFit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nTheta\n✅\n✅\n✅\n✅\n\n\nOptimizedTheta\n✅\n✅\n✅\n✅\n\n\nDynamicTheta\n✅\n✅\n✅\n✅\n\n\nDynamicOptimizedTheta\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#multiple-seasonalities",
    "href": "src/core/models_intro.html#multiple-seasonalities",
    "title": "StatsForecast’s Models",
    "section": "Multiple Seasonalities",
    "text": "Multiple Seasonalities\nSuited for signals with more than one clear seasonality. Useful for low-frequency data like electricity and logs.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nMSTL\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#garch-and-arch-models",
    "href": "src/core/models_intro.html#garch-and-arch-models",
    "title": "StatsForecast’s Models",
    "section": "GARCH and ARCH Models",
    "text": "GARCH and ARCH Models\nSuited for modeling time series that exhibit non-constant volatility over time. The ARCH model is a particular case of GARCH.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nGARCH\n✅\n✅\n✅\n✅\n\n\nARCH\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#baseline-models",
    "href": "src/core/models_intro.html#baseline-models",
    "title": "StatsForecast’s Models",
    "section": "Baseline Models",
    "text": "Baseline Models\nClassical models for establishing baseline.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nHistoricAverage\n✅\n✅\n✅\n✅\n\n\nNaive\n✅\n✅\n✅\n✅\n\n\nRandomWalkWithDrift\n✅\n✅\n✅\n✅\n\n\nSeasonalNaive\n✅\n✅\n✅\n✅\n\n\nWindowAverage\n✅\n\n\n\n\n\nSeasonalWindowAverage\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#exponential-smoothing",
    "href": "src/core/models_intro.html#exponential-smoothing",
    "title": "StatsForecast’s Models",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nUses a weighted average of all past observations where the weights decrease exponentially into the past. Suitable for data with clear trend and/or seasonality. Use the SimpleExponential family for data with no clear trend or seasonality.\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nSimpleExponentialSmoothing\n✅\n\n\n\n\n\nSimpleExponentialSmoothingOptimized\n✅\n\n\n\n\n\nSeasonalExponentialSmoothing\n✅\n\n\n\n\n\nSeasonalExponentialSmoothingOptimized\n✅\n\n\n\n\n\nHolt\n✅\n✅\n✅\n✅\n\n\nHoltWinters\n✅\n✅\n✅\n✅"
  },
  {
    "objectID": "src/core/models_intro.html#sparse-or-intermittent",
    "href": "src/core/models_intro.html#sparse-or-intermittent",
    "title": "StatsForecast’s Models",
    "section": "Sparse or Intermittent",
    "text": "Sparse or Intermittent\nSuited for series with very few non-zero observations\n\n\n\n\n\n\n\n\n\n\nModel\nPoint Forecast\nProbabilistic Forecast\nInsample fitted values\nProbabilistic fitted values\n\n\n\n\nADIDA\n✅\n\n\n\n\n\nCrostonClassic\n✅\n\n\n\n\n\nCrostonOptimized\n✅\n\n\n\n\n\nCrostonSBA\n✅\n\n\n\n\n\nIMAPA\n✅\n\n\n\n\n\nTSB\n✅"
  },
  {
    "objectID": "src/core/distributed.fugue.html",
    "href": "src/core/distributed.fugue.html",
    "title": "FugueBackend",
    "section": "",
    "text": "from statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\n\nsf.cross_validation(df=series, h=horizon, step_size = 24,\n    n_windows = 2, level=[90]).head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n0\n2000-07-10\n2000-07-09\n2.472186\n2.264802\n2.029021\n2.500583\n\n\n0\n2000-07-11\n2000-07-09\n3.369775\n3.207784\n2.972003\n3.443565\n\n\n0\n2000-07-12\n2000-07-09\n4.245229\n4.248131\n4.012350\n4.483912\n\n\n0\n2000-07-13\n2000-07-09\n5.113708\n5.267366\n5.031586\n5.503148\n\n\n0\n2000-07-14\n2000-07-09\n6.127178\n6.203136\n5.967356\n6.438918\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\n# Convert to Spark\nsdf = spark.createDataFrame(series)\n\n# Returns a Spark DataFrame\nsf.cross_validation(df=sdf, h=horizon, step_size = 24,\n    n_windows = 2, level=[90]).show()\n\n+---------+-------------------+-------------------+----------+----------+-------------+-------------+\n|unique_id|                 ds|             cutoff|         y|   AutoETS|AutoETS-lo-90|AutoETS-hi-90|\n+---------+-------------------+-------------------+----------+----------+-------------+-------------+\n|        0|2000-07-10 00:00:00|2000-07-09 00:00:00|  2.472186| 2.2648022|     2.029021|    2.5005832|\n|        0|2000-07-11 00:00:00|2000-07-09 00:00:00| 3.3697753| 3.2077837|    2.9720025|    3.4435647|\n|        0|2000-07-12 00:00:00|2000-07-09 00:00:00| 4.2452292|  4.248131|      4.01235|     4.483912|\n|        0|2000-07-13 00:00:00|2000-07-09 00:00:00| 5.1137075| 5.2673664|    5.0315857|    5.5031476|\n|        0|2000-07-14 00:00:00|2000-07-09 00:00:00|  6.127178| 6.2031364|    5.9673557|    6.4389176|\n|        0|2000-07-15 00:00:00|2000-07-09 00:00:00|0.02901458|0.29491338|   0.05913232|    0.5306944|\n|        0|2000-07-16 00:00:00|2000-07-09 00:00:00| 1.2172083| 1.2661165|    1.0303354|    1.5018975|\n|        0|2000-08-03 00:00:00|2000-08-02 00:00:00|  5.191732|  5.262299|     5.024893|    5.4997053|\n|        0|2000-08-04 00:00:00|2000-08-02 00:00:00| 6.2941585|  6.193887|     5.956481|     6.431294|\n|        0|2000-08-05 00:00:00|2000-08-02 00:00:00| 0.4155242|0.27708933|  0.039682955|   0.51449573|\n|        0|2000-08-06 00:00:00|2000-08-02 00:00:00| 1.3144909| 1.2614795|     1.024073|    1.4988859|\n|        0|2000-08-07 00:00:00|2000-08-02 00:00:00| 2.4363253| 2.2563472|    2.0189407|    2.4937534|\n|        0|2000-08-08 00:00:00|2000-08-02 00:00:00|  3.136771| 3.2256677|    2.9882612|     3.463074|\n|        0|2000-08-09 00:00:00|2000-08-02 00:00:00| 4.3990235| 4.2520094|     4.014603|    4.4894156|\n|        1|2000-03-07 00:00:00|2000-03-06 00:00:00| 1.1842923| 1.1227854|   0.88283557|    1.3627354|\n|        1|2000-03-08 00:00:00|2000-03-06 00:00:00| 2.0684502| 2.3335178|     2.093568|    2.5734677|\n|        1|2000-03-09 00:00:00|2000-03-06 00:00:00|  3.411059|  3.249278|    3.0093281|    3.4892278|\n|        1|2000-03-10 00:00:00|2000-03-06 00:00:00|  4.094924| 4.3513813|    4.1114316|    4.5913315|\n|        1|2000-03-11 00:00:00|2000-03-06 00:00:00| 5.2556596| 5.2070827|     4.967133|     5.447033|\n|        1|2000-03-12 00:00:00|2000-03-06 00:00:00| 6.1121583|   6.28834|      6.04839|      6.52829|\n+---------+-------------------+-------------------+----------+----------+-------------+-------------+\nonly showing top 20 rows\nsource\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/core/distributed.fugue.html#dask-distributed-predictions",
    "href": "src/core/distributed.fugue.html#dask-distributed-predictions",
    "title": "FugueBackend",
    "section": "Dask Distributed Predictions",
    "text": "Dask Distributed Predictions\nHere we provide an example for the distribution of the StatsForecast predictions using Fugue to execute the code in a Dask cluster.\nTo do it we instantiate the FugueBackend class with a DaskExecutionEngine.\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\nfrom fugue_dask import DaskExecutionEngine\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import Naive\nfrom statsforecast.utils import generate_series\n\n# Generate Synthetic Panel Data\ndf = generate_series(10).reset_index()\ndf['unique_id'] = df['unique_id'].astype(str)\ndf = dd.from_pandas(df, npartitions=10)\n\n# Instantiate FugueBackend with DaskExecutionEngine\ndask_client = Client()\nengine = DaskExecutionEngine(dask_client=dask_client)\n\nWe have simply create the class to the usual StatsForecast instantiation.\n\nsf = StatsForecast(models=[Naive()], freq='D')\n\n\nDistributed Forecast\nFor extremely fast distributed predictions we use FugueBackend as backend that operates like the original StatsForecast.forecast method.\nIt receives as input a pandas.DataFrame with columns [unique_id,ds,y] and exogenous, where the ds (datestamp) column should be of a format expected by Pandas. The y column must be numeric, and represents the measurement we wish to forecast. And the unique_id uniquely identifies the series in the panel data.\n\n# Distributed predictions with FugueBackend.\nsf.forecast(df=df, h=12).compute()\n\n\n\nDistributed Cross-Validation\nFor extremely fast distributed temporcal cross-validation we use cross_validation method that operates like the original StatsForecast.cross_validation method.\n\n# Distributed cross-validation with FugueBackend.\nsf.cross_validation(df=df, h=12, n_windows=2).compute()"
  },
  {
    "objectID": "src/utils.html",
    "href": "src/utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n generate_series (n_series:int, freq:str='D', min_length:int=50,\n                  max_length:int=500, n_static_features:int=0,\n                  equal_ends:bool=False, engine:str='pandas', seed:int=0)\n\nGenerate Synthetic Panel Series.\nGenerates n_series of frequency freq of different lengths in the interval [min_length, max_length]. If n_static_features &gt; 0, then each series gets static features with random values. If equal_ends == True then all series end at the same date.\nParameters: n_series: int, number of series for synthetic panel. min_length: int, minimal length of synthetic panel’s series. max_length: int, minimal length of synthetic panel’s series. n_static_features: int, default=0, number of static exogenous variables for synthetic panel’s series. equal_ends: bool, if True, series finish in the same date stamp ds. freq: str, frequency of the data, panda’s available frequencies. engine: str, engine to be used in DataFrame construction; NOTE: index does not exist in polars DataFrame\nReturns: freq: pandas.DataFrame | polars.DataFrame, synthetic panel with columns [unique_id, ds, y] and exogenous.\n\nfrom statsforecast.utils import generate_series\n\nsynthetic_panel = generate_series(n_series=2)\nsynthetic_panel.groupby('unique_id').head(4)\n\n\n\n\n\n\n\n\ny\nds\n\n\nunique_id\n\n\n\n\n\n\n0\n0.357595\n2000-01-01\n\n\n0\n1.301382\n2000-01-02\n\n\n0\n2.272442\n2000-01-03\n\n\n0\n3.211827\n2000-01-04\n\n\n1\n5.399023\n2000-01-01\n\n\n1\n6.092818\n2000-01-02\n\n\n1\n0.476396\n2000-01-03\n\n\n1\n1.343744\n2000-01-04\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/utils.html#model-utils",
    "href": "src/utils.html#model-utils",
    "title": "Utils",
    "section": "Model utils",
    "text": "Model utils"
  },
  {
    "objectID": "src/ets.html",
    "href": "src/ets.html",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/ets.html#etscalc",
    "href": "src/ets.html#etscalc",
    "title": "ETS Model",
    "section": "",
    "text": "source\n\n\n\n ets_target_fn (par, p_y, p_nstate, p_errortype, p_trendtype,\n                p_seasontype, p_damped, p_lower, p_upper, p_opt_crit,\n                p_nmse, p_bounds, p_m, p_optAlpha, p_optBeta, p_optGamma,\n                p_optPhi, p_givenAlpha, p_givenBeta, p_givenGamma,\n                p_givenPhi, alpha, beta, gamma, phi)\n\n\nsource\n\n\n\n\n is_constant (x)"
  },
  {
    "objectID": "src/arima.html",
    "href": "src/arima.html",
    "title": "ARIMA",
    "section": "",
    "text": "source\n\npredict_arima\n\n predict_arima (model, n_ahead, newxreg=None, se_fit=True)\n\n\nmyarima(ap, order=(2, 1, 1), seasonal={'order': (0, 1, 0), 'period': 12}, \n        constant=False, ic='aicc', method='CSS-ML')['aic']\n\n\nsource\n\n\narima_string\n\n arima_string (model, padding=False)\n\n\nsource\n\n\nforecast_arima\n\n forecast_arima (model, h=None, level=None, fan=False, xreg=None,\n                 blambda=None, bootstrap=False, npaths=5000, biasadj=None)\n\n\nsource\n\n\nfitted_arima\n\n fitted_arima (model, h=1)\n\nReturns h-step forecasts for the data used in fitting the model.\n\nsource\n\n\nauto_arima_f\n\n auto_arima_f (x, d=None, D=None, max_p=5, max_q=5, max_P=2, max_Q=2,\n               max_order=5, max_d=2, max_D=1, start_p=2, start_q=2,\n               start_P=1, start_Q=1, stationary=False, seasonal=True,\n               ic='aicc', stepwise=True, nmodels=94, trace=False,\n               approximation=None, method=None, truncate=None, xreg=None,\n               test='kpss', test_kwargs=None, seasonal_test='seas',\n               seasonal_test_kwargs=None, allowdrift=True, allowmean=True,\n               blambda=None, biasadj=False, period=1)\n\n\nsource\n\n\nprint_statsforecast_ARIMA\n\n print_statsforecast_ARIMA (model, digits=3, se=True)\n\n\nsource\n\n\nARIMASummary\n\n ARIMASummary (model)\n\nARIMA Summary.\n\nsource\n\n\nAutoARIMA\n\n AutoARIMA (d:Optional[int]=None, D:Optional[int]=None, max_p:int=5,\n            max_q:int=5, max_P:int=2, max_Q:int=2, max_order:int=5,\n            max_d:int=2, max_D:int=1, start_p:int=2, start_q:int=2,\n            start_P:int=1, start_Q:int=1, stationary:bool=False,\n            seasonal:bool=True, ic:str='aicc', stepwise:bool=True,\n            nmodels:int=94, trace:bool=False,\n            approximation:Optional[bool]=None, method:Optional[str]=None,\n            truncate:Optional[bool]=None, test:str='kpss',\n            test_kwargs:Optional[str]=None, seasonal_test:str='seas',\n            seasonal_test_kwargs:Optional[Dict]=None,\n            allowdrift:bool=True, allowmean:bool=True,\n            blambda:Optional[float]=None, biasadj:bool=False,\n            period:int=1)\n\nAn AutoARIMA estimator.\nReturns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided.\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/distributed.utils.html",
    "href": "src/distributed.utils.html",
    "title": "Distributed utils",
    "section": "",
    "text": "source\n\nforecast\n\n forecast (df, models, freq, h, fallback_model=None, X_df=None,\n           level=None,\n           parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\nsource\n\n\ncross_validation\n\n cross_validation (df, models, freq, h, n_windows=1, step_size=1,\n                   test_size=None, input_size=None,\n                   parallel:Optional[ForwardRef('ParallelBackend')]=None)\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "src/distributed.multiprocess.html",
    "href": "src/distributed.multiprocess.html",
    "title": "MultiprocessBackend",
    "section": "",
    "text": "source\n\nMultiprocessBackend\n\n MultiprocessBackend (n_jobs:int)\n\nMultiprocessBackend Parent Class for Distributed Computation.\nParameters: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nNotes:\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/spark.html",
    "href": "docs/how-to-guides/spark.html",
    "title": "Spark",
    "section": "",
    "text": "StatsForecast works on top of Spark, Dask, and Ray through Fugue. StatsForecast will read the input DataFrame and use the corresponding engine. For example, if the input is a Spark DataFrame, StatsForecast will use the existing Spark session to run the forecast.\nA benchmark (with older syntax) can be found here where we forecasted one million timeseries in under 15 minutes.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#installation",
    "href": "docs/how-to-guides/spark.html#installation",
    "title": "Spark",
    "section": "Installation",
    "text": "Installation\nAs long as Spark is installed and configured, StatsForecast will be able to use it. If executing on a distributed Spark cluster, make use the statsforecast library is installed across all the workers."
  },
  {
    "objectID": "docs/how-to-guides/spark.html#statsforecast-on-pandas",
    "href": "docs/how-to-guides/spark.html#statsforecast-on-pandas",
    "title": "Spark",
    "section": "StatsForecast on Pandas",
    "text": "StatsForecast on Pandas\nBefore running on Spark, it’s recommended to test on a smaller Pandas dataset to make sure everything is working. This example also helps show the small differences when using Spark.\n\nfrom statsforecast.core import StatsForecast\nfrom statsforecast.models import ( \n    AutoARIMA,\n    AutoETS,\n)\nfrom statsforecast.utils import generate_series\n\nn_series = 4\nhorizon = 7\n\nseries = generate_series(n_series)\n\nsf = StatsForecast(\n    models=[AutoETS(season_length=7)],\n    freq='D',\n)\nsf.forecast(df=series, h=horizon).head()\n\n\n\n\n\n\n\n\nds\nAutoETS\n\n\nunique_id\n\n\n\n\n\n\n0\n2000-08-10\n5.261609\n\n\n0\n2000-08-11\n6.196357\n\n\n0\n2000-08-12\n0.282309\n\n\n0\n2000-08-13\n1.264195\n\n\n0\n2000-08-14\n2.262453"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#executing-on-spark",
    "href": "docs/how-to-guides/spark.html#executing-on-spark",
    "title": "Spark",
    "section": "Executing on Spark",
    "text": "Executing on Spark\nTo run the forecasts distributed on Spark, just pass in a Spark DataFrame instead. Instead of having the unique_id as an index, it needs to be a column because Spark has no index.\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\n\n# Make unique_id a column\nseries = series.reset_index()\nseries['unique_id'] = series['unique_id'].astype(str)\n\n# Convert to Spark\nsdf = spark.createDataFrame(series)\n\n# Returns a Spark DataFrame\nsf.forecast(df=sdf, h=horizon, level=[90]).show(5)\n\n+---------+-------------------+----------+\n|unique_id|                 ds|   AutoETS|\n+---------+-------------------+----------+\n|        1|2000-04-07 00:00:00|  4.312628|\n|        1|2000-04-08 00:00:00|  5.228625|\n|        1|2000-04-09 00:00:00|   6.24151|\n|        1|2000-04-10 00:00:00|0.23369633|\n|        1|2000-04-11 00:00:00|  1.173954|\n+---------+-------------------+----------+\nonly showing top 5 rows"
  },
  {
    "objectID": "docs/how-to-guides/spark.html#helpful-configuration",
    "href": "docs/how-to-guides/spark.html#helpful-configuration",
    "title": "Spark",
    "section": "Helpful Configuration",
    "text": "Helpful Configuration\nThere are some Spark-specific configurations that may help optimize the workload.\n\"spark.speculation\": \"true\",\n\"spark.sql.shuffle.partitions\": \"8000\",\n\"spark.sql.adaptive.enabled\": \"false\",\n\"spark.task.cpus\": \"1\""
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#motivation",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#motivation",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Motivation",
    "text": "Motivation\nThe AutoARIMA model is widely used to forecast time series in production and as a benchmark. However, the python implementation (pmdarima) is so slow that prevent data scientist practioners from quickly iterating and deploying AutoARIMA in production for a large number of time series. In this notebook we present Nixtla’s AutoARIMA based on the R implementation (developed by Rob Hyndman) and optimized using numba."
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#example",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#example",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Example",
    "text": "Example\n\nLibraries\n\n!pip install statsforecast prophet statsmodels sklearn matplotlib pmdarima\n\n\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom itertools import product\nfrom multiprocessing import cpu_count, Pool # for prophet\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima as auto_arima_p\nfrom prophet import Prophet\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, _TS\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom sklearn.model_selection import ParameterGrid\n\nImporting plotly failed. Interactive plots will not work.\n\n\n\nUseful functions\nThe plot_grid function defined below will be useful to plot different time series, and different models’ forecasts.\n\ndef plot_grid(df_train, df_test=None, plot_random=True):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) &gt;= 8, \"Must provide at least 8 ts\"\n    \n    if plot_random:\n        unique_ids = random.sample(list(unique_ids), k=8)\n    else:\n        unique_uids = unique_ids[:8]\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        axes[idx, idy].plot(train_uid['ds'], train_uid['y'], label = 'y_train')\n        if df_test is not None:\n            max_ds = train_uid['ds'].max()\n            test_uid = df_test.query('unique_id == @uid')\n            for model in df_test.drop(['unique_id', 'ds'], axis=1).columns:\n                if all(np.isnan(test_uid[model])):\n                    continue\n                axes[idx, idy].plot(test_uid['ds'], test_uid[model], label=model)\n\n        axes[idx, idy].set_title(f'M4 Hourly: {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Target')\n        axes[idx, idy].legend(loc='upper left')\n        axes[idx, idy].xaxis.set_major_locator(plt.MaxNLocator(20))\n        axes[idx, idy].grid()\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\ndef plot_autocorrelation_grid(df_train):\n    fig, axes = plt.subplots(4, 2, figsize = (24, 14))\n\n    unique_ids = df_train['unique_id'].unique()\n\n    assert len(unique_ids) &gt;= 8, \"Must provide at least 8 ts\"\n\n    unique_ids = random.sample(list(unique_ids), k=8)\n\n    for uid, (idx, idy) in zip(unique_ids, product(range(4), range(2))):\n        train_uid = df_train.query('unique_id == @uid')\n        plot_acf(train_uid['y'].values, ax=axes[idx, idy], \n                 title=f'ACF M4 Hourly {uid}')\n        axes[idx, idy].set_xlabel('Timestamp [t]')\n        axes[idx, idy].set_ylabel('Autocorrelation')\n    fig.subplots_adjust(hspace=0.5)\n    plt.show()\n\n\n\n\nData\nFor testing purposes, we will use the Hourly dataset from the M4 competition.\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\n\n\n!wget https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\n\n\ntrain = pd.read_csv('M4-Hourly.csv')\ntest = pd.read_csv('M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n\nIn this example we will use a subset of the data to avoid waiting too long. You can modify the number of series if you want.\n\nn_series = 16\nuids = train['unique_id'].unique()[:n_series]\ntrain = train.query('unique_id in @uids')\ntest = test.query('unique_id in @uids')\n\n\nplot_grid(train, test)\n\n\n\n\nWould an autorregresive model be the right choice for our data? There is no doubt that we observe seasonal periods. The autocorrelation function (acf) can help us to answer the question. Intuitively, we have to observe a decreasing correlation to opt for an AR model.\n\nplot_autocorrelation_grid(train)\n\n\n\n\nThus, we observe a high autocorrelation for previous lags and also for the seasonal lags. Therefore, we will let auto_arima to handle our data.\n\n\nTraining and forecasting\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Hourly data, it would be benefitial to use 24 as seasonality.\n\n?AutoARIMA\n\n\nInit signature:\nAutoARIMA(\n    d: Optional[int] = None,\n    D: Optional[int] = None,\n    max_p: int = 5,\n    max_q: int = 5,\n    max_P: int = 2,\n    max_Q: int = 2,\n    max_order: int = 5,\n    max_d: int = 2,\n    max_D: int = 1,\n    start_p: int = 2,\n    start_q: int = 2,\n    start_P: int = 1,\n    start_Q: int = 1,\n    stationary: bool = False,\n    seasonal: bool = True,\n    ic: str = 'aicc',\n    stepwise: bool = True,\n    nmodels: int = 94,\n    trace: bool = False,\n    approximation: Optional[bool] = False,\n    method: Optional[str] = None,\n    truncate: Optional[bool] = None,\n    test: str = 'kpss',\n    test_kwargs: Optional[str] = None,\n    seasonal_test: str = 'seas',\n    seasonal_test_kwargs: Optional[Dict] = None,\n    allowdrift: bool = False,\n    allowmean: bool = False,\n    blambda: Optional[float] = None,\n    biasadj: bool = False,\n    parallel: bool = False,\n    num_cores: int = 2,\n    season_length: int = 1,\n)\nDocstring:      &lt;no docstring&gt;\nFile:           ~/fede/statsforecast/statsforecast/models.py\nType:           type\nSubclasses:     \n\n\n\n\nAs we see, we can pass season_length to AutoARIMA, so the definition of our models would be,\n\nmodels = [AutoARIMA(season_length=24, approximation=True)]\n\n\nfcst = StatsForecast(df=train, \n                     models=models, \n                     freq='H', \n                     n_jobs=-1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla = end - init\ntime_nixtla\n\n20.36360502243042\n\n\n\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\n\n\nunique_id\n\n\n\n\n\n\nH1\n701\n616.084167\n\n\nH1\n702\n544.432129\n\n\nH1\n703\n510.414490\n\n\nH1\n704\n481.046539\n\n\nH1\n705\n460.893066\n\n\n\n\n\n\n\n\nforecasts = forecasts.reset_index()\n\n\ntest = test.merge(forecasts, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)"
  },
  {
    "objectID": "docs/how-to-guides/autoarima_vs_prophet.html#alternatives",
    "href": "docs/how-to-guides/autoarima_vs_prophet.html#alternatives",
    "title": "AutoARIMA Comparison (Prophet and pmdarima)",
    "section": "Alternatives",
    "text": "Alternatives\n\npmdarima\nYou can use the StatsForecast class to parallelize your own models. In this section we will use it to run the auto_arima model from pmdarima.\n\nclass PMDAutoARIMA(_TS):\n    \n    def __init__(self, season_length: int):\n        self.season_length = season_length\n        \n    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n        mod = auto_arima_p(\n            y, m=self.season_length,\n            with_intercept=False #ensure comparability with Nixtla's implementation\n        ) \n        return {'mean': mod.predict(h)}\n    \n    def __repr__(self):\n        return 'pmdarima'\n\n\nn_series_pmdarima = 2\n\n\nfcst = StatsForecast(\n    df = train.query('unique_id in [\"H1\", \"H10\"]'), \n    models=[PMDAutoARIMA(season_length=24)],\n    freq='H',\n    n_jobs=-1\n)\n\n\ninit = time.time()\nforecast_pmdarima = fcst.forecast(48)\nend = time.time()\n\ntime_pmdarima = end - init\ntime_pmdarima\n\n349.93623208999634\n\n\n\nforecast_pmdarima.head()\n\n\n\n\n\n\n\n\nds\npmdarima\n\n\nunique_id\n\n\n\n\n\n\nH1\n701\n627.479370\n\n\nH1\n702\n570.364380\n\n\nH1\n703\n541.831482\n\n\nH1\n704\n516.475647\n\n\nH1\n705\n503.044586\n\n\n\n\n\n\n\n\nforecast_pmdarima = forecast_pmdarima.reset_index()\n\n\ntest = test.merge(forecast_pmdarima, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test, plot_random=False)\n\n\n\n\n\n\nProphet\nProphet is designed to receive a pandas dataframe, so we cannot use StatForecast. Therefore, we need to parallize from scratch.\n\nparams_grid = {'seasonality_mode': ['multiplicative','additive'],\n               'growth': ['linear', 'flat'], \n               'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5], \n               'n_changepoints': [5, 10, 15, 20]} \ngrid = ParameterGrid(params_grid)\n\n\ndef fit_and_predict(index, ts):\n    df = ts.drop(columns='unique_id', axis=1)\n    max_ds = df['ds'].max()\n    df['ds'] = pd.date_range(start='1970-01-01', periods=df.shape[0], freq='H')\n    df_val = df.tail(48) \n    df_train = df.drop(df_val.index) \n    y_val = df_val['y'].values\n    \n    if len(df_train) &gt;= 48:\n        val_results = {'losses': [], 'params': []}\n\n        for params in grid:\n            model = Prophet(seasonality_mode=params['seasonality_mode'],\n                            growth=params['growth'],\n                            weekly_seasonality=True,\n                            daily_seasonality=True,\n                            yearly_seasonality=True,\n                            n_changepoints=params['n_changepoints'],\n                            changepoint_prior_scale=params['changepoint_prior_scale'])\n            model = model.fit(df_train)\n            \n            forecast = model.make_future_dataframe(periods=48, \n                                                   include_history=False, \n                                                   freq='H')\n            forecast = model.predict(forecast)\n            forecast['unique_id'] = index\n            forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n            \n            loss = np.mean(abs(y_val - forecast['yhat'].values))\n            \n            val_results['losses'].append(loss)\n            val_results['params'].append(params)\n\n        idx_params = np.argmin(val_results['losses']) \n        params = val_results['params'][idx_params]\n    else:\n        params = {'seasonality_mode': 'multiplicative',\n                  'growth': 'flat',\n                  'n_changepoints': 150,\n                  'changepoint_prior_scale': 0.5}\n    model = Prophet(seasonality_mode=params['seasonality_mode'],\n                    growth=params['growth'],\n                    weekly_seasonality=True,\n                    daily_seasonality=True,\n                    yearly_seasonality=True,\n                    n_changepoints=params['n_changepoints'],\n                    changepoint_prior_scale=params['changepoint_prior_scale'])\n    model = model.fit(df)\n    \n    forecast = model.make_future_dataframe(periods=48, \n                                           include_history=False, \n                                           freq='H')\n    forecast = model.predict(forecast)\n    forecast.insert(0, 'unique_id', index)\n    forecast['ds'] = np.arange(max_ds + 1, max_ds + 48 + 1)\n    forecast = forecast.filter(items=['unique_id', 'ds', 'yhat'])\n    \n    return forecast\n\n\nlogging.getLogger('prophet').setLevel(logging.WARNING)\n\n\nclass suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n\n\ninit = time.time()\nwith suppress_stdout_stderr():\n    with Pool(cpu_count()) as pool:\n        forecast_prophet = pool.starmap(fit_and_predict, train.groupby('unique_id'))\nend = time.time()\nforecast_prophet = pd.concat(forecast_prophet).rename(columns={'yhat': 'prophet'})\ntime_prophet = end - init\ntime_prophet\n\n2022-08-19 23:07:24 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:25 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:41 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:07:42 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n2022-08-19 23:08:00 prophet.models WARNING: Optimization terminated abnormally. Falling back to Newton.\n\n\n120.9244737625122\n\n\n\nforecast_prophet\n\n\n\n\n\n\n\n\nunique_id\nds\nprophet\n\n\n\n\n0\nH1\n701\n631.867439\n\n\n1\nH1\n702\n561.001661\n\n\n2\nH1\n703\n499.299334\n\n\n3\nH1\n704\n456.132082\n\n\n4\nH1\n705\n431.884528\n\n\n...\n...\n...\n...\n\n\n43\nH112\n744\n5634.503804\n\n\n44\nH112\n745\n5622.643542\n\n\n45\nH112\n746\n5546.302705\n\n\n46\nH112\n747\n5457.777165\n\n\n47\nH112\n748\n5373.944098\n\n\n\n\n768 rows × 3 columns\n\n\n\n\ntest = test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_grid(train, test)\n\n\n\n\n\n\nEvaluation\n\n\nTime\nSince AutoARIMA works with numba is useful to calculate the time for just one time series.\n\nfcst = StatsForecast(df=train.query('unique_id == \"H1\"'), \n                     models=models, freq='H', \n                     n_jobs=1)\n\n\ninit = time.time()\nforecasts = fcst.forecast(48)\nend = time.time()\n\ntime_nixtla_1 = end - init\ntime_nixtla_1\n\n11.437001705169678\n\n\n\ntimes = pd.DataFrame({'n_series': np.arange(1, 414 + 1)})\ntimes['pmdarima'] = time_pmdarima * times['n_series'] / n_series_pmdarima\ntimes['prophet'] = time_prophet * times['n_series'] / n_series\ntimes['AutoARIMA_nixtla'] = time_nixtla_1 + times['n_series'] * (time_nixtla - time_nixtla_1) / n_series\ntimes = times.set_index('n_series')\n\n\ntimes.tail(5)\n\n\n\n\n\n\n\n\npmdarima\nprophet\nAutoARIMA_nixtla\n\n\nn_series\n\n\n\n\n\n\n\n410\n71736.927578\n3098.689640\n240.181212\n\n\n411\n71911.895694\n3106.247420\n240.739124\n\n\n412\n72086.863811\n3113.805199\n241.297037\n\n\n413\n72261.831927\n3121.362979\n241.854950\n\n\n414\n72436.800043\n3128.920759\n242.412863\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (24, 7))\n(times/3600).plot(ax=axes[0], linewidth=4)\nnp.log10(times).plot(ax=axes[1], linewidth=4)\naxes[0].set_title('Time across models [Hours]', fontsize=22)\naxes[1].set_title('Time across models [Log10 Scale]', fontsize=22)\naxes[0].set_ylabel('Time [Hours]', fontsize=20)\naxes[1].set_ylabel('Time Seconds [Log10 Scale]', fontsize=20)\nfig.suptitle('Time comparison using M4-Hourly data', fontsize=27)\nfor ax in axes:\n    ax.set_xlabel('Number of Time Series [N]', fontsize=20)\n    ax.legend(prop={'size': 20})\n    ax.grid()\n    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n        label.set_fontsize(20)\n\n\n\n\n\nfig.savefig('computational-efficiency.png', dpi=300)\n\n\n\nPerformance\n\npmdarima (only two time series)\n\nname_models = test.drop(['unique_id', 'ds', 'y_test'], 1).columns.tolist()\n\n\ntest_pmdarima = test.query('unique_id in [\"H1\", \"H10\"]')\neval_pmdarima = []\nfor model in name_models:\n    mae = np.mean(abs(test_pmdarima[model] - test_pmdarima['y_test']))\n    eval_pmdarima.append({'model': model, 'mae': mae})\npd.DataFrame(eval_pmdarima).sort_values('mae')\n\n\n\n\n\n\n\n\nmodel\nmae\n\n\n\n\n0\nAutoARIMA\n20.289669\n\n\n1\npmdarima\n26.461525\n\n\n2\nprophet\n43.155861\n\n\n\n\n\n\n\n\n\nProphet\n\neval_prophet = []\nfor model in name_models:\n    if 'pmdarima' in model:\n        continue\n    mae = np.mean(abs(test[model] - test['y_test']))\n    eval_prophet.append({'model': model, 'mae': mae})\npd.DataFrame(eval_prophet).sort_values('mae')\n\n\n\n\n\n\n\n\nmodel\nmae\n\n\n\n\n0\nAutoARIMA\n680.202970\n\n\n1\nprophet\n1066.049049\n\n\n\n\n\n\n\nFor a complete comparison check the complete experiment."
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html",
    "href": "docs/how-to-guides/ets_ray_m5.html",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "",
    "text": "In this notebook we show how to use StatsForecast and ray to forecast thounsands of time series in less than 6 minutes (M5 dataset). Also, we show that StatsForecast has better performance in time and accuracy compared to Prophet running on a Spark cluster using DataBricks.\nIn this example, we used a ray cluster (AWS) of 11 instances of type m5.2xlarge (8 cores, 32 GB RAM).\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#installing-statsforecast-library",
    "href": "docs/how-to-guides/ets_ray_m5.html#installing-statsforecast-library",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Installing StatsForecast Library",
    "text": "Installing StatsForecast Library\n\n!pip install \"statsforecast[ray]\" neuralforecast s3fs pyarrow\n\n\nfrom time import time\n\nimport pandas as pd\nfrom neuralforecast.data.datasets.m5 import M5, M5Evaluation\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import ETS"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#download-data",
    "href": "docs/how-to-guides/ets_ray_m5.html#download-data",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Download data",
    "text": "Download data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nY_df = pd.read_parquet('s3://m5-benchmarks/data/train/target.parquet')\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n\nY_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_1_001_CA_1\n2011-01-29\n3.0\n\n\n1\nFOODS_1_001_CA_1\n2011-01-30\n0.0\n\n\n2\nFOODS_1_001_CA_1\n2011-01-31\n0.0\n\n\n3\nFOODS_1_001_CA_1\n2011-02-01\n1.0\n\n\n4\nFOODS_1_001_CA_1\n2011-02-02\n4.0\n\n\n\n\n\n\n\nSince the M5 dataset contains intermittent time series, we add a constant to avoid problems during the training phase. Later, we will substract the constant from the forecasts.\n\nconstant = 10\nY_df['y'] += constant"
  },
  {
    "objectID": "docs/how-to-guides/ets_ray_m5.html#train-the-model",
    "href": "docs/how-to-guides/ets_ray_m5.html#train-the-model",
    "title": "Forecasting at Scale using ETS and ray (M5)",
    "section": "Train the model",
    "text": "Train the model\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality. Observe that we need to pass the ray address to the ray_address argument.\n\nfcst = StatsForecast(\n    df=Y_df, \n    models=[ETS(season_length=7, model='ZNA')], \n    freq='D', \n    #n_jobs=-1\n    ray_address='ray://ADDRESS:10001'\n)\n\n\ninit = time()\nY_hat = fcst.forecast(28)\nend = time()\nprint(f'Minutes taken by StatsForecast using: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/ray/lib/python3.7/site-packages/ray/util/client/worker.py:618: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n  UserWarning,\n\n\nMinutes taken by StatsForecast using: 5.4817593971888225\n\n\nStatsForecast and ray took only 5.48 minutes to train 30,490 time series, compared to 18.23 minutes for Prophet and Spark.\nWe remove the constant.\n\nY_hat['ETS'] -= constant\n\n\nEvaluating performance\nThe M5 competition used the weighted root mean squared scaled error. You can find details of the metric here.\n\nY_hat = Y_hat.reset_index().set_index(['unique_id', 'ds']).unstack()\nY_hat = Y_hat.droplevel(0, 1).reset_index()\n\n\n*_, S_df = M5.load('./data')\nY_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n\n100%|███████████████████████████████████████████████████████████| 50.2M/50.2M [00:00&lt;00:00, 77.1MiB/s]\n\n\n\nM5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n\n\n\n\n\n\n\n\nwrmsse\n\n\n\n\nTotal\n0.677233\n\n\nLevel1\n0.435558\n\n\nLevel2\n0.522863\n\n\nLevel3\n0.582109\n\n\nLevel4\n0.488484\n\n\nLevel5\n0.567825\n\n\nLevel6\n0.587605\n\n\nLevel7\n0.662774\n\n\nLevel8\n0.647712\n\n\nLevel9\n0.732107\n\n\nLevel10\n1.013124\n\n\nLevel11\n0.970465\n\n\nLevel12\n0.916175\n\n\n\n\n\n\n\nAlso, StatsForecast is more accurate than Prophet, since the overall WMRSSE is 0.68, against 0.77 obtained by prophet."
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html",
    "href": "docs/how-to-guides/automatic_forecasting.html",
    "title": "Automatic Time Series Forecasting",
    "section": "",
    "text": "Tip\n\n\n\nAutomatic forecasts of large numbers of univariate time series are often needed. It is common to have multiple product lines or skus that need forecasting. In these circumstances, an automatic forecasting algorithm is an essential tool. Automatic forecasting algorithms must determine an appropriate time series model, estimate the parameters and compute the forecasts. They must be robust to unusual time series patterns, and applicable to large numbers of series without user intervention.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#install-statsforecast-and-load-data",
    "href": "docs/how-to-guides/automatic_forecasting.html#install-statsforecast-and-load-data",
    "title": "Automatic Time Series Forecasting",
    "section": "1. Install statsforecast and load data",
    "text": "1. Install statsforecast and load data\nUse pip to install statsforecast and load Air Passangers dataset as an example\n\n!pip install statsforecast\n\nfrom statsforecast.utils import AirPassengersDF\n\nY_df = AirPassengersDF"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#import-statsforecast-and-models",
    "href": "docs/how-to-guides/automatic_forecasting.html#import-statsforecast-and-models",
    "title": "Automatic Time Series Forecasting",
    "section": "2. Import StatsForecast and models",
    "text": "2. Import StatsForecast and models\nImport the core StatsForecast class and the models you want to use\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA, AutoETS, AutoTheta, AutoCES"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#instatiate-the-class",
    "href": "docs/how-to-guides/automatic_forecasting.html#instatiate-the-class",
    "title": "Automatic Time Series Forecasting",
    "section": "3. Instatiate the class",
    "text": "3. Instatiate the class\nInstantiate the StatsForecast class with the appropriate parameters\n\nseason_length = 12 # Define season length as 12 months for monthly data\nhorizon = 1 # Forecast horizon is set to 1 month\n\n# Define a list of models for forecasting\nmodels = [\n    AutoARIMA(season_length=season_length), # ARIMA model with automatic order selection and seasonal component\n    AutoETS(season_length=season_length), # ETS model with automatic error, trend, and seasonal component\n    AutoTheta(season_length=season_length), # Theta model with automatic seasonality detection\n    AutoCES(season_length=season_length), # CES model with automatic seasonality detection\n]\n\n# Instantiate StatsForecast class with models, data frequency ('M' for monthly),\n# and parallel computation on all CPU cores (n_jobs=-1)\nsf = StatsForecast(\n    models=models, # models for forecasting\n    freq='M',  # frequency of the data\n    n_jobs=-1  # number of jobs to run in parallel, -1 means using all processors\n)"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#a-forecast-with-forecast-method",
    "href": "docs/how-to-guides/automatic_forecasting.html#a-forecast-with-forecast-method",
    "title": "Automatic Time Series Forecasting",
    "section": "4. a) Forecast with forecast method",
    "text": "4. a) Forecast with forecast method\nThe .forecast method is faster for distributed computing and does not save the fittted models\n\n# Generate forecasts for the specified horizon using the sf object\nY_hat_df = sf.forecast(df=Y_df, h=horizon) # forecast data\n\n# Display the first few rows of the forecast DataFrame\nY_hat_df.head() # preview of forecasted data"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#b-forecast-with-fit-and-predict",
    "href": "docs/how-to-guides/automatic_forecasting.html#b-forecast-with-fit-and-predict",
    "title": "Automatic Time Series Forecasting",
    "section": "4. b) Forecast with fit and predict",
    "text": "4. b) Forecast with fit and predict\nThe .fit method saves the fitted models\n\nsf.fit(df=Y_df) # Fit the models to the data using the fit method of the StatsForecast object\n\nsf.fitted_ # Access fitted models from the StatsForecast object\n\nY_hat_df = sf.predict(h=horizon) # Predict or forecast 'horizon' steps ahead using the predict method\n\nY_hat_df.head() # Preview the first few rows of the forecasted data"
  },
  {
    "objectID": "docs/how-to-guides/automatic_forecasting.html#references",
    "href": "docs/how-to-guides/automatic_forecasting.html#references",
    "title": "Automatic Time Series Forecasting",
    "section": "References",
    "text": "References\nHyndman, RJ and Khandakar, Y (2008) “Automatic time series forecasting: The forecast package for R”, Journal of Statistical Software, 26(3)."
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html",
    "href": "docs/how-to-guides/exogenous.html",
    "title": "Exogenous Regressors",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#introduction",
    "href": "docs/how-to-guides/exogenous.html#introduction",
    "title": "Exogenous Regressors",
    "section": "Introduction",
    "text": "Introduction\nExogenous regressors are variables that can affect the values of a time series. They may not be directly related to the variable that is beging forecasted, but they can still have an impact on it. Examples of exogenous regressors are weather data, economic indicators, or promotional sales. They are typically collected from external sources and by incorporating them into a forecasting model, they can improve the accuracy of our predictions.\nBy the end of this tutorial, you’ll have a good understanding of how to incorporate exogenous regressors into StatsForecast’s models. Furthermore, you’ll see how to evaluate their performance and decide whether or not they can help enhance the forecast.\nOutline\n\nInstall libraries\nLoad and explore the data\nSplit train/test set\nAdd exogenous regressors\nCreate future exogenous regressors\nTrain model\nEvaluate results\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#install-libraries",
    "href": "docs/how-to-guides/exogenous.html#install-libraries",
    "title": "Exogenous Regressors",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#load-and-explore-the-data",
    "href": "docs/how-to-guides/exogenous.html#load-and-explore-the-data",
    "title": "Exogenous Regressors",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nIn this example, we’ll use a single time series from the M5 Competition dataset. This series represents the daily sales of a product in a Walmart store. We’ll first import the complete dataset from datasetsforecast, which you can install using pip install datasetsforecast.\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m5 import M5\n\nThe function to load the data is M5.load, which requieres the following argument.\n\ndirectory: (str) The directory where the data will be downloaded.\n\nThis function returns multiple outputs. We need the first two.\n\nY_df: (pandas DataFrame) The target time series with columns [unique_id, ds, y].\nX_df: (pandas DataFrame) Exogenous time series with columns [unique_id, ds, exogenous regressors].\n\n\nY_df, X_df, *_ = M5.load('./data')\n\nINFO:datasetsforecast.utils:Successfully downloaded m5.zip, 50219189, bytes.\nINFO:datasetsforecast.utils:Decompressing zip file...\nINFO:datasetsforecast.utils:Successfully decompressed data/m5/datasets/m5.zip\n\n\nWe now need to filter the dataset. The product-store combination that we’ll use in this notebook has unique_id = FOODS_3_586_CA_3. This time series was chosen because it is not intermittent and has exogenous regressors that will be useful for forecasting.\n\n# Filter data \nY_ts = Y_df[Y_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\nX_ts = X_df[X_df['unique_id'] == 'FOODS_3_586_CA_3'].reset_index(drop = True)\n\nY_ts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n56.0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n55.0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n45.0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n57.0\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n54.0\n\n\n\n\n\n\n\nWe can plot the sales of this product-store combination with the statsforecast.plot method from the StatsForecast class. This method has multiple parameters, and the requiered ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nengine: str = plotly. It can also be matplotlib. plotly generates interactive plots, while matplotlib generates static plots.\n\n\nfrom statsforecast import StatsForecast\n\n/Users/fedex/miniconda3/envs/statsforecast/lib/python3.10/site-packages/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n\n\n\nStatsForecast.plot(Y_ts)\n\n\n                                                \n\n\nThe M5 Competition included several exogenous regressors. Here we’ll use the following two.\n\nsell_price: The price of the product for the given store. The price is provided per week.\nsnap_CA: A binary variable indicating whether the store allows SNAP purchases (1 if yes, 0 otherwise). SNAP stands for Supplement Nutrition Assitance Program, and it gives individuals and families money to help them purchase food products.\n\n\nX_ts = X_ts[['unique_id', 'ds', 'sell_price', 'snap_CA']]\nX_ts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nsell_price\nsnap_CA\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n1.48\n0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n1.48\n0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n1.48\n0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n1.48\n1\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n1.48\n1\n\n\n\n\n\n\n\nHere the unique_id is a category, but for the exogenous regressors it needs to be a string.\n\nX_ts['unique_id'] = X_ts.unique_id.astype(str)\n\nWe can plot the exogenous regressors using plotly. We could use statsforecast.plot, but then one of the regressors must be renamed y, and the name must be changed back to the original before generating the forecast.\n\nStatsForecast.plot(X_ts)\n\n\n                                                \n\n\nFrom this plot, we can conclude that price has increased twice and that SNAP occurs at regular intervals."
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#split-traintest-set",
    "href": "docs/how-to-guides/exogenous.html#split-traintest-set",
    "title": "Exogenous Regressors",
    "section": "Split train/test set",
    "text": "Split train/test set\nIn the M5 Competition, participants had to forecast sales for the last 28 days in the dataset. We’ll use the same forecast horizon and create the train and test sets accordingly.\n\n# Extract dates for train and test set \ndates = Y_df['ds'].unique()\ndtrain = dates[:-28]\ndtest = dates[-28:]\n\nY_train = Y_ts.query('ds in @dtrain')\nY_test = Y_ts.query('ds in @dtest') \n\nX_train = X_ts.query('ds in @dtrain') \nX_test = X_ts.query('ds in @dtest')"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#add-exogenous-regressors",
    "href": "docs/how-to-guides/exogenous.html#add-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Add exogenous regressors",
    "text": "Add exogenous regressors\nThe exogenous regressors need to be place after the target variable y.\n\ntrain = Y_train.merge(X_ts, how = 'left', on = ['unique_id', 'ds']) \ntrain.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nsell_price\nsnap_CA\n\n\n\n\n0\nFOODS_3_586_CA_3\n2011-01-29\n56.0\n1.48\n0\n\n\n1\nFOODS_3_586_CA_3\n2011-01-30\n55.0\n1.48\n0\n\n\n2\nFOODS_3_586_CA_3\n2011-01-31\n45.0\n1.48\n0\n\n\n3\nFOODS_3_586_CA_3\n2011-02-01\n57.0\n1.48\n1\n\n\n4\nFOODS_3_586_CA_3\n2011-02-02\n54.0\n1.48\n1"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#create-future-exogenous-regressors",
    "href": "docs/how-to-guides/exogenous.html#create-future-exogenous-regressors",
    "title": "Exogenous Regressors",
    "section": "Create future exogenous regressors",
    "text": "Create future exogenous regressors\nWe need to include the future values of the exogenous regressors so that we can produce the forecasts. Notice that we already have this information in X_test.\n\nX_test.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nsell_price\nsnap_CA\n\n\n\n\n1941\nFOODS_3_586_CA_3\n2016-05-23\n1.68\n0\n\n\n1942\nFOODS_3_586_CA_3\n2016-05-24\n1.68\n0\n\n\n1943\nFOODS_3_586_CA_3\n2016-05-25\n1.68\n0\n\n\n1944\nFOODS_3_586_CA_3\n2016-05-26\n1.68\n0\n\n\n1945\nFOODS_3_586_CA_3\n2016-05-27\n1.68\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the future values of the exogenous regressors are not available, then they must be forecasted or the regressors need to be eliminated from the model. Without them, it is not possible to generate the forecast."
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#train-model",
    "href": "docs/how-to-guides/exogenous.html#train-model",
    "title": "Exogenous Regressors",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we’ll use AutoARIMA, which is one of the models available in StatsForecast that allows exogenous regressors. To use this model, we first need to import it from statsforecast.models and then we need to instatiate it. Given that we’re working with daily data, we need to set season_length = 7.\n\nfrom statsforecast.models import AutoARIMA\n\n# Create a list with the model and its instantiation parameters \nmodels = [AutoARIMA(season_length = 7)]\n\nNext, we need to instantiate a new StatsForecast object, which has the following parameters.\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    models=models, \n    freq='D', \n    n_jobs=-1\n)\n\nNow we’re ready to generate the forecast. To do this, we’ll use the forecast method, which takes the following arguments.\n\nh: An integer that represents the forecast horizon. In this case, we’ll forecast the next 28 days.\nX_df: A pandas dataframe with the future values of the exogenous regressors.\nlevel: A list of floats with the confidence levels of the prediction intervals. For example, level=[95] means that the range of values should include the actual future value with probability 95%.\n\n\nhorizon = 28\nlevel = [95]\n\nfcst = sf.forecast(df=train, h=horizon, X_df=X_test, level=level)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoARIMA\nAutoARIMA-lo-95\nAutoARIMA-hi-95\n\n\n\n\n0\nFOODS_3_586_CA_3\n2016-05-23\n72.956276\n44.109074\n101.803490\n\n\n1\nFOODS_3_586_CA_3\n2016-05-24\n71.138611\n40.761467\n101.515747\n\n\n2\nFOODS_3_586_CA_3\n2016-05-25\n68.140945\n37.550083\n98.731804\n\n\n3\nFOODS_3_586_CA_3\n2016-05-26\n65.485588\n34.841637\n96.129539\n\n\n4\nFOODS_3_586_CA_3\n2016-05-27\n64.961441\n34.291969\n95.630905\n\n\n\n\n\n\n\nWe can plot the forecasts with the statsforecast.plot method described above.\n\nStatsForecast.plot(Y_ts, fcst, max_insample_length=28*2)"
  },
  {
    "objectID": "docs/how-to-guides/exogenous.html#evaluate-results",
    "href": "docs/how-to-guides/exogenous.html#evaluate-results",
    "title": "Exogenous Regressors",
    "section": "Evaluate results",
    "text": "Evaluate results\nWe’ll merge the test set and the forecast to evaluate the accuracy using the mean absolute error (MAE).\n\nres = Y_test.merge(fcst, how='left', on=['unique_id', 'ds'])\nres.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nAutoARIMA\nAutoARIMA-lo-95\nAutoARIMA-hi-95\n\n\n\n\n0\nFOODS_3_586_CA_3\n2016-05-23\n66.0\n72.956276\n44.109074\n101.803490\n\n\n1\nFOODS_3_586_CA_3\n2016-05-24\n62.0\n71.138611\n40.761467\n101.515747\n\n\n2\nFOODS_3_586_CA_3\n2016-05-25\n40.0\n68.140945\n37.550083\n98.731804\n\n\n3\nFOODS_3_586_CA_3\n2016-05-26\n72.0\n65.485588\n34.841637\n96.129539\n\n\n4\nFOODS_3_586_CA_3\n2016-05-27\n69.0\n64.961441\n34.291969\n95.630905\n\n\n\n\n\n\n\n\nmae = abs(res['y']-res['AutoARIMA']).mean()\nprint('The MAE with exogenous regressors is '+str(round(mae,2)))\n\nThe MAE with exogenous regressors is 11.42\n\n\nTo check whether the exogenous regressors were useful or not, we need to generate the forecast again, now without them. To do this, we simple pass the dataframe wihtout exogenous variables to the forecast method. Notice that the data only includes unique_id, ds, and y. The forecast method no longer requieres the future values of the exogenous regressors X_df.\n\n# univariate model \nfcst_u = sf.forecast(df=train[['unique_id', 'ds', 'y']], h=28)\n\nres_u = Y_test.merge(fcst_u, how='left', on=['unique_id', 'ds'])\nmae_u = abs(res_u['y']-res_u['AutoARIMA']).mean()\n\n\nprint('The MAE without exogenous regressors is '+str(round(mae_u,2)))\n\nThe MAE without exogenous regressors is 12.18\n\n\nHence, we can conclude that using sell_price and snap_CA as external regressors helped improve the forecast."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html",
    "href": "docs/tutorials/multipleseasonalities.html",
    "title": "Multiple seasonalities",
    "section": "",
    "text": "Tip\n\n\n\nFor this task, StatsForecast’s MSTL is 68% more accurate and 600% faster than Prophet and NeuralProphet. (Reproduce experiments here)\nMultiple seasonal data refers to time series that have more than one clear seasonality. Multiple seasonality is traditionally present in data that is sampled at a low frequency. For example, hourly electricity data exhibits daily and weekly seasonality. That means that there are clear patterns of electricity consumption for specific hours of the day like 6:00pm vs 3:00am or for specific days like Sunday vs Friday.\nTraditional statistical models are not able to model more than one seasonal length. In this example, we will show how to model the two seasonalities efficiently using Multiple Seasonal-Trend decompositions with LOESS (MSTL).\nFor this example, we will use hourly electricity load data from Pennsylvania, New Jersey, and Maryland (PJM). The original data can be found here. (Click here for info on PJM)\nFirst, we will load the data, then we will use the StatsForecast.fit and StatsForecast.predict methods to predict the next 24 hours. We will then decompose the different elements of the time series into trends and its multiple seasonalities. At the end, you will use the StatsForecast.forecast for production-ready forecasting.\nOutline\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#install-libraries",
    "href": "docs/tutorials/multipleseasonalities.html#install-libraries",
    "title": "Multiple seasonalities",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast ``"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#load-data",
    "href": "docs/tutorials/multipleseasonalities.html#load-data",
    "title": "Multiple seasonalities",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nYou will read the data with pandas and change the necessary names. This step should take around 2s.\n\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/jnagura/Energy-consumption-prediction-analysis/master/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf.tail()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n32891\nPJM_Load_hourly\n2001-01-01 20:00:00\n35209.0\n\n\n32892\nPJM_Load_hourly\n2001-01-01 21:00:00\n34791.0\n\n\n32893\nPJM_Load_hourly\n2001-01-01 22:00:00\n33669.0\n\n\n32894\nPJM_Load_hourly\n2001-01-01 23:00:00\n31809.0\n\n\n32895\nPJM_Load_hourly\n2001-01-02 00:00:00\n29506.0\n\n\n\n\n\n\n\nStatsForecast can handle unsorted data, however, for plotting purposes, it is convenient to sort the data frame.\n\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA. In this case, it will print just one series given that we have just one unique_id.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(df)\n\n/Users/max.mergenthaler/Nixtla/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nThe time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#fit-an-mstl-model",
    "href": "docs/tutorials/multipleseasonalities.html#fit-an-mstl-model",
    "title": "Multiple seasonalities",
    "section": "Fit an MSTL model",
    "text": "Fit an MSTL model\nThe MSTL (Multiple Seasonal-Trend decompositions using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a non-seasonal model and each seasonality using a SeasonalNaive model. You can choose the non-seasonal model you want to use to forecast the trend component of the MSTL model. In this example, we will use an AutoARIMA.\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] for season length. The trend component will be forecasted with an AutoARIMA model. (You can also try with: AutoTheta, AutoCES, and AutoETS)\n\n# Create a list of models and instantiation parameters\n\nmodels = [MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)]\n\nWe fit the models by instantiating a new StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\nsf = StatsForecast(\n    models=models, # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nUse the fit method to fit each model to each time series. In this case, we are just fitting one model to one series. Check this guide to learn how to fit many models to many series.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nsf = sf.fit(df=df)"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#decompose-the-series",
    "href": "docs/tutorials/multipleseasonalities.html#decompose-the-series",
    "title": "Multiple seasonalities",
    "section": "Decompose the series",
    "text": "Decompose the series\nOnce the model is fitted, access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case, we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal168\nremainder\n\n\n\n\n0\n22259.0\n26183.898892\n-5215.124554\n609.000432\n681.225229\n\n\n1\n21244.0\n26181.599305\n-6255.673234\n603.823918\n714.250011\n\n\n2\n20651.0\n26179.294886\n-6905.329895\n636.820423\n740.214587\n\n\n3\n20421.0\n26176.985472\n-7073.420118\n615.825999\n701.608647\n\n\n4\n20713.0\n26174.670877\n-7062.395760\n991.521912\n609.202971\n\n\n...\n...\n...\n...\n...\n...\n\n\n32891\n36392.0\n33123.552727\n4387.149171\n-488.177882\n-630.524015\n\n\n32892\n35082.0\n33148.242575\n3479.852929\n-682.928737\n-863.166767\n\n\n32893\n33890.0\n33172.926165\n2307.808829\n-650.566775\n-940.168219\n\n\n32894\n32590.0\n33197.603322\n748.587723\n-555.177849\n-801.013195\n\n\n32895\n31569.0\n33222.273902\n-967.124123\n-265.895357\n-420.254422\n\n\n\n\n32896 rows × 5 columns\n\n\n\nWe will use matplotlib, to visualize the different components of the series.\n\nimport matplotlib.pyplot as plt\n\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe a clear upward trend (orange line) and seasonality repeating every day (24H) and every week (168H)."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#predict-the-next-24-hours",
    "href": "docs/tutorials/multipleseasonalities.html#predict-the-next-24-hours",
    "title": "Multiple seasonalities",
    "section": "Predict the next 24 hours",
    "text": "Predict the next 24 hours\n\nProbabilistic forecasting with levels\n\nTo generate forecasts use the predict method.\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis step should take less than 1 second.\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266\n\n\n\n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\nsf.plot(df, forecasts, max_insample_length=24 * 7)"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#forecast-in-production",
    "href": "docs/tutorials/multipleseasonalities.html#forecast-in-production",
    "title": "Multiple seasonalities",
    "section": "Forecast in production",
    "text": "Forecast in production\nIf you want to gain speed in productive settings where you have multiple series or models we recommend using the StatsForecast.forecast method instead of .fit and .predict.\nThe main difference is that the .forecast doest not store the fitted values and is highly scalable in distributed environments.\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals. Depending on your computer, this step should take around 1min. (If you want to speed things up to a couple of seconds, remove the AutoModels like ARIMA and Theta)\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 10 seconds. The second time -once Numba compiled your settings- it should take less than 5s.\n\n\n\nforecasts_df = sf.forecast(h=24, level=[90])\n\nforecasts_df.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266"
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#references",
    "href": "docs/tutorials/multipleseasonalities.html#references",
    "title": "Multiple seasonalities",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”."
  },
  {
    "objectID": "docs/tutorials/multipleseasonalities.html#next-steps",
    "href": "docs/tutorials/multipleseasonalities.html#next-steps",
    "title": "Multiple seasonalities",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearn how to use cross-validation to assess the robustness of your model."
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html",
    "href": "docs/tutorials/anomalydetection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Prerequesites\n\n\n\n\n\nThis tutorial assumes basic familiarity with StatsForecast. For a minimal example visit the Quick Start\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#introduction",
    "href": "docs/tutorials/anomalydetection.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection is a crucial task in time series forecasting. It involves identifying unusual observations that don’t follow the expected dataset patterns. Anomalies, also known as outliers, can be caused by a variety of factors, such as errors in the data collection process, sudden changes in the underlying patterns of the data, or unexpected events. They can pose problems for many forecasting models since they can distort trends, seasonal patterns, or autocorrelation estimates. As a result, anomalies can have a significant impact on the accuracy of the forecasts, and for this reason, it is essential to be able to identify them. Furthermore, anomaly detection has many applications across different industries, such as detecting fraud in financial data, monitoring the performance of online services, or identifying usual patterns in energy usage.\nBy the end of this tutorial, you’ll have a good understanding of how to detect anomalies in time series data using StatsForecast’s probabilistic models.\nOutline:\n\nInstall libraries\nLoad and explore data\nTrain model\nRecover insample forecasts and identify anomalies\n\n\n\n\n\n\n\nImportant\n\n\n\nOnce an anomaly has been identified, we must decide what to do with it. For example, we could remove it or replace it with another value. The correct course of action is context-dependent and beyond this notebook’s scope. Removing an anomaly will likely improve the accuracy of the forecast, but it can also underestimate the amount of randomness in the data.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#install-libraries",
    "href": "docs/tutorials/anomalydetection.html#install-libraries",
    "title": "Anomaly Detection",
    "section": "Install libraries",
    "text": "Install libraries\nWe assume that you have StatsForecast already installed. If not, check this guide for instructions on how to install StatsForecast\nInstall the necessary packages using pip install statsforecast\n\npip install statsforecast -U"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#load-and-explore-the-data",
    "href": "docs/tutorials/anomalydetection.html#load-and-explore-the-data",
    "title": "Anomaly Detection",
    "section": "Load and explore the data",
    "text": "Load and explore the data\nFor this example, we’ll use the hourly dataset of the M4 Competition. We’ll first import the data from datasetsforecast, which you can install using pip install datasetsforecast\n\npip install datasetsforecast -U\n\n\nfrom datasetsforecast.m4 import M4\n\nThe function to load the data is M4.load. It requieres the following two arguments:\n\ndirectory: (str) The directory where the data will be downloaded.\ngroup: (str). The group name, which can be Yearly, Quarterly, Monthly, Weekly, Daily or Hourly.\n\nThis function returns multiple outputs, but only the first one with the target series is needed.\n\ndf_total, *_ = M4.load('./data', 'Hourly')\ndf_total.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nH1\n1\n605.0\n\n\n1\nH1\n2\n586.0\n\n\n2\nH1\n3\n586.0\n\n\n3\nH1\n4\n559.0\n\n\n4\nH1\n5\n511.0\n\n\n\n\n\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, df and y.\n\nunique_id: (string, int or category) A unique identifier for the series.\nds: (datestamp or int) A datestamp in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS or an integer indexing time.\ny: (numeric) The measurement we wish to forecast.\n\nIn this case, the unique_id and y columns already have the requiered format, but we need to change the data type of the ds column.\n\ndf_total['ds'] = df_total['ds'].astype(int)\n\nFrom this dataset, we’ll select the first 8 time series to reduce the total execution time. You can select any number you want by changing the value of n_series.\n\nn_series = 8 \nuids = df_total['unique_id'].unique()[:n_series]\ndf = df_total.query('unique_id in @uids')\n\nWe can plot these series using the plot method from the StatsForecast class. This method has multiple parameters, and the required ones to generate the plots in this notebook are explained below.\n\ndf: A pandas dataframe with columns [unique_id, ds, y].\nforecasts_df: A pandas dataframe with columns [unique_id, ds] and models.\nunique_ids: (list[str]) A list with the ids of the time series we want to plot.\nplot_random: (bool = True) Plots the time series randomly.\nplot_anomalies: (bool = False) Plots anomalies for each prediction interval.\nengine: (str = plotly). The library used to generate the plots. It can also be matplotlib for static plots.\n\n\nfrom statsforecast import StatsForecast\n\n\nStatsForecast.plot(df, plot_random = False)"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#train-model",
    "href": "docs/tutorials/anomalydetection.html#train-model",
    "title": "Anomaly Detection",
    "section": "Train model",
    "text": "Train model\nTo generate the forecast, we’ll use the MSTL model, which is well-suited for low-frequency data like the one used here. We first need to import it from statsforecast.models and then we need to instantiate it. Since we’re using hourly data, we have two seasonal periods: one every 24 hours (hourly) and one every 24*7 hours (daily). Hence, we need to set season_length = [24, 24*7].\n\nfrom statsforecast.models import MSTL\n\n# Create a list of models and instantiation parameters \nmodels = [MSTL(season_length = [24, 24*7])]\n\nTo instantiate a new StatsForecast object, we need the following parameters:\n\ndf: The dataframe with the training data.\nmodels: The list of models defined in the previous step.\nfreq: A string indicating the frequency of the data. See pandas’ available frequencies.\nn_jobs: An integer that indicates the number of jobs used in parallel processing. Use -1 to select all cores.\n\n\nsf = StatsForecast(\n    df = df, \n    models = models, \n    freq = 'H', \n    n_jobs = -1\n)\n\nWe’ll now predict the next 48 hours. To do this, we’ll use the forecast method, which requieres the following arguments:\n\nh: (int) The forecasting horizon.\nlevel: (list[float]) The confidence levels of the prediction intervals\nfitted: (bool = False) Returns insample predictions.\n\nIt is important that we select a level and set fitted = True since we’ll need the insample forecasts and their prediction intervals to detect the anomalies.\n\nhorizon = 48\nlevels = [99] \n\nfcst = sf.forecast(h = 48, level = levels, fitted = True)\nfcst = fcst.reset_index()\nfcst.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n0\nH1\n749\n615.943970\n597.662170\n634.225708\n\n\n1\nH1\n750\n559.297791\n531.316650\n587.278931\n\n\n2\nH1\n751\n515.693542\n479.151337\n552.235718\n\n\n3\nH1\n752\n480.719269\n436.241547\n525.197021\n\n\n4\nH1\n753\n467.146484\n415.199738\n519.093262\n\n\n\n\n\n\n\nWe can plot the forecasts using the plot method from before.\n\nStatsForecast.plot(df, fcst, plot_random = False)"
  },
  {
    "objectID": "docs/tutorials/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "href": "docs/tutorials/anomalydetection.html#recover-insample-forecasts-and-identify-anomalies",
    "title": "Anomaly Detection",
    "section": "Recover insample forecasts and identify anomalies",
    "text": "Recover insample forecasts and identify anomalies\nIn this example, an anomaly will be any observation outside the prediction interval of the insample forecasts for a given confidence level (here we selected 99%). Hence, we first need to recover the insample forecasts using the forecast_fitted_values method.\n\ninsample_forecasts = sf.forecast_fitted_values().reset_index()\ninsample_forecasts.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n0\nH1\n1\n605.0\n604.924500\n588.010376\n621.838623\n\n\n1\nH1\n2\n586.0\n585.221802\n568.307678\n602.135925\n\n\n2\nH1\n3\n586.0\n589.740723\n572.826599\n606.654846\n\n\n3\nH1\n4\n559.0\n557.778076\n540.863953\n574.692200\n\n\n4\nH1\n5\n511.0\n506.747009\n489.832886\n523.661133\n\n\n\n\n\n\n\nWe can now find all the observations above or below the 99% prediction interval for the insample forecasts.\n\nanomalies = insample_forecasts.loc[(insample_forecasts['y'] &gt;= insample_forecasts['MSTL-hi-99']) | (insample_forecasts['y'] &lt;= insample_forecasts['MSTL-lo-99'])]\nanomalies.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\nMSTL\nMSTL-lo-99\nMSTL-hi-99\n\n\n\n\n168\nH1\n169\n813.0\n779.849792\n762.935669\n796.763916\n\n\n279\nH1\n280\n692.0\n672.638123\n655.723999\n689.552246\n\n\n289\nH1\n290\n770.0\n792.015442\n775.101318\n808.929565\n\n\n308\nH1\n309\n844.0\n867.809387\n850.895203\n884.723511\n\n\n336\nH1\n337\n853.0\n822.427002\n805.512878\n839.341187\n\n\n\n\n\n\n\nWe can plot the anomalies by adding the plot_anomalies = True argument to the plot method.\n\nStatsForecast.plot(insample_forecasts, plot_random = False, plot_anomalies = True)\n\n\n                                                \n\n\nIf we want to take a closer look, we can use the unique_ids argument to select one particular time series, for example, H10.\n\nStatsForecast.plot(insample_forecasts, unique_ids = ['H10'], plot_anomalies = True)\n\n\n                                                \n\n\nHere we identified the anomalies in the data using the MSTL model, but any probabilistic model from StatsForecast can be used. We also selected the 99% prediction interval of the insample forecasts, but other confidence levels can be used as well."
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html",
    "href": "docs/tutorials/statisticalneuralmethods.html",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "",
    "text": "Statistical, Machine Learning, and Neural Forecasting Methods In this tutorial, we will explore the process of forecasting on the M5 dataset by utilizing the most suitable model for each time series. We’ll accomplish this through an essential technique known as cross-validation. This approach helps us in estimating the predictive performance of our models, and in selecting the model that yields the best performance for each time series.\nThe M5 dataset comprises of hierarchical sales data, spanning five years, from Walmart. The aim is to forecast daily sales for the next 28 days. The dataset is broken down into the 50 states of America, with 10 stores in each state.\nIn the realm of time series forecasting and analysis, one of the more complex tasks is identifying the model that is optimally suited for a specific group of series. Quite often, this selection process leans heavily on intuition, which may not necessarily align with the empirical reality of our dataset.\nIn this tutorial, we aim to provide a more structured, data-driven approach to model selection for different groups of series within the M5 benchmark dataset. This dataset, well-known in the field of forecasting, allows us to showcase the versatility and power of our methodology.\nWe will train an assortment of models from various forecasting paradigms:\nStatsForecast\nMLForecast\nMachine Learning: Leveraging ML models like LightGBM, XGBoost, and LinearRegression can be advantageous due to their capacity to uncover intricate patterns in data. We’ll use the MLForecast library for this purpose.\nNeuralForecast\nDeep Learning: DL models, such as Transformers (AutoTFT) and Neural Networks (AutoNHITS), allow us to handle complex non-linear dependencies in time series data. We’ll utilize the NeuralForecast library for these models.\nUsing the Nixtla suite of libraries, we’ll be able to drive our model selection process with data, ensuring we utilize the most suitable models for specific groups of series in our dataset.\nOutline:\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#installing-libraries",
    "href": "docs/tutorials/statisticalneuralmethods.html#installing-libraries",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Installing Libraries",
    "text": "Installing Libraries\n\n!pip install statsforecast mlforecast neuralforecast datasetforecast s3fs pyarrow"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#download-and-prepare-data",
    "href": "docs/tutorials/statisticalneuralmethods.html#download-and-prepare-data",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Download and prepare data",
    "text": "Download and prepare data\nThe example uses the M5 dataset. It consists of 30,490 bottom time series.\n\nimport pandas as pd\n\n\n# Load the training target dataset from the provided URL\nY_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet')\n\n# Rename columns to match the Nixtlaverse's expectations\n# The 'item_id' becomes 'unique_id' representing the unique identifier of the time series\n# The 'timestamp' becomes 'ds' representing the time stamp of the data points\n# The 'demand' becomes 'y' representing the target variable we want to forecast\nY_df = Y_df.rename(columns={\n    'item_id': 'unique_id', \n    'timestamp': 'ds', \n    'demand': 'y'\n})\n\n# Convert the 'ds' column to datetime format to ensure proper handling of date-related operations in subsequent steps\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nFor simplicity sake we will keep just one category\n\nY_df = Y_df.query('unique_id.str.startswith(\"FOODS_3\")').reset_index(drop=True)\n\nY_df['unique_id'] = Y_df['unique_id'].astype(str)"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#statsforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "StatsForecast",
    "text": "StatsForecast\nStatsForecast is a comprehensive library providing a suite of popular univariate time series forecasting models, all designed with a focus on high performance and scalability.\nHere’s what makes StatsForecast a powerful tool for time series forecasting:\n\nCollection of Local Models: StatsForecast provides a diverse collection of local models that can be applied to each time series individually, allowing us to capture unique patterns within each series.\nSimplicity: With StatsForecast, training, forecasting, and backtesting multiple models become a straightforward process, requiring only a few lines of code. This simplicity makes it a convenient tool for both beginners and experienced practitioners.\nOptimized for Speed: The implementation of the models in StatsForecast is optimized for speed, ensuring that large-scale computations are performed efficiently, thereby reducing the overall time for model training and prediction.\nHorizontal Scalability: One of the distinguishing features of StatsForecast is its ability to scale horizontally. It is compatible with distributed computing frameworks such as Spark, Dask, and Ray. This feature allows it to handle large datasets by distributing the computations across multiple nodes in a cluster, making it a go-to solution for large-scale time series forecasting tasks.\n\nStatsForecast receives a list of models to fit each time series. Since we are dealing with Daily data, it would be benefitial to use 7 as seasonality.\n\n# Import necessary models from the statsforecast library\nfrom statsforecast.models import (\n    # SeasonalNaive: A model that uses the previous season's data as the forecast\n    SeasonalNaive,\n    # Naive: A simple model that uses the last observed value as the forecast\n    Naive,\n    # HistoricAverage: This model uses the average of all historical data as the forecast\n    HistoricAverage,\n    # CrostonOptimized: A model specifically designed for intermittent demand forecasting\n    CrostonOptimized,\n    # ADIDA: Adaptive combination of Intermittent Demand Approaches, a model designed for intermittent demand\n    ADIDA,\n    # IMAPA: Intermittent Multiplicative AutoRegressive Average, a model for intermittent series that incorporates autocorrelation\n    IMAPA,\n    # AutoETS: Automated Exponential Smoothing model that automatically selects the best Exponential Smoothing model based on AIC\n    AutoETS\n)\n\nWe fit the models by instantiating a new StatsForecast object with the following parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\nn_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\nfallback_model: a model to be used if a model fails. Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\nhorizon = 28\nmodels = [\n    SeasonalNaive(season_length=7),\n    Naive(),\n    HistoricAverage(),\n    CrostonOptimized(),\n    ADIDA(),\n    IMAPA(),\n    AutoETS(season_length=7)\n]\n\n\n# Instantiate the StatsForecast class\nsf = StatsForecast(\n    models=models,  # A list of models to be used for forecasting\n    freq='D',  # The frequency of the time series data (in this case, 'D' stands for daily frequency)\n    n_jobs=-1,  # The number of CPU cores to use for parallel execution (-1 means use all available cores)\n)\n\nThe forecast method takes two arguments: forecasts next h (horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\nThis block of code times how long it takes to run the forecasting function of the StatsForecast class, which predicts the next 28 days (h=28). The level is set to [90], meaning it will compute the 90% prediction interval. The time is calculated in minutes and printed out at the end.\n\nfrom time import time\n\n# Get the current time before forecasting starts, this will be used to measure the execution time\ninit = time()\n\n# Call the forecast method of the StatsForecast instance to predict the next 28 days (h=28) \n# Level is set to [90], which means that it will compute the 90% prediction interval\nfcst_df = sf.forecast(df=Y_df, h=28, level=[90])\n\n# Get the current time after the forecasting ends\nend = time()\n\n# Calculate and print the total time taken for the forecasting in minutes\nprint(f'Forecast Minutes: {(end - init) / 60}')\n\nForecast Minutes: 2.270755163828532\n\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-05-23\n1.0\n-2.847174\n4.847174\n2.0\n0.098363\n3.901637\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.381414\n-1.028122\n1.790950\n\n\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n-3.847174\n3.847174\n2.0\n-0.689321\n4.689321\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.286933\n-1.124136\n1.698003\n\n\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n-3.847174\n3.847174\n2.0\n-1.293732\n5.293732\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.334987\n-1.077614\n1.747588\n\n\nFOODS_3_001_CA_1\n2016-05-26\n1.0\n-2.847174\n4.847174\n2.0\n-1.803274\n5.803274\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.186851\n-1.227280\n1.600982\n\n\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n-3.847174\n3.847174\n2.0\n-2.252190\n6.252190\n0.448738\n-1.009579\n1.907055\n0.345192\n0.345477\n0.347249\n0.308112\n-1.107548\n1.723771"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#mlforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#mlforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nMLForecast is a powerful library that provides automated feature creation for time series forecasting, facilitating the use of global machine learning models. It is designed for high performance and scalability.\nKey features of MLForecast include:\n\nSupport for sklearn models: MLForecast is compatible with models that follow the scikit-learn API. This makes it highly flexible and allows it to seamlessly integrate with a wide variety of machine learning algorithms.\nSimplicity: With MLForecast, the tasks of training, forecasting, and backtesting models can be accomplished in just a few lines of code. This streamlined simplicity makes it user-friendly for practitioners at all levels of expertise.\nOptimized for speed: MLForecast is engineered to execute tasks rapidly, which is crucial when handling large datasets and complex models.\nHorizontal Scalability: MLForecast is capable of horizontal scaling using distributed computing frameworks such as Spark, Dask, and Ray. This feature enables it to efficiently process massive datasets by distributing the computations across multiple nodes in a cluster, making it ideal for large-scale time series forecasting tasks.\n\n\nfrom mlforecast import MLForecast\nfrom mlforecast.target_transforms import Differences\nfrom mlforecast.utils import PredictionIntervals\nfrom window_ops.expanding import expanding_mean\n\n\n!pip install lightgbm xgboost\n\n\n# Import the necessary models from various libraries\n\n# LGBMRegressor: A gradient boosting framework that uses tree-based learning algorithms from the LightGBM library\nfrom lightgbm import LGBMRegressor\n\n# XGBRegressor: A gradient boosting regressor model from the XGBoost library\nfrom xgboost import XGBRegressor\n\n# LinearRegression: A simple linear regression model from the scikit-learn library\nfrom sklearn.linear_model import LinearRegression\n\nTo use MLForecast for time series forecasting, we instantiate a new MLForecast object and provide it with various parameters to tailor the modeling process to our specific needs:\n\nmodels: This parameter accepts a list of machine learning models you wish to use for forecasting. You can import your preferred models from scikit-learn, lightgbm and xgboost.\nfreq: This is a string indicating the frequency of your data (hourly, daily, weekly, etc.). The specific format of this string should align with pandas’ recognized frequency strings.\ntarget_transforms: These are transformations applied to the target variable before model training and after model prediction. This can be useful when working with data that may benefit from transformations, such as log-transforms for highly skewed data.\nlags: This parameter accepts specific lag values to be used as regressors. Lags represent how many steps back in time you want to look when creating features for your model. For example, if you want to use the previous day’s data as a feature for predicting today’s value, you would specify a lag of 1.\nlags_transforms: These are specific transformations for each lag. This allows you to apply transformations to your lagged features.\ndate_features: This parameter specifies date-related features to be used as regressors. For instance, you might want to include the day of the week or the month as a feature in your model.\nnum_threads: This parameter controls the number of threads to use for parallelizing feature creation, helping to speed up this process when working with large datasets.\n\nAll these settings are passed to the MLForecast constructor. Once the MLForecast object is initialized with these settings, we call its fit method and pass the historical data frame as the argument. The fit method trains the models on the provided historical data, readying them for future forecasting tasks.\n\n# Instantiate the MLForecast object\nmlf = MLForecast(\n    models=[LGBMRegressor(), XGBRegressor(), LinearRegression()],  # List of models for forecasting: LightGBM, XGBoost and Linear Regression\n    freq='D',  # Frequency of the data - 'D' for daily frequency\n    lags=list(range(1, 7)),  # Specific lags to use as regressors: 1 to 6 days\n    lag_transforms = {\n        1:  [expanding_mean],  # Apply expanding mean transformation to the lag of 1 day\n    },\n    date_features=['year', 'month', 'day', 'dayofweek', 'quarter', 'week'],  # Date features to use as regressors\n)\n\nJust call the fit models to train the select models. In this case we are generating conformal prediction intervals.\n\n# Start the timer to calculate the time taken for fitting the models\ninit = time()\n\n# Fit the MLForecast models to the data, with prediction intervals set using a window size of 28 days\nmlf.fit(Y_df, prediction_intervals=PredictionIntervals(window_size=28))\n\n# Calculate the end time after fitting the models\nend = time()\n\n# Print the time taken to fit the MLForecast models, in minutes\nprint(f'MLForecast Minutes: {(end - init) / 60}')\n\nMLForecast Minutes: 2.2809854547182717\n\n\nAfter that, just call predict to generate forecasts.\n\nfcst_mlf_df = mlf.predict(28, level=[90])\n\n\nfcst_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nLGBMRegressor\nXGBRegressor\nLinearRegression\nLGBMRegressor-lo-90\nLGBMRegressor-hi-90\nXGBRegressor-lo-90\nXGBRegressor-hi-90\nLinearRegression-lo-90\nLinearRegression-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n0.549520\n0.598431\n0.359638\n-0.213915\n1.312955\n-0.020050\n1.216912\n0.030000\n0.689277\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.553196\n0.337268\n0.100361\n-0.251383\n1.357775\n-0.201449\n0.875985\n-0.216195\n0.416917\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.599668\n0.349604\n0.175840\n-0.203974\n1.403309\n-0.284416\n0.983624\n-0.150593\n0.502273\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n0.638097\n0.322144\n0.156460\n0.118688\n1.157506\n-0.085872\n0.730160\n-0.273851\n0.586771\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.763305\n0.300362\n0.328194\n-0.313091\n1.839701\n-0.296636\n0.897360\n-0.657089\n1.313476"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#neuralforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#neuralforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nNeuralForecast is a robust collection of neural forecasting models that focuses on usability and performance. It includes a variety of model architectures, from classic networks such as Multilayer Perceptrons (MLP) and Recurrent Neural Networks (RNN) to novel contributions like N-BEATS, N-HITS, Temporal Fusion Transformers (TFT), and more.\nKey features of NeuralForecast include:\n\nA broad collection of global models. Out of the box implementation of MLP, LSTM, RNN, TCN, DilatedRNN, NBEATS, NHITS, ESRNN, TFT, Informer, PatchTST and HINT.\nA simple and intuitive interface that allows training, forecasting, and backtesting of various models in a few lines of code.\nSupport for GPU acceleration to improve computational speed.\n\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\n# Read the results from Colab\nfcst_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/forecast-nf.parquet')\n\n\nfcst_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n0.0\n2.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\n# Merge the forecasts from StatsForecast and NeuralForecast\nfcst_df = fcst_df.merge(fcst_nf_df, how='left', on=['unique_id', 'ds'])\n\n# Merge the forecasts from MLForecast into the combined forecast dataframe\nfcst_df = fcst_df.merge(fcst_mlf_df, how='left', on=['unique_id', 'ds'])\n\n\nfcst_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\n...\nAutoTFT-hi-90\nLGBMRegressor\nXGBRegressor\nLinearRegression\nLGBMRegressor-lo-90\nLGBMRegressor-hi-90\nXGBRegressor-lo-90\nXGBRegressor-hi-90\nLinearRegression-lo-90\nLinearRegression-hi-90\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-05-23\n1.0\n-2.847174\n4.847174\n2.0\n0.098363\n3.901637\n0.448738\n-1.009579\n...\n2.0\n0.549520\n0.598431\n0.359638\n-0.213915\n1.312955\n-0.020050\n1.216912\n0.030000\n0.689277\n\n\n1\nFOODS_3_001_CA_1\n2016-05-24\n0.0\n-3.847174\n3.847174\n2.0\n-0.689321\n4.689321\n0.448738\n-1.009579\n...\n2.0\n0.553196\n0.337268\n0.100361\n-0.251383\n1.357775\n-0.201449\n0.875985\n-0.216195\n0.416917\n\n\n2\nFOODS_3_001_CA_1\n2016-05-25\n0.0\n-3.847174\n3.847174\n2.0\n-1.293732\n5.293732\n0.448738\n-1.009579\n...\n1.0\n0.599668\n0.349604\n0.175840\n-0.203974\n1.403309\n-0.284416\n0.983624\n-0.150593\n0.502273\n\n\n3\nFOODS_3_001_CA_1\n2016-05-26\n1.0\n-2.847174\n4.847174\n2.0\n-1.803274\n5.803274\n0.448738\n-1.009579\n...\n2.0\n0.638097\n0.322144\n0.156460\n0.118688\n1.157506\n-0.085872\n0.730160\n-0.273851\n0.586771\n\n\n4\nFOODS_3_001_CA_1\n2016-05-27\n0.0\n-3.847174\n3.847174\n2.0\n-2.252190\n6.252190\n0.448738\n-1.009579\n...\n2.0\n0.763305\n0.300362\n0.328194\n-0.313091\n1.839701\n-0.296636\n0.897360\n-0.657089\n1.313476\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#forecast-plots",
    "href": "docs/tutorials/statisticalneuralmethods.html#forecast-plots",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Forecast plots",
    "text": "Forecast plots\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3)\n\n\n                                                \n\n\nUse the plot function to explore models and ID’s\n\nsf.plot(Y_df, fcst_df, max_insample_length=28 * 3, \n        models=['CrostonOptimized', 'AutoNHITS', 'SeasonalNaive', 'LGBMRegressor'])"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#cross-validation-in-statsforecast",
    "href": "docs/tutorials/statisticalneuralmethods.html#cross-validation-in-statsforecast",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Cross Validation in StatsForecast",
    "text": "Cross Validation in StatsForecast\nThe cross_validation method from the StatsForecast class accepts the following arguments:\n\ndf: A DataFrame representing the training data.\nh (int): The forecast horizon, represented as the number of steps into the future that we wish to predict. For example, if we’re forecasting hourly data, h=24 would represent a 24-hour forecast.\nstep_size (int): The step size between each cross-validation window. This parameter determines how often we want to run the forecasting process.\nn_windows (int): The number of windows used for cross validation. This parameter defines how many past forecasting processes we want to evaluate.\n\nThese parameters allow us to control the extent and granularity of our cross-validation process. By tuning these settings, we can balance between computational cost and the thoroughness of the cross-validation.\n\ninit = time()\ncv_df = sf.cross_validation(df=Y_df, h=horizon, n_windows=3, step_size=horizon, level=[90])\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n/home/ubuntu/statsforecast/statsforecast/ets.py:1041: RuntimeWarning:\n\ndivide by zero encountered in double_scalars\n\n\n\nCV Minutes: 5.206169327100118\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\nNaive\nNaive-lo-90\nNaive-hi-90\nHistoricAverage\nHistoricAverage-lo-90\nHistoricAverage-hi-90\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoETS-lo-90\nAutoETS-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n2.0\n-1.878885\n5.878885\n0.0\n-1.917011\n1.917011\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.655286\n-0.765731\n2.076302\n\n\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.0\n-3.878885\n3.878885\n0.0\n-2.711064\n2.711064\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.568595\n-0.853966\n1.991155\n\n\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.0\n-3.878885\n3.878885\n0.0\n-3.320361\n3.320361\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.618805\n-0.805298\n2.042908\n\n\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n1.0\n-2.878885\n4.878885\n0.0\n-3.834023\n3.834023\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.455891\n-0.969753\n1.881534\n\n\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n1.0\n-2.878885\n4.878885\n0.0\n-4.286568\n4.286568\n0.449111\n-1.021813\n1.920036\n0.618472\n0.618375\n0.617998\n0.591197\n-0.835987\n2.018380"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#mlforecast-1",
    "href": "docs/tutorials/statisticalneuralmethods.html#mlforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "MLForecast",
    "text": "MLForecast\nThe cross_validation method from the MLForecast class takes the following arguments.\n\ndata: training data frame\nwindow_size (int): represents h steps into the future that are being forecasted. In this case, 24 hours ahead.\nstep_size (int): step size between each window. In other words: how often do you want to run the forecasting processes.\nn_windows (int): number of windows used for cross-validation. In other words: what number of forecasting processes in the past do you want to evaluate.\nprediction_intervals: class to compute conformal intervals.\n\n\ninit = time()\ncv_mlf_df = mlf.cross_validation(\n    data=Y_df, \n    window_size=horizon, \n    n_windows=3, \n    step_size=horizon, \n    level=[90],\n)\nend = time()\nprint(f'CV Minutes: {(end - init) / 60}')\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:576: UserWarning:\n\nExcuting `cross_validation` after `fit` can produce unexpected errors\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/mlforecast/forecast.py:468: UserWarning:\n\nPlease rerun the `fit` method passing a proper value to prediction intervals to compute them.\n\n\n\nCV Minutes: 2.961174162228902\n\n\nThe crossvaldation_df object is a new data frame that includes the following columns:\n\nunique_id index: (If you dont like working with index just run forecasts_cv_df.resetindex())\nds: datestamp or temporal index\ncutoff: the last datestamp or temporal index for the n_windows. If n_windows=1, then one unique cuttoff value, if n_windows=2 then two unique cutoff values.\ny: true value\n\"model\": columns with the model’s name and fitted value.\n\n\ncv_mlf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\ny\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.435674\n0.556261\n-0.312492\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n1.0\n0.639676\n0.625806\n-0.041924\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n1.0\n0.792989\n0.659650\n0.263699\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.806868\n0.535121\n0.482491\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.829106\n0.313353\n0.677326"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#neuralforecast-1",
    "href": "docs/tutorials/statisticalneuralmethods.html#neuralforecast-1",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "NeuralForecast",
    "text": "NeuralForecast\nThis machine doesn’t have GPU, but Google Colabs offers some for free.\nUsing Colab’s GPU to train NeuralForecast.\n\ncv_nf_df = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/cross-validation-nf.parquet')\n\n\ncv_nf_df.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ncutoff\nAutoNHITS\nAutoNHITS-lo-90\nAutoNHITS-hi-90\nAutoTFT\nAutoTFT-lo-90\nAutoTFT-hi-90\ny\n\n\n\n\n0\nFOODS_3_001_CA_1\n2016-02-29\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n1\nFOODS_3_001_CA_1\n2016-03-01\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n2\nFOODS_3_001_CA_1\n2016-03-02\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n1.0\n\n\n3\nFOODS_3_001_CA_1\n2016-03-03\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0\n\n\n4\nFOODS_3_001_CA_1\n2016-03-04\n2016-02-28\n0.0\n0.0\n2.0\n1.0\n0.0\n2.0\n0.0"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "href": "docs/tutorials/statisticalneuralmethods.html#merge-cross-validation-forecasts",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Merge cross validation forecasts",
    "text": "Merge cross validation forecasts\n\ncv_df = cv_df.merge(cv_nf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])\ncv_df = cv_df.merge(cv_mlf_df.drop(columns=['y']), how='left', on=['unique_id', 'ds', 'cutoff'])"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#plots-cv",
    "href": "docs/tutorials/statisticalneuralmethods.html#plots-cv",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Plots CV",
    "text": "Plots CV\n\ncutoffs = cv_df['cutoff'].unique()\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        Y_df, \n        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n        max_insample_length=28 * 5, \n        unique_ids=['FOODS_3_001_CA_1'],\n    )\n    img.show()\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\n\nAggregate Demand\n\nagg_cv_df = cv_df.loc[:,~cv_df.columns.str.contains('hi|lo')].groupby(['ds', 'cutoff']).sum(numeric_only=True).reset_index()\nagg_cv_df.insert(0, 'unique_id', 'agg_demand')\n\n\nagg_Y_df = Y_df.groupby(['ds']).sum(numeric_only=True).reset_index()\nagg_Y_df.insert(0, 'unique_id', 'agg_demand')\n\n\nfor cutoff in cutoffs:\n    img = sf.plot(\n        agg_Y_df, \n        agg_cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']),\n        max_insample_length=28 * 5,\n    )\n    img.show()"
  },
  {
    "objectID": "docs/tutorials/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "href": "docs/tutorials/statisticalneuralmethods.html#evaluation-per-series-and-cv-window",
    "title": "Statistical, Machine Learning and Neural Forecasting methods",
    "section": "Evaluation per series and CV window",
    "text": "Evaluation per series and CV window\nIn this section, we will evaluate the performance of each model for each time series and each cross validation window. Since we have many combinations, we will use dask to parallelize the evaluation. The parallelization will be done using fugue.\n\nfrom typing import List, Callable\n\nfrom distributed import Client\nfrom fugue import transform\nfrom fugue_dask import DaskExecutionEngine\nfrom datasetsforecast.losses import mse, mae, smape\n\nThe evaluate function receives a unique combination of a time series and a window, and calculates different metrics for each model in df.\n\ndef evaluate(df: pd.DataFrame, metrics: List[Callable]) -&gt; pd.DataFrame:\n    eval_ = {}\n    models = df.loc[:, ~df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\n    for model in models:\n        eval_[model] = {}\n        for metric in metrics:\n            eval_[model][metric.__name__] = metric(df['y'], df[model])\n    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n    eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n    eval_df.insert(0, 'unique_id', df['unique_id'].iloc[0])\n    return eval_df\n\n\nstr_models = cv_df.loc[:, ~cv_df.columns.str.contains('unique_id|y|ds|cutoff|lo|hi')].columns\nstr_models = ','.join([f\"{model}:float\" for model in str_models])\ncv_df['cutoff'] = cv_df['cutoff'].astype(str)\ncv_df['unique_id'] = cv_df['unique_id'].astype(str)\n\nLet’s cleate a dask client.\n\nclient = Client() # without this, dask is not in distributed mode\n# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\nengine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\": 96})\n\nThe transform function takes the evaluate functions and applies it to each combination of time series (unique_id) and cross validation window (cutoff) using the dask client we created before.\n\nevaluation_df = transform(\n    cv_df.loc[:, ~cv_df.columns.str.contains('lo|hi')], \n    evaluate, \n    engine=\"dask\",\n    params={'metrics': [mse, mae, smape]}, \n    schema=f\"unique_id:str,cutoff:str,metric:str, {str_models}\", \n    as_local=True,\n    partition={'by': ['unique_id', 'cutoff']}\n)\n\n/home/ubuntu/miniconda/envs/statsforecast/lib/python3.10/site-packages/distributed/client.py:3109: UserWarning:\n\nSending large graph of size 49.63 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n\n\n\n\nevaluation_df.head()\n\n\n\n\n\n\n\n\nunique_id\ncutoff\nmetric\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\n\n\n0\nFOODS_3_003_WI_3\n2016-02-28\nmse\n1.142857\n1.142857\n0.816646\n0.816471\n1.142857\n1.142857\n1.142857\n1.142857\n1.142857\n0.832010\n1.020361\n0.887121\n\n\n1\nFOODS_3_003_WI_3\n2016-02-28\nmae\n0.571429\n0.571429\n0.729592\n0.731261\n0.571429\n0.571429\n0.571429\n0.571429\n0.571429\n0.772788\n0.619949\n0.685413\n\n\n2\nFOODS_3_003_WI_3\n2016-02-28\nsmape\n71.428574\n71.428574\n158.813507\n158.516235\n200.000000\n200.000000\n200.000000\n71.428574\n71.428574\n145.901947\n188.159164\n178.883743\n\n\n3\nFOODS_3_013_CA_3\n2016-04-24\nmse\n4.000000\n6.214286\n2.406764\n3.561202\n2.267853\n2.267600\n2.268677\n2.750000\n2.125000\n2.160508\n2.370228\n2.289606\n\n\n4\nFOODS_3_013_CA_3\n2016-04-24\nmae\n1.500000\n2.142857\n1.214286\n1.340446\n1.214286\n1.214286\n1.214286\n1.107143\n1.142857\n1.140084\n1.157548\n1.148813\n\n\n\n\n\n\n\n\n# Calculate the mean metric for each cross validation window\nevaluation_df.groupby(['cutoff', 'metric']).mean(numeric_only=True)\n\n\n\n\n\n\n\n\n\nSeasonalNaive\nNaive\nHistoricAverage\nCrostonOptimized\nADIDA\nIMAPA\nAutoETS\nAutoNHITS\nAutoTFT\nLGBMRegressor\nXGBRegressor\nLinearRegression\n\n\ncutoff\nmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-02-28\nmae\n1.744289\n2.040496\n1.730704\n1.633017\n1.527965\n1.528772\n1.497553\n1.434938\n1.485419\n1.688403\n1.514102\n1.576320\n\n\nmse\n14.510710\n19.080585\n12.858994\n11.785032\n11.114497\n11.100909\n10.347847\n10.010982\n10.964664\n10.436206\n10.968788\n10.792831\n\n\nsmape\n85.202042\n87.719086\n125.418488\n124.749908\n127.591858\n127.704102\n127.790672\n79.132614\n80.983368\n118.489983\n140.420578\n127.043137\n\n\n2016-03-27\nmae\n1.795973\n2.106449\n1.754029\n1.662087\n1.570701\n1.572741\n1.535301\n1.432412\n1.502393\n1.712493\n1.600193\n1.601612\n\n\nmse\n14.810259\n26.044472\n12.804104\n12.020620\n12.083861\n12.120033\n11.315013\n9.445867\n10.762877\n10.723589\n12.924312\n10.943772\n\n\nsmape\n87.407471\n89.453247\n123.587196\n123.460030\n123.428459\n123.538521\n123.612991\n79.926781\n82.013168\n116.089699\n138.885941\n127.304871\n\n\n2016-04-24\nmae\n1.785983\n1.990774\n1.762506\n1.609268\n1.527627\n1.529721\n1.501820\n1.447401\n1.505127\n1.692946\n1.541845\n1.590985\n\n\nmse\n13.476350\n16.234917\n13.151311\n10.647048\n10.072225\n10.062395\n9.393439\n9.363891\n10.436214\n10.347073\n10.774202\n10.608137\n\n\nsmape\n89.238815\n90.685867\n121.124947\n119.721245\n120.325401\n120.345284\n120.649582\n81.402748\n83.614029\n113.334198\n136.755234\n124.618622\n\n\n\n\n\n\n\nResults showed in previous experiments.\n\n\n\nmodel\nMSE\n\n\n\n\nMQCNN\n10.09\n\n\nDeepAR-student_t\n10.11\n\n\nDeepAR-lognormal\n30.20\n\n\nDeepAR\n9.13\n\n\nNPTS\n11.53\n\n\n\nTop 3 models: DeepAR, AutoNHITS, AutoETS.\n\nDistribution of errors\n\n!pip install seaborn\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nevaluation_df_melted = pd.melt(evaluation_df, id_vars=['unique_id', 'cutoff', 'metric'], var_name='model', value_name='error')\n\n\nSMAPE\n\nsns.violinplot(evaluation_df_melted.query('metric==\"smape\"'), x='error', y='model')\n\n&lt;Axes: xlabel='error', ylabel='model'&gt;\n\n\n\n\n\n\n\n\nChoose models for groups of series\nFeature:\n\nA unified dataframe with forecasts for all different models\nEasy Ensamble\nE.g. Average predictions\nOr MinMax (Choosing is ensembling)\n\n\n# Choose the best model for each time series, metric, and cross validation window\nevaluation_df['best_model'] = evaluation_df.idxmin(axis=1, numeric_only=True)\n# count how many times a model wins per metric and cross validation window\ncount_best_model = evaluation_df.groupby(['cutoff', 'metric', 'best_model']).size().rename('n').to_frame().reset_index()\n# plot results\nsns.barplot(count_best_model, x='n', y='best_model', hue='metric')\n\n&lt;Axes: xlabel='n', ylabel='best_model'&gt;\n\n\n\n\n\n\n\nEt pluribus unum: an inclusive forecasting Pie.\n\n# For the mse, calculate how many times a model wins\neval_series_df = evaluation_df.query('metric == \"mse\"').groupby(['unique_id']).mean(numeric_only=True)\neval_series_df['best_model'] = eval_series_df.idxmin(axis=1)\ncounts_series = eval_series_df.value_counts('best_model')\nplt.pie(counts_series, labels=counts_series.index, autopct='%.0f%%')\nplt.show()\n\n\n\n\n\nsf.plot(Y_df, cv_df.drop(columns=['cutoff', 'y']), \n        max_insample_length=28 * 6, \n        models=['AutoNHITS'],\n        unique_ids=eval_series_df.query('best_model == \"AutoNHITS\"').index[:8])"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html",
    "href": "docs/tutorials/electricitypeakforecasting.html",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#introduction",
    "href": "docs/tutorials/electricitypeakforecasting.html#introduction",
    "title": "Detect Demand Peaks",
    "section": "",
    "text": "Predicting peaks in different markets is useful. In the electricity market, consuming electricity at peak demand is penalized with higher tarifs. When an individual or company consumes electricity when its most demanded, regulators calls that a coincident peak (CP).\nIn the Texas electricity market (ERCOT), the peak is the monthly 15-minute interval when the ERCOT Grid is at a point of highest capacity. The peak is caused by all consumers’ combined demand on the electrical grid. The coincident peak demand is an important factor used by ERCOT to determine final electricity consumption bills. ERCOT registers the CP demand of each client for 4 months, between June and September, and uses this to adjust electricity prices. Clients can therefore save on electricity bills by reducing the coincident peak demand.\nIn this example we will train an MSTL (Multiple Seasonal-Trend decomposition using LOESS) model on historic load data to forecast day-ahead peaks on September 2022. Multiple seasonality is traditionally present in low sampled electricity data. Demand exhibits daily and weekly seasonality, with clear patterns for specific hours of the day such as 6:00pm vs 3:00am or for specific days such as Sunday vs Friday.\nFirst, we will load ERCOT historic demand, then we will use the StatsForecast.cross_validation method to fit the MSTL model and forecast daily load during September. Finally, we show how to use the forecasts to detect the coincident peak.\nOutline\n\nInstall libraries\nLoad and explore the data\nFit MSTL model and forecast\nPeak detection\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use Colab to run this Notebook interactively"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#libraries",
    "href": "docs/tutorials/electricitypeakforecasting.html#libraries",
    "title": "Detect Demand Peaks",
    "section": "Libraries",
    "text": "Libraries\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nInstall the necessary packages using pip install statsforecast"
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#load-data",
    "href": "docs/tutorials/electricitypeakforecasting.html#load-data",
    "title": "Detect Demand Peaks",
    "section": "Load Data",
    "text": "Load Data\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp or int) column should be either an integer indexing time or a datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast. We will rename the\n\nFirst, download and read the 2022 historic total demand of the ERCOT market, available here. The data processing includes adding the missing hour due to daylight saving time, parsing the date to datetime format, and filtering columns of interest. This step should take around 2s.\n\nimport numpy as np\nimport pandas as pd\n\n\n# Load data\nhistoric = pd.read_csv('./Native_Load_2022.csv')\n# Add missing hour due to daylight saving time\nhistoric = pd.concat([historic, pd.DataFrame({'Hour Ending':['03/13/2022 03:00'], 'ERCOT':['43980.57']})])\nhistoric = historic.sort_values('Hour Ending').reset_index(drop=True)\n# Convert to datetime\nhistoric['ERCOT'] = historic['ERCOT'].str.replace(',','').astype(float)\nhistoric = historic[~pd.isna(historic['ERCOT'])]\nhistoric['ds'] = pd.to_datetime(historic['Hour Ending'].str[:10]) + pd.to_timedelta(np.tile(range(24), len(historic)//24),'h')\nhistoric['unique_id'] = 'ERCOT'\nhistoric['y'] = historic['ERCOT']\n# Select relevant columns and dates\nY_df = historic[['unique_id', 'ds', 'y']]\nY_df = Y_df[Y_df['ds']&lt;='2022-10-01']\n\nPlot the series using the plot method from the StatsForecast class. This method prints up to 8 random series from the dataset and is useful for basic EDA.\n\n\n\n\n\n\nNote\n\n\n\nThe StatsForecast.plot method uses Plotly as a default engine. You can change to MatPlotLib by setting engine=\"matplotlib\".\n\n\n\nfrom statsforecast import StatsForecast\n\nStatsForecast.plot(Y_df)\n\n/Users/cchallu/NIXTLA/statsforecast/statsforecast/core.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n\n\n\n                                                \n\n\nWe observe that the time series exhibits seasonal patterns. Moreover, the time series contains 6,552 observations, so it is necessary to use computationally efficient methods to deploy them in production."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "href": "docs/tutorials/electricitypeakforecasting.html#fit-and-forecast-mstl-model",
    "title": "Detect Demand Peaks",
    "section": "Fit and Forecast MSTL model",
    "text": "Fit and Forecast MSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\n\n\n\n\n\n\nTip\n\n\n\nCheck our detailed explanation and tutorial on MSTL here\n\n\nImport the StatsForecast class and the models you need.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA\n\nFirst, instantiate the model and define the parameters. The electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities. See this link for a detailed explanation on how to set seasonal lengths. In this example we use the AutoARIMA model for the trend component, however, any StatsForecast model can be used. The complete list of models is available here.\n\nmodels = [MSTL(\n            season_length=[24, 24 * 7], # seasonalities of the time series \n            trend_forecaster=AutoARIMA(nmodels=10) # model used to forecast trend\n            )\n          ]\n\n\n\n\n\n\n\nTip\n\n\n\nThe parameter nmodels of the AutoARIMA controls the number of models considered in stepwise search. The default is 94, reduce it to decrease training times!\n\n\nWe fit the model by instantiating a StatsForecast object with the following required parameters:\n\nmodels: a list of models. Select the models you want from models and import them.\nfreq: a string indicating the frequency of the data. (See panda’s available frequencies.)\n\n\n# Instantiate StatsForecast class as sf\nsf = StatsForecast(\n    df=Y_df, \n    models=models,\n    freq='H', \n)\n\n\n\n\n\n\n\nTip\n\n\n\nStatsForecast also supports this optional parameter.\n\nn_jobs: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores. (Default: 1)\nfallback_model: a model to be used if a model fails. (Default: none)\n\n\n\nThe cross_validation method allows the user to simulate multiple historic forecasts, greatly simplifying pipelines by replacing for loops with fit and predict methods. This method re-trains the model and forecast each window. See this tutorial for an animation of how the windows are defined.\nUse the cross_validation method to produce all the daily forecasts for September. To produce daily forecasts set the forecasting horizon h as 24. In this example we are simulating deploying the pipeline during September, so set the number of windows as 30 (one for each day). Finally, set the step size between windows as 24, to only produce one forecast per day.\n\ncrossvalidation_df = sf.cross_validation(\n    df=Y_df,\n    h=24,\n    step_size=24,\n    n_windows=30\n  )\n\n\ncrossvalidation_df.head()\n\n\n\n\n\n\n\n\nds\ncutoff\ny\nMSTL\n\n\nunique_id\n\n\n\n\n\n\n\n\nERCOT\n2022-09-01 00:00:00\n2022-08-31 23:00:00\n45482.468750\n47126.179688\n\n\nERCOT\n2022-09-01 01:00:00\n2022-08-31 23:00:00\n43602.660156\n45088.542969\n\n\nERCOT\n2022-09-01 02:00:00\n2022-08-31 23:00:00\n42284.820312\n43897.175781\n\n\nERCOT\n2022-09-01 03:00:00\n2022-08-31 23:00:00\n41663.160156\n43187.812500\n\n\nERCOT\n2022-09-01 04:00:00\n2022-08-31 23:00:00\n41710.621094\n43369.859375\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using cross_validation make sure the forecasts are produced at the desired timestamps. Check the cutoff column which specifices the last timestamp before the forecasting window."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#peak-detection",
    "href": "docs/tutorials/electricitypeakforecasting.html#peak-detection",
    "title": "Detect Demand Peaks",
    "section": "Peak Detection",
    "text": "Peak Detection\nFinally, we use the forecasts in crossvaldation_df to detect the daily hourly demand peaks. For each day, we set the detected peaks as the highest forecasts. In this case, we want to predict one peak (npeaks); depending on your setting and goals, this parameter might change. For example, the number of peaks can correspond to how many hours a battery can be discharged to reduce demand.\n\nnpeaks = 1 # Number of peaks\n\nFor the ERCOT 4CP detection task we are interested in correctly predicting the highest monthly load. Next, we filter the day in September with the highest hourly demand and predict the peak.\n\ncrossvalidation_df = crossvalidation_df.reset_index()[['ds','y','MSTL']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['MSTL'].argsort().iloc[-npeaks:].values # Predicted peaks\n\nIn the following plot we see how the MSTL model is able to correctly detect the coincident peak for September 2022.\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['MSTL'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['MSTL'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this example we only include September. However, MSTL can correctly predict the peaks for the 4 months of 2022. You can try this by increasing the nwindows parameter of cross_validation or filtering the Y_df dataset. The complete run for all months take only 10 minutes."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#next-steps",
    "href": "docs/tutorials/electricitypeakforecasting.html#next-steps",
    "title": "Detect Demand Peaks",
    "section": "Next steps",
    "text": "Next steps\nStatsForecast and MSTL in particular are good benchmarking models for peak detection. However, it might be useful to explore further and newer forecasting algorithms. We have seen particularly good results with the N-HiTS, a deep-learning model from Nixtla’s NeuralForecast library.\nLearn how to predict ERCOT demand peaks with our deep-learning N-HiTS model and the NeuralForecast library in this tutorial."
  },
  {
    "objectID": "docs/tutorials/electricitypeakforecasting.html#references",
    "href": "docs/tutorials/electricitypeakforecasting.html#references",
    "title": "Detect Demand Peaks",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”.\nCristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). “N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting”. Accepted at AAAI 2023."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html",
    "href": "docs/tutorials/electricityloadforecasting.html",
    "title": "Electricity Load Forecast",
    "section": "",
    "text": "Give us a ⭐ on Github"
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#introduction",
    "href": "docs/tutorials/electricityloadforecasting.html#introduction",
    "title": "Electricity Load Forecast",
    "section": "Introduction",
    "text": "Introduction\nSome time series are generated from very low frequency data. These data generally exhibit multiple seasonalities. For example, hourly data may exhibit repeated patterns every hour (every 24 observations) or every day (every 24 * 7, hours per day, observations). This is the case for electricity load. Electricity load may vary hourly, e.g., during the evenings electricity consumption may be expected to increase. But also, the electricity load varies by week. Perhaps on weekends there is an increase in electrical activity.\nIn this example we will show how to model the two seasonalities of the time series to generate accurate forecasts in a short time. We will use hourly PJM electricity load data. The original data can be found here."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#libraries",
    "href": "docs/tutorials/electricityloadforecasting.html#libraries",
    "title": "Electricity Load Forecast",
    "section": "Libraries",
    "text": "Libraries\nIn this example we will use the following libraries:\n\nStatsForecast. Lightning ⚡️ fast forecasting with statistical and econometric models. Includes the MSTL model for multiple seasonalities.\nDatasetsForecast. Used to evaluate the performance of the forecasts.\nProphet. Benchmark model developed by Facebook.\nNeuralProphet. Deep Learning version of Prophet. Used as benchark.\n\n\n!pip install statsforecast\n!pip install datasetsforecast\n!pip install prophet\n!pip install \"neuralprophet[live]\""
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "href": "docs/tutorials/electricityloadforecasting.html#forecast-using-multiple-seasonalities",
    "title": "Electricity Load Forecast",
    "section": "Forecast using Multiple Seasonalities",
    "text": "Forecast using Multiple Seasonalities\n\nElectricity Load Data\nAccording to the dataset’s page,\n\nPJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia. The hourly power consumption data comes from PJM’s website and are in megawatts (MW).\n\nLet’s take a look to the data.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\npd.plotting.register_matplotlib_converters()\nplt.rc(\"figure\", figsize=(10, 8))\nplt.rc(\"font\", size=10)\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/master/data/PJM_Load_hourly.csv')\ndf.columns = ['ds', 'y']\ndf.insert(0, 'unique_id', 'PJM_Load_hourly')\ndf['ds'] = pd.to_datetime(df['ds'])\ndf = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\ndf.tail()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n32891\nPJM_Load_hourly\n2001-12-31 20:00:00\n36392.0\n\n\n32892\nPJM_Load_hourly\n2001-12-31 21:00:00\n35082.0\n\n\n32893\nPJM_Load_hourly\n2001-12-31 22:00:00\n33890.0\n\n\n32894\nPJM_Load_hourly\n2001-12-31 23:00:00\n32590.0\n\n\n32895\nPJM_Load_hourly\n2002-01-01 00:00:00\n31569.0\n\n\n\n\n\n\n\n\ndf.plot(x='ds', y='y')\n\n&lt;Axes: xlabel='ds'&gt;\n\n\n\n\n\nWe clearly observe that the time series exhibits seasonal patterns. Moreover, the time series contains 32,896 observations, so it is necessary to use very computationally efficient methods to display them in production.\n\n\nMSTL model\nThe MSTL (Multiple Seasonal-Trend decomposition using LOESS) model, originally developed by Kasun Bandara, Rob J Hyndman and Christoph Bergmeir, decomposes the time series in multiple seasonalities using a Local Polynomial Regression (LOESS). Then it forecasts the trend using a custom non-seasonal model and each seasonality using a SeasonalNaive model.\nStatsForecast contains a fast implementation of the MSTL model. Also, the decomposition of the time series can be calculated.\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import MSTL, AutoARIMA, SeasonalNaive\nfrom statsforecast.utils import AirPassengers as ap\n\nFirst we must define the model parameters. As mentioned before, the electricity load presents seasonalities every 24 hours (Hourly) and every 24 * 7 (Daily) hours. Therefore, we will use [24, 24 * 7] as the seasonalities that the MSTL model receives. We must also specify the manner in which the trend will be forecasted. In this case we will use the AutoARIMA model.\n\nmstl = MSTL(\n    season_length=[24, 24 * 7], # seasonalities of the time series \n    trend_forecaster=AutoARIMA() # model used to forecast trend\n)\n\nOnce the model is instantiated, we have to instantiate the StatsForecast class to create forecasts.\n\nsf = StatsForecast(\n    models=[mstl], # model used to fit each time series \n    freq='H', # frequency of the data\n)\n\n\nFit the model\nAfer that, we just have to use the fit method to fit each model to each time series.\n\nsf = sf.fit(df=df)\n\n\n\nDecompose the time series in multiple seasonalities\nOnce the model is fitted, we can access the decomposition using the fitted_ attribute of StatsForecast. This attribute stores all relevant information of the fitted models for each of the time series.\nIn this case we are fitting a single model for a single time series, so by accessing the fitted_ location [0, 0] we will find the relevant information of our model. The MSTL class generates a model_ attribute that contains the way the series was decomposed.\n\nsf.fitted_[0, 0].model_\n\n\n\n\n\n\n\n\ndata\ntrend\nseasonal24\nseasonal168\nremainder\n\n\n\n\n0\n22259.0\n26183.898892\n-5215.124554\n609.000432\n681.225229\n\n\n1\n21244.0\n26181.599305\n-6255.673234\n603.823918\n714.250011\n\n\n2\n20651.0\n26179.294886\n-6905.329895\n636.820423\n740.214587\n\n\n3\n20421.0\n26176.985472\n-7073.420118\n615.825999\n701.608647\n\n\n4\n20713.0\n26174.670877\n-7062.395760\n991.521912\n609.202971\n\n\n...\n...\n...\n...\n...\n...\n\n\n32891\n36392.0\n33123.552727\n4387.149171\n-488.177882\n-630.524015\n\n\n32892\n35082.0\n33148.242575\n3479.852929\n-682.928737\n-863.166767\n\n\n32893\n33890.0\n33172.926165\n2307.808829\n-650.566775\n-940.168219\n\n\n32894\n32590.0\n33197.603322\n748.587723\n-555.177849\n-801.013195\n\n\n32895\n31569.0\n33222.273902\n-967.124123\n-265.895357\n-420.254422\n\n\n\n\n32896 rows × 5 columns\n\n\n\nLet’s look graphically at the different components of the time series.\n\nsf.fitted_[0, 0].model_.tail(24 * 28).plot(subplots=True, grid=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe observe that there is a clear trend towards the high (orange line). This component would be predicted with the AutoARIMA model. We can also observe that every 24 hours and every 24 * 7 hours there is a very well defined pattern. These two components will be forecast separately using a SeasonalNaive model.\n\n\nProduce forecasts\nTo generate forecasts we only have to use the predict method specifying the forecast horizon (h). In addition, to calculate prediction intervals associated to the forecasts, we can include the parameter level that receives a list of levels of the prediction intervals we want to build. In this case we will only calculate the 90% forecast interval (level=[90]).\n\nforecasts = sf.predict(h=24, level=[90])\nforecasts.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2002-01-01 01:00:00\n29956.744141\n29585.187500\n30328.298828\n\n\nPJM_Load_hourly\n2002-01-01 02:00:00\n29057.691406\n28407.498047\n29707.884766\n\n\nPJM_Load_hourly\n2002-01-01 03:00:00\n28654.699219\n27767.101562\n29542.298828\n\n\nPJM_Load_hourly\n2002-01-01 04:00:00\n28499.009766\n27407.640625\n29590.378906\n\n\nPJM_Load_hourly\n2002-01-01 05:00:00\n28821.716797\n27552.236328\n30091.197266\n\n\n\n\n\n\n\nLet’s look at our forecasts graphically.\n\n_, ax = plt.subplots(1, 1, figsize = (20, 7))\ndf_plot = pd.concat([df, forecasts]).set_index('ds').tail(24 * 7)\ndf_plot[['y', 'MSTL']].plot(ax=ax, linewidth=2)\nax.fill_between(df_plot.index, \n                df_plot['MSTL-lo-90'], \n                df_plot['MSTL-hi-90'],\n                alpha=.35,\n                color='orange',\n                label='MSTL-level-90')\nax.set_title('PJM Load Hourly', fontsize=22)\nax.set_ylabel('Electricity Load', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n\n\n\n\nIn the next section we will plot different models so it is convenient to reuse the previous code with the following function.\n\ndef plot_forecasts(y_hist, y_true, y_pred, models):\n    _, ax = plt.subplots(1, 1, figsize = (20, 7))\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    df_plot = pd.concat([y_hist, y_true]).set_index('ds').tail(24 * 7)\n    df_plot[['y'] + models].plot(ax=ax, linewidth=2)\n    colors = ['orange', 'green', 'red']\n    for model, color in zip(models, colors):\n        ax.fill_between(df_plot.index, \n                        df_plot[f'{model}-lo-90'], \n                        df_plot[f'{model}-hi-90'],\n                        alpha=.35,\n                        color=color,\n                        label=f'{model}-level-90')\n    ax.set_title('PJM Load Hourly', fontsize=22)\n    ax.set_ylabel('Electricity Load', fontsize=20)\n    ax.set_xlabel('Timestamp [t]', fontsize=20)\n    ax.legend(prop={'size': 15})\n    ax.grid()\n\n\n\n\nPerformance of the MSTL model\n\nSplit Train/Test sets\nTo validate the accuracy of the MSTL model, we will show its performance on unseen data. We will use a classical time series technique that consists of dividing the data into a training set and a test set. We will leave the last 24 observations (the last day) as the test set. So the model will train on 32,872 observations.\n\ndf_test = df.tail(24)\ndf_train = df.drop(df_test.index)\n\n\n\nMSTL model\nIn addition to the MSTL model, we will include the SeasonalNaive model as a benchmark to validate the added value of the MSTL model. Including StatsForecast models is as simple as adding them to the list of models to be fitted.\n\nsf = StatsForecast(\n    models=[mstl, SeasonalNaive(season_length=24)], # add SeasonalNaive model to the list\n    freq='H'\n)\n\nTo measure the fitting time we will use the time module.\n\nfrom time import time\n\nTo retrieve the forecasts of the test set we only have to do fit and predict as before.\n\ninit = time()\nsf = sf.fit(df=df_train)\nforecasts_test = sf.predict(h=len(df_test), level=[90])\nend = time()\nforecasts_test.head()\n\n\n\n\n\n\n\n\nds\nMSTL\nMSTL-lo-90\nMSTL-hi-90\nSeasonalNaive\nSeasonalNaive-lo-90\nSeasonalNaive-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\n\n\n\nPJM_Load_hourly\n2001-12-31 01:00:00\n28345.212891\n27973.572266\n28716.853516\n28326.0\n23468.693359\n33183.304688\n\n\nPJM_Load_hourly\n2001-12-31 02:00:00\n27567.455078\n26917.085938\n28217.824219\n27362.0\n22504.693359\n32219.306641\n\n\nPJM_Load_hourly\n2001-12-31 03:00:00\n27260.001953\n26372.138672\n28147.865234\n27108.0\n22250.693359\n31965.306641\n\n\nPJM_Load_hourly\n2001-12-31 04:00:00\n27328.125000\n26236.410156\n28419.839844\n26865.0\n22007.693359\n31722.306641\n\n\nPJM_Load_hourly\n2001-12-31 05:00:00\n27640.673828\n26370.773438\n28910.572266\n26808.0\n21950.693359\n31665.306641\n\n\n\n\n\n\n\n\ntime_mstl = (end - init) / 60\nprint(f'MSTL Time: {time_mstl:.2f} minutes')\n\nMSTL Time: 0.22 minutes\n\n\nThen we were able to generate forecasts for the next 24 hours. Now let’s look at the graphical comparison of the forecasts with the actual values.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\nLet’s look at those produced only by MSTL.\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL'])\n\n\n\n\nWe note that MSTL produces very accurate forecasts that follow the behavior of the time series. Now let us calculate numerically the accuracy of the model. We will use the following metrics: MAE, MAPE, MASE, RMSE, SMAPE.\n\nfrom datasetsforecast.losses import (\n    mae, mape, mase, rmse, smape\n)\n\n\ndef evaluate_performace(y_hist, y_true, y_pred, models):\n    y_true = y_true.merge(y_pred, how='left', on=['unique_id', 'ds'])\n    evaluation = {}\n    for model in models:\n        evaluation[model] = {}\n        for metric in [mase, mae, mape, rmse, smape]:\n            metric_name = metric.__name__\n            if metric_name == 'mase':\n                evaluation[model][metric_name] = metric(y_true['y'].values, \n                                                 y_true[model].values, \n                                                 y_hist['y'].values, seasonality=24)\n            else:\n                evaluation[model][metric_name] = metric(y_true['y'].values, y_true[model].values)\n    return pd.DataFrame(evaluation).T\n\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nWe observe that MSTL has an improvement of about 60% over the SeasonalNaive method in the test set measured in MASE.\n\n\nComparison with Prophet\nOne of the most widely used models for time series forecasting is Prophet. This model is known for its ability to model different seasonalities (weekly, daily yearly). We will use this model as a benchmark to see if the MSTL adds value for this time series.\n\nfrom prophet import Prophet\n\n# create prophet model\nprophet = Prophet(interval_width=0.9)\ninit = time()\nprophet.fit(df_train)\n# produce forecasts\nfuture = prophet.make_future_dataframe(periods=len(df_test), freq='H', include_history=False)\nforecast_prophet = prophet.predict(future)\nend = time()\n# data wrangling\nforecast_prophet = forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\nforecast_prophet.columns = ['ds', 'Prophet', 'Prophet-lo-90', 'Prophet-hi-90']\nforecast_prophet.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_prophet.head()\n\n23:41:40 - cmdstanpy - INFO - Chain [1] start processing\n23:41:56 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nunique_id\nds\nProphet\nProphet-lo-90\nProphet-hi-90\n\n\n\n\n0\nPJM_Load_hourly\n2001-12-31 01:00:00\n25317.658386\n20757.919539\n30313.561582\n\n\n1\nPJM_Load_hourly\n2001-12-31 02:00:00\n24024.188077\n19304.093939\n28667.495805\n\n\n2\nPJM_Load_hourly\n2001-12-31 03:00:00\n23348.306824\n18608.982825\n28497.334752\n\n\n3\nPJM_Load_hourly\n2001-12-31 04:00:00\n23356.150113\n18721.142270\n28136.888630\n\n\n4\nPJM_Load_hourly\n2001-12-31 05:00:00\n24130.861217\n19896.188455\n28970.202276\n\n\n\n\n\n\n\n\ntime_prophet = (end - init) / 60\nprint(f'Prophet Time: {time_prophet:.2f} minutes')\n\nProphet Time: 0.30 minutes\n\n\n\ntimes = pd.DataFrame({'model': ['MSTL', 'Prophet'], 'time (mins)': [time_mstl, time_prophet]})\ntimes\n\n\n\n\n\n\n\n\nmodel\ntime (mins)\n\n\n\n\n0\nMSTL\n0.217266\n\n\n1\nProphet\n0.301172\n\n\n\n\n\n\n\nWe observe that the time required for Prophet to perform the fit and predict pipeline is greater than MSTL. Let’s look at the forecasts produced by Prophet.\n\nforecasts_test = forecasts_test.merge(forecast_prophet, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'SeasonalNaive', 'Prophet'])\n\n\n\n\nWe note that Prophet is able to capture the overall behavior of the time series. However, in some cases it produces forecasts well below the actual value. It also does not correctly adjust the valleys.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'Prophet', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nProphet\n1.094768\n2273.036373\n7.343292\n2709.400341\n7.688665\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nIn terms of accuracy, Prophet is not able to produce better forecasts than the SeasonalNaive model, however, the MSTL model improves Prophet’s forecasts by 69% (MASE).\n\n\nComparison with NeuralProphet\nNeuralProphet is the version of Prophet using deep learning. This model is also capable of handling different seasonalities so we will also use it as a benchmark.\n\nfrom neuralprophet import NeuralProphet\n\nneuralprophet = NeuralProphet(quantiles=[0.05, 0.95])\ninit = time()\nneuralprophet.fit(df_train.drop(columns='unique_id'))\nfuture = neuralprophet.make_future_dataframe(df=df_train.drop(columns='unique_id'), periods=len(df_test))\nforecast_np = neuralprophet.predict(future)\nend = time()\nforecast_np = forecast_np[['ds', 'yhat1', 'yhat1 5.0%', 'yhat1 95.0%']]\nforecast_np.columns = ['ds', 'NeuralProphet', 'NeuralProphet-lo-90', 'NeuralProphet-hi-90']\nforecast_np.insert(0, 'unique_id', 'PJM_Load_hourly')\nforecast_np.head()\n\nWARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\nINFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\nINFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 76\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.973% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 95.833% of the data.\nINFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\nINFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunique_id\nds\nNeuralProphet\nNeuralProphet-lo-90\nNeuralProphet-hi-90\n\n\n\n\n0\nPJM_Load_hourly\n2001-12-31 01:00:00\n25019.892578\n22296.675781\n27408.724609\n\n\n1\nPJM_Load_hourly\n2001-12-31 02:00:00\n24128.816406\n21439.851562\n26551.615234\n\n\n2\nPJM_Load_hourly\n2001-12-31 03:00:00\n23736.679688\n20961.978516\n26289.349609\n\n\n3\nPJM_Load_hourly\n2001-12-31 04:00:00\n23476.744141\n20731.619141\n26050.443359\n\n\n4\nPJM_Load_hourly\n2001-12-31 05:00:00\n23899.162109\n21217.503906\n26449.603516\n\n\n\n\n\n\n\n\ntime_np = (end - init) / 60\nprint(f'Prophet Time: {time_np:.2f} minutes')\n\nProphet Time: 2.95 minutes\n\n\n\ntimes = times.append({'model': 'NeuralProphet', 'time (mins)': time_np}, ignore_index=True)\ntimes\n\n\n\n\n\n\n\n\nmodel\ntime (mins)\n\n\n\n\n0\nMSTL\n0.217266\n\n\n1\nProphet\n0.301172\n\n\n2\nNeuralProphet\n2.946358\n\n\n\n\n\n\n\nWe observe that NeuralProphet requires a longer processing time than Prophet and MSTL.\n\nforecasts_test = forecasts_test.merge(forecast_np, how='left', on=['unique_id', 'ds'])\n\n\nplot_forecasts(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet'])\n\n\n\n\nThe forecasts graph shows that NeuralProphet generates very similar results to Prophet, as expected.\n\nevaluate_performace(df_train, df_test, forecasts_test, models=['MSTL', 'NeuralProphet', 'Prophet', 'SeasonalNaive'])\n\n\n\n\n\n\n\n\nmase\nmae\nmape\nrmse\nsmape\n\n\n\n\nMSTL\n0.341926\n709.932048\n2.182804\n892.888012\n2.162832\n\n\nNeuralProphet\n1.084915\n2252.578613\n7.280202\n2671.145730\n7.615492\n\n\nProphet\n1.094768\n2273.036373\n7.343292\n2709.400341\n7.688665\n\n\nSeasonalNaive\n0.894653\n1857.541667\n5.648190\n2201.384101\n5.868604\n\n\n\n\n\n\n\nWith respect to numerical evaluation, NeuralProphet improves the results of Prophet, as expected, however, MSTL improves over NeuralProphet’s foreacasts by 68% (MASE).\n\n\n\n\n\n\nImportant\n\n\n\nThe performance of NeuralProphet can be improved using hyperparameter optimization, which can increase the fitting time significantly. In this example we show its performance with the default version."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#conclusion",
    "href": "docs/tutorials/electricityloadforecasting.html#conclusion",
    "title": "Electricity Load Forecast",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we introduced MSTL, a model originally developed by Kasun Bandara, Rob Hyndman and Christoph Bergmeir capable of handling time series with multiple seasonalities. We also showed that for the PJM electricity load time series offers better performance in time and accuracy than the Prophet and NeuralProphet models."
  },
  {
    "objectID": "docs/tutorials/electricityloadforecasting.html#references",
    "href": "docs/tutorials/electricityloadforecasting.html#references",
    "title": "Electricity Load Forecast",
    "section": "References",
    "text": "References\n\nBandara, Kasun & Hyndman, Rob & Bergmeir, Christoph. (2021). “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns”."
  },
  {
    "objectID": "docs/contribute/issue-labels.html",
    "href": "docs/contribute/issue-labels.html",
    "title": "Understanding Issue Labels",
    "section": "",
    "text": "This segment delves into the variety of issue labels used within the Nixtla GitHub repository.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/issue-labels.html#labels-relevant-to-contributors",
    "href": "docs/contribute/issue-labels.html#labels-relevant-to-contributors",
    "title": "Understanding Issue Labels",
    "section": "Labels Relevant to Contributors",
    "text": "Labels Relevant to Contributors\nShould you be a contributor now or in the future, it’s important to take note of issues flagged with these labels.\n\nThe first-timers-only Label\nFor those who have not yet contributed to Nixtla, start by looking for issues tagged as first-timers-only.\nPlease note that before we can accept your contribution to Nixtla, you’ll need to sign our Contributor License Agreement.\nYou can browse all first-timers-only issues here.\n\n\nThe good first issue Label\nIssues labeled as good first issue are ideal for newcomers.\nYou can browse all good first issue issues here.\n\n\nThe help wanted Label\nIssues tagged as help wanted are open to anyone who wishes to contribute to Nixtla.\nYou can browse all help wanted issues here.\n\n\nThe bug Label\nThe bug label flags issues that outline something that’s currently not functioning correctly.\nYou can report a bug by following the instructions here.\n\n\nThe discussion Label\nIf an issue is labeled as discussion, it signifies that more conversation is needed before it can be resolved.\n\n\nThe documentation Label\nThe documentation label identifies issues pertaining to our documentation.\nYou can contribute to improving our documentation by creating issues following the guidelines here.\n\n\nThe enhancement Label\nAs Nixtla continues to evolve, there are always areas that can be enhanced. All issues suggesting improvements to Nixtla are tagged with the enhancement label.\nYou can propose a feature by following the instructions here.\n\n\nThe discussion Label\nIf an issue is labeled as discussion, it needs more information before it can be resolved.\n\n\nThe requested Label\nOur users are welcomed to propose improvements, report bugs, request feature, etc. Any issue originating from them is flagged as requested."
  },
  {
    "objectID": "docs/contribute/contribute.html",
    "href": "docs/contribute/contribute.html",
    "title": "Contribute to Nixtla",
    "section": "",
    "text": "Thank you for your interest in contributing to Nixtla. Nixtla is free, open-source software and welcomes all types of contributions, including documentation changes, bug reports, bug fixes, or new source code changes.\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/contribute.html#contribution-issues",
    "href": "docs/contribute/contribute.html#contribution-issues",
    "title": "Contribute to Nixtla",
    "section": "Contribution issues 🔧",
    "text": "Contribution issues 🔧\nMost of the issues that are open for contributions will be tagged with good first issue or help wanted. A great place to start looking will be our GitHub projects for:\n\nCommunity writers dashboard.\nCommunity code contributors dashboard.\n\nAlso, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance!\nAfter you find the issue that you want to contribute to, follow the fork-and-pull workflow:\n\nFork the Nixtla repository you want to work on (e.g. StatsForecast or NeuralForecast)\nClone the repository locally (git clone) and create a new branch (git checkout -b my-new-branch)\nMake changes and commit them\nPush your local branch to your fork\nSubmit a Pull Request so that we can review your changes\nWrite a commit message\nMake sure that the CI tests are GREEN (CI tests refer to automated tests that are run on code changes to ensure that new additions or modifications do not introduce new errors or break existing functionality.)\n\n\nBe sure to merge the latest from “upstream” before making a Pull Request!\n\nYou can find a complete step-by-step guide on this fork-and-pull workflow here.\nPull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA."
  },
  {
    "objectID": "docs/contribute/contribute.html#documentation",
    "href": "docs/contribute/contribute.html#documentation",
    "title": "Contribute to Nixtla",
    "section": "Documentation 📖",
    "text": "Documentation 📖\nWe are committed to continuously improving our documentation. As such, we warmly welcome any Pull Requests that focus on improving our grammar, documentation structure, or fixing any typos.\n\nCheck the documentation tagged issues and help us."
  },
  {
    "objectID": "docs/contribute/contribute.html#write-for-us",
    "href": "docs/contribute/contribute.html#write-for-us",
    "title": "Contribute to Nixtla",
    "section": "Write for us 📝",
    "text": "Write for us 📝\nDo you find Nixtla useful and want to share your story or create some content? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you 💚\n\nThis document is based on the documentation from MindsDB"
  },
  {
    "objectID": "docs/contribute/techstack.html",
    "href": "docs/contribute/techstack.html",
    "title": "Contributing Code to Nixtla Development",
    "section": "",
    "text": "Curious about the skills required to contribute to the Nixtla project?\n\n\n\n\nIf you’re interested in making code contributions, possessing any of the following skills can assist you in getting started:\n\nGitHub\nPython 3\nconda\nnbdev\n\n\n\n\n\nForecasting: Principles and Practice\nPython Adaptation of Forecasting: Principles and Practice\n\nHappy forecasting!\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/contribute/techstack.html#required-skills-for-contribution",
    "href": "docs/contribute/techstack.html#required-skills-for-contribution",
    "title": "Contributing Code to Nixtla Development",
    "section": "",
    "text": "If you’re interested in making code contributions, possessing any of the following skills can assist you in getting started:\n\nGitHub\nPython 3\nconda\nnbdev\n\n\n\n\n\nForecasting: Principles and Practice\nPython Adaptation of Forecasting: Principles and Practice\n\nHappy forecasting!"
  },
  {
    "objectID": "docs/getting-started/getting_started_short.html",
    "href": "docs/getting-started/getting_started_short.html",
    "title": "Quick Start",
    "section": "",
    "text": "StatsForecast follows the sklearn model API. For this minimal example, you will create an instance of the StatsForecast class and then call its fit and predict methods. We recommend this option if speed is not paramount and you want to explore the fitted values and parameters.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to forecast many series, we recommend using the forecast method. Check this Getting Started with multiple time series guide.\n\n\nThe input to StatsForecast is always a data frame in long format with three columns: unique_id, ds and y:\n\nThe unique_id (string, int or category) represents an identifier for the series.\nThe ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\nThe y (numeric) represents the measurement we wish to forecast.\n\nAs an example, let’s look at the US Air Passengers dataset. This time series consists of monthly totals of a US airline passengers from 1949 to 1960. The CSV is available here.\nWe assume you have StatsForecast already installed. Check this guide for instructions on how to install StatsForecast.\nFirst, we’ll import the data:\n\nimport pandas as pd\n\n\ndf = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/air-passengers.csv')\ndf.head()\n\n\n\n\n\n\n\n\nunique_id\nds\ny\n\n\n\n\n0\nAirPassengers\n1949-01-01\n112\n\n\n1\nAirPassengers\n1949-02-01\n118\n\n\n2\nAirPassengers\n1949-03-01\n132\n\n\n3\nAirPassengers\n1949-04-01\n129\n\n\n4\nAirPassengers\n1949-05-01\n121\n\n\n\n\n\n\n\nWe fit the model by instantiating a new StatsForecast object with its two required parameters: https://nixtla.github.io/statsforecast/src/core/models.html * models: a list of models. Select the models you want from models and import them. For this example, we will use a AutoARIMA model. We set season_length to 12 because we expect seasonal effects every 12 months. (See: Seasonal periods)\n\nfreq: a string indicating the frequency of the data. (See pandas available frequencies.)\n\nAny settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n\n\n\n\n\n\nNote\n\n\n\nStatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s.\n\n\n\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\n\nsf = StatsForecast(\n    models = [AutoARIMA(season_length = 12)],\n    freq = 'M'\n)\n\nsf.fit(df)\n\nThe predict method takes two arguments: forecasts the next h (for horizon) and level.\n\nh (int): represents the forecast h steps into the future. In this case, 12 months ahead.\nlevel (list of floats): this optional parameter is used for probabilistic forecasting. Set the level (or confidence percentile) of your prediction interval. For example, level=[90] means that the model expects the real value to be inside that interval 90% of the times.\n\nThe forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals.\n\nforecast_df = sf.predict(h=12, level=[90]) \n\nforecast_df.tail()\n\n\n\n\n\n\n\n\nds\nAutoARIMA\nAutoARIMA-lo-90\nAutoARIMA-hi-90\n\n\nunique_id\n\n\n\n\n\n\n\n\nAirPassengers\n1961-07-31\n633.230774\n589.562378\n676.899170\n\n\nAirPassengers\n1961-08-31\n535.230774\n489.082153\n581.379456\n\n\nAirPassengers\n1961-09-30\n488.230804\n439.728699\n536.732910\n\n\nAirPassengers\n1961-10-31\n417.230804\n366.484253\n467.977356\n\n\nAirPassengers\n1961-11-30\n459.230804\n406.334930\n512.126648\n\n\n\n\n\n\n\nYou can plot the forecast by calling the StatsForecast.plot method and passing in your forecast dataframe.\n\ndf[\"ds\"]=pd.to_datetime(df[\"ds\"])\nsf.plot(df, forecast_df, level=[90])\n\n\n                                                \n\n\n\n\n\n\n\n\nNext Steps\n\n\n\n\nBuild and end-to-end forecasting pipeline following best practices in End to End Walkthrough\nForecast millions of series in a scalable cluster in the cloud using Spark and Nixtla\nDetect anomalies in your past observations\n\n\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "docs/getting-started/installation.html",
    "href": "docs/getting-started/installation.html",
    "title": "Install",
    "section": "",
    "text": "You can install the released version of StatsForecast from the Python package index with:\npip install statsforecast\nor\nconda install -c conda-forge statsforecast\n\n\n\n\n\n\nWarning\n\n\n\nWe are constantly updating StatsForecast, so we suggest fixing the version to avoid issues. pip install statsforecast==\"1.0.0\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe recommend installing your libraries inside a python virtual or conda environment.\n\n\n\nUser our env (optional)\nIf you don’t have a Conda environment and need tools like Numba, Pandas, NumPy, Jupyter, StatsModels, and Nbdev you can use ours by following these steps:\n\nClone the StatsForecast repo:\n\n$ git clone https://github.com/Nixtla/statsforecast.git && cd statsforecast\n\nCreate the environment using the environment.yml file:\n\n$ conda env create -f environment.yml\n\nActivate the environment:\n\n$ conda activate statsforecast\n\n\n\n\nGive us a ⭐ on Github"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "StatsForecast Blog",
    "section": "",
    "text": "Scalable Time Series Modeling with open-source projects\n\n\n\n\n\nHow to Forecast 1M Time Series in 15 Minutes with Spark, Fugue and Nixtla’s Statsforecast.\n\n\n\n\n\n\nOct 5, 2022\n\n\nFugue , Nixtla\n\n\n\n\n\n\nNo matching items\n\nGive us a ⭐ on Github"
  }
]